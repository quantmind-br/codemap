## Context

You are an AI assistant helping with software development tasks.

**Current Date:** 2025-12-05 18:47:17

---

## Task Description

Faça uma análise das tools existentes no servidor mcp e como cada uma delas pode ajuder um agente de codificação. Verifique se é possível reduzir o número de tools a partir da consolidação das ferramentas. Exemplo: Existem tools que servem para gerar um conhecimento prévio da base de código e gerar contexto. Todas as informações para gerar contexto poderiam ser acionadas junto. Verifique também como seria possívem implementar indexação automática da base de código e como fazer a reindexação a medida que a base de código for modificada sem gerar um reindexação total da base de código.

---

## Project Constraints & Rules



---

## Project Structure

└── codemap/
    ├── analyze/
    │   ├── anthropic.go [7.1KB]
    │   ├── client.go [3.4KB]
    │   ├── embed.go [6.4KB]
    │   ├── factory.go [2.0KB]
    │   ├── gemini.go [11.2KB]
    │   ├── mock.go [5.2KB]
    │   ├── ollama.go [8.0KB]
    │   ├── openai.go [9.2KB]
    │   ├── prompts.go [7.0KB]
    │   ├── retriever.go [9.4KB]
    │   ├── source.go [8.5KB]
    │   └── tokens.go [3.2KB]
    ├── assets/
    ├── cache/
    │   └── cache.go [7.0KB]
    ├── config/
    │   └── config.go [9.7KB]
    ├── development-docs/
    │   ├── plans/
    │   │   ├── 01_knowledge_graph.md [2.0KB]
    │   │   ├── 02_llm_integration.md [1.4KB]
    │   │   └── 03_hybrid_retrieval.md [1.5KB]
    │   ├── 0001-enhanced-code-analysis-plan.md [41.0KB]
    │   ├── 0001-enhanced-code-analysis.md [7.0KB]
    │   ├── 0002-token-heuristics-symbol-search-plan.md [16.1KB]
    │   ├── 0002-token-heuristics-symbol-search.md [9.1KB]
    │   ├── 0003-graphrag-implementation-plan.md [2.7KB]
    │   ├── 0003-graphrag-implementation.md [24.7KB]
    │   ├── 0004-gemini-integration-plan.md [29.5KB]
    │   └── 0004-gemini-integration.md [5.1KB]
    ├── graph/
    │   ├── builder.go [8.2KB]
    │   ├── query.go [6.6KB]
    │   ├── store.go [4.4KB]
    │   ├── types.go [7.9KB]
    │   └── vectors.go [7.9KB]
    ├── mcp/
    │   ├── main.go [38.1KB]
    │   └── tools_test.go [11.4KB]
    ├── render/
    │   ├── api.go [3.9KB]
    │   ├── colors.go [3.3KB]
    │   ├── depgraph.go [11.3KB]
    │   ├── skyline.go [16.0KB]
    │   └── tree.go [10.9KB]
    ├── scanner/
    │   ├── grammars/
    │   ├── queries/
    │   │   ├── bash.scm [252B]
    │   │   ├── c.scm [413B]
    │   │   ├── c_sharp.scm [781B]
    │   │   ├── cpp.scm [442B]
    │   │   ├── dart.scm [559B]
    │   │   ├── go.scm [749B]
    │   │   ├── java.scm [660B]
    │   │   ├── javascript.scm [827B]
    │   │   ├── kotlin.scm [181B]
    │   │   ├── php.scm [459B]
    │   │   ├── python.scm [540B]
    │   │   ├── r.scm [783B]
    │   │   ├── ruby.scm [449B]
    │   │   ├── rust.scm [631B]
    │   │   ├── swift.scm [361B]
    │   │   └── typescript.scm [928B]
    │   ├── testdata/
    │   │   └── corpus/
    │   │       ├── go/
    │   │       │   ├── expected.json [655B]
    │   │       │   ├── main.go [697B]
    │   │       │   └── types.go [728B]
    │   │       ├── python/
    │   │       │   ├── classes.py [1.1KB]
    │   │       │   ├── expected.json [676B]
    │   │       │   └── main.py [803B]
    │   │       ├── typescript/
    │   │       │   ├── expected.json [704B]
    │   │       │   ├── main.ts [876B]
    │   │       │   └── types.ts [953B]
    │   │       └── corpus_test.go [2.8KB]
    │   ├── build-grammars.sh [3.4KB]
    │   ├── calls.go [7.8KB]
    │   ├── deps.go [4.2KB]
    │   ├── git.go [6.8KB]
    │   ├── grammar.go [13.0KB]
    │   ├── grammar_unix.go [618B]
    │   ├── grammar_windows.go [795B]
    │   ├── symbol.go [1.9KB]
    │   ├── types.go [4.8KB]
    │   └── walker.go [4.3KB]
    ├── scripts/
    │   └── build-grammars.sh [3.1KB]
    ├── AGENTS.md [2.7KB]
    ├── CLAUDE.md [5.9KB]
    ├── CODE_OF_CONDUCT.md [5.1KB]
    ├── CONTRIBUTING.md [2.3KB]
    ├── GEMINI.md [3.3KB]
    ├── LICENSE [1.0KB]
    ├── Makefile [1.5KB]
    ├── README.md [15.2KB]
    ├── TOOLS-DESCRIPTION.md [6.3KB]
    ├── bug.md [18.2KB]
    ├── codemap.rb [5.1KB]
    ├── go.mod [1.6KB]
    ├── go.sum [9.7KB]
    └── main.go [39.8KB]

<file path="analyze/anthropic.go">
package analyze

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"time"
)

const anthropicAPIVersion = "2023-06-01"

// AnthropicClient implements LLMClient for Anthropic's Claude API.
type AnthropicClient struct {
	apiKey     string
	baseURL    string
	httpClient *http.Client
	config     ClientConfig
}

// NewAnthropicClient creates a new Anthropic Claude client.
func NewAnthropicClient(apiKey string, cfg ClientConfig) (*AnthropicClient, error) {
	if apiKey == "" {
		return nil, ErrNoAPIKey
	}
	if cfg.Model == "" {
		cfg.Model = "claude-sonnet-4-20250514"
	}
	if cfg.Timeout == 0 {
		cfg.Timeout = 60 * time.Second
	}

	return &AnthropicClient{
		apiKey:  apiKey,
		baseURL: "https://api.anthropic.com/v1",
		httpClient: &http.Client{
			Timeout: cfg.Timeout,
		},
		config: cfg,
	}, nil
}

// Name returns the provider name.
func (c *AnthropicClient) Name() string {
	return "anthropic"
}

// Ping checks if the Anthropic API is reachable.
func (c *AnthropicClient) Ping(ctx context.Context) error {
	// Anthropic doesn't have a dedicated health endpoint, so we make a minimal request
	req := &CompletionRequest{
		Messages: []Message{
			{Role: "user", Content: "Hi"},
		},
		MaxTokens: 1,
	}
	_, err := c.Complete(ctx, req)
	return err
}

// Models returns a list of available models.
// Anthropic doesn't have a models listing API, so we return known models.
func (c *AnthropicClient) Models(ctx context.Context) ([]string, error) {
	return []string{
		"claude-sonnet-4-20250514",
		"claude-3-5-sonnet-20241022",
		"claude-3-5-haiku-20241022",
		"claude-3-opus-20240229",
		"claude-3-sonnet-20240229",
		"claude-3-haiku-20240307",
	}, nil
}

// anthropicRequest is the request format for Anthropic's messages API.
type anthropicRequest struct {
	Model         string             `json:"model"`
	MaxTokens     int                `json:"max_tokens"`
	Messages      []anthropicMessage `json:"messages"`
	System        string             `json:"system,omitempty"`
	Temperature   float64            `json:"temperature,omitempty"`
	StopSequences []string           `json:"stop_sequences,omitempty"`
}

type anthropicMessage struct {
	Role    string `json:"role"`
	Content string `json:"content"`
}

// anthropicResponse is the response format from Anthropic's messages API.
type anthropicResponse struct {
	ID      string `json:"id"`
	Type    string `json:"type"`
	Role    string `json:"role"`
	Content []struct {
		Type string `json:"type"`
		Text string `json:"text"`
	} `json:"content"`
	Model        string `json:"model"`
	StopReason   string `json:"stop_reason"`
	StopSequence string `json:"stop_sequence,omitempty"`
	Usage        struct {
		InputTokens  int `json:"input_tokens"`
		OutputTokens int `json:"output_tokens"`
	} `json:"usage"`
}

// anthropicErrorResponse is the error format from Anthropic.
type anthropicErrorResponse struct {
	Type  string `json:"type"`
	Error struct {
		Type    string `json:"type"`
		Message string `json:"message"`
	} `json:"error"`
}

// Complete generates a completion using Anthropic's messages API.
func (c *AnthropicClient) Complete(ctx context.Context, req *CompletionRequest) (*CompletionResponse, error) {
	start := time.Now()

	model := req.Model
	if model == "" {
		model = c.config.Model
	}

	temp := req.Temperature
	if temp == 0 {
		temp = c.config.Temperature
	}

	maxTokens := req.MaxTokens
	if maxTokens == 0 {
		maxTokens = c.config.MaxTokens
	}

	// Convert messages - Anthropic uses a different format
	var systemPrompt string
	messages := make([]anthropicMessage, 0, len(req.Messages))

	for _, m := range req.Messages {
		if m.Role == "system" {
			systemPrompt = m.Content
		} else {
			messages = append(messages, anthropicMessage{
				Role:    m.Role,
				Content: m.Content,
			})
		}
	}

	anthropicReq := anthropicRequest{
		Model:         model,
		MaxTokens:     maxTokens,
		Messages:      messages,
		System:        systemPrompt,
		Temperature:   temp,
		StopSequences: req.Stop,
	}

	body, err := json.Marshal(anthropicReq)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	// Retry logic with exponential backoff
	var resp *http.Response
	var lastErr error
	for attempt := 0; attempt <= c.config.MaxRetries; attempt++ {
		if attempt > 0 {
			time.Sleep(time.Duration(attempt*attempt) * time.Second)
		}

		// Check context before each attempt
		if ctx.Err() != nil {
			return nil, ErrTimeout
		}

		// Create new request for each attempt (body reader is consumed after each request)
		httpReq, err := http.NewRequestWithContext(ctx, "POST", c.baseURL+"/messages", bytes.NewReader(body))
		if err != nil {
			return nil, err
		}
		httpReq.Header.Set("Content-Type", "application/json")
		httpReq.Header.Set("x-api-key", c.apiKey)
		httpReq.Header.Set("anthropic-version", anthropicAPIVersion)

		resp, lastErr = c.httpClient.Do(httpReq)
		if lastErr != nil {
			// Network error - retry
			if ctx.Err() != nil {
				return nil, ErrTimeout
			}
			continue
		}

		// Check response status
		switch {
		case resp.StatusCode == http.StatusOK:
			// Success - break out of retry loop
			lastErr = nil
		case resp.StatusCode == http.StatusTooManyRequests:
			// Rate limited - close body and retry
			resp.Body.Close()
			lastErr = ErrRateLimited
			continue
		case resp.StatusCode >= 500:
			// Server error - close body and retry
			resp.Body.Close()
			lastErr = fmt.Errorf("server error: %d", resp.StatusCode)
			continue
		default:
			// Client error (4xx) - don't retry, break to handle error with body
			lastErr = nil
		}
		break
	}

	if lastErr != nil {
		return nil, fmt.Errorf("request failed after %d retries: %w", c.config.MaxRetries, lastErr)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		bodyBytes, _ := io.ReadAll(resp.Body)
		var errResp anthropicErrorResponse
		if json.Unmarshal(bodyBytes, &errResp) == nil && errResp.Error.Message != "" {
			return nil, fmt.Errorf("anthropic error: %s", errResp.Error.Message)
		}
		return nil, fmt.Errorf("anthropic error (status %d): %s", resp.StatusCode, string(bodyBytes))
	}

	var anthropicResp anthropicResponse
	if err := json.NewDecoder(resp.Body).Decode(&anthropicResp); err != nil {
		return nil, fmt.Errorf("failed to parse response: %w", err)
	}

	// Extract text from content blocks
	var content string
	for _, block := range anthropicResp.Content {
		if block.Type == "text" {
			content += block.Text
		}
	}

	return &CompletionResponse{
		Content:      content,
		Model:        anthropicResp.Model,
		FinishReason: anthropicResp.StopReason,
		Duration:     time.Since(start),
		Usage: TokenUsage{
			PromptTokens:     anthropicResp.Usage.InputTokens,
			CompletionTokens: anthropicResp.Usage.OutputTokens,
			TotalTokens:      anthropicResp.Usage.InputTokens + anthropicResp.Usage.OutputTokens,
		},
	}, nil
}

// Embed generates an embedding vector.
// Note: Anthropic doesn't provide an embeddings API, so this returns an error.
// Use a different provider (Ollama, OpenAI) for embeddings.
func (c *AnthropicClient) Embed(ctx context.Context, req *EmbeddingRequest) (*EmbeddingResponse, error) {
	return nil, fmt.Errorf("anthropic does not provide an embeddings API; use ollama or openai for embeddings")
}
</file>
<file path="analyze/client.go">
// Package analyze provides LLM integration for code analysis and summarization.
package analyze

import (
	"context"
	"errors"
	"time"
)

// Common errors returned by LLM clients.
var (
	ErrNoAPIKey       = errors.New("API key not configured")
	ErrRateLimited    = errors.New("rate limited by provider")
	ErrModelNotFound  = errors.New("model not found")
	ErrContextTooLong = errors.New("context exceeds model limit")
	ErrTimeout        = errors.New("request timed out")
)

// Message represents a chat message in a conversation.
type Message struct {
	Role    string // "system", "user", or "assistant"
	Content string
}

// CompletionRequest holds parameters for a completion request.
type CompletionRequest struct {
	// Messages is the conversation history
	Messages []Message

	// Model overrides the default model (optional)
	Model string

	// Temperature controls randomness (0.0 = deterministic, 1.0 = creative)
	Temperature float64

	// MaxTokens limits the response length
	MaxTokens int

	// Stop sequences that will halt generation
	Stop []string
}

// CompletionResponse holds the result of a completion request.
type CompletionResponse struct {
	// Content is the generated text
	Content string

	// Model is the model that was used
	Model string

	// Usage contains token counts
	Usage TokenUsage

	// FinishReason indicates why generation stopped
	FinishReason string

	// Duration is how long the request took
	Duration time.Duration
}

// EmbeddingRequest holds parameters for an embedding request.
type EmbeddingRequest struct {
	// Text is the content to embed
	Text string

	// Model overrides the default embedding model (optional)
	Model string
}

// EmbeddingResponse holds the result of an embedding request.
type EmbeddingResponse struct {
	// Embedding is the vector representation
	Embedding []float64

	// Model is the model that was used
	Model string

	// Usage contains token counts
	Usage TokenUsage

	// Duration is how long the request took
	Duration time.Duration
}

// TokenUsage tracks token consumption.
type TokenUsage struct {
	PromptTokens     int
	CompletionTokens int
	TotalTokens      int
}

// LLMClient defines the interface for interacting with LLM providers.
type LLMClient interface {
	// Complete generates a completion for the given messages.
	Complete(ctx context.Context, req *CompletionRequest) (*CompletionResponse, error)

	// Embed generates an embedding vector for the given text.
	Embed(ctx context.Context, req *EmbeddingRequest) (*EmbeddingResponse, error)

	// Models returns a list of available models.
	Models(ctx context.Context) ([]string, error)

	// Ping checks if the provider is reachable.
	Ping(ctx context.Context) error

	// Name returns the provider name.
	Name() string
}

// ClientConfig holds common configuration for LLM clients.
type ClientConfig struct {
	// Model is the default model to use
	Model string

	// EmbeddingModel is the default model for embeddings
	EmbeddingModel string

	// Timeout is the request timeout
	Timeout time.Duration

	// MaxRetries is the maximum number of retry attempts
	MaxRetries int

	// Temperature is the default temperature
	Temperature float64

	// MaxTokens is the default max tokens
	MaxTokens int

	// Debug enables verbose logging
	Debug bool
}

// DefaultClientConfig returns a configuration with sensible defaults.
func DefaultClientConfig() ClientConfig {
	return ClientConfig{
		Timeout:     60 * time.Second,
		MaxRetries:  3,
		Temperature: 0.1,
		MaxTokens:   2048,
		Debug:       false,
	}
}
</file>
<file path="analyze/embed.go">
package analyze

import (
	"context"
	"fmt"
	"strings"
	"time"

	"codemap/graph"
)

// EmbeddingConfig configures the embedding pipeline
type EmbeddingConfig struct {
	// BatchSize is the number of nodes to embed per batch
	BatchSize int

	// MaxRetries is the number of retries for failed embeddings
	MaxRetries int

	// RetryDelay is the initial delay between retries
	RetryDelay time.Duration

	// ProgressFn is called after each batch with (completed, total)
	ProgressFn func(completed, total int)

	// SkipExisting skips nodes that already have embeddings
	SkipExisting bool
}

// DefaultEmbeddingConfig returns sensible defaults
func DefaultEmbeddingConfig() EmbeddingConfig {
	return EmbeddingConfig{
		BatchSize:    10,
		MaxRetries:   3,
		RetryDelay:   time.Second,
		SkipExisting: true,
	}
}

// EmbeddingStats tracks embedding generation statistics
type EmbeddingStats struct {
	Total     int
	Embedded  int
	Skipped   int
	Failed    int
	Duration  time.Duration
	TokensIn  int
	TokensOut int
}

// NodeToText converts a graph node to text suitable for embedding.
// The strategy is: Signature + DocString + Path (for context)
func NodeToText(node *graph.Node) string {
	var parts []string

	// Add kind and name for context
	kindStr := node.Kind.String()
	if node.Name != "" {
		parts = append(parts, fmt.Sprintf("%s: %s", kindStr, node.Name))
	}

	// Add signature if available (most important for functions/methods)
	if node.Signature != "" {
		parts = append(parts, fmt.Sprintf("Signature: %s", node.Signature))
	}

	// Add docstring if available
	if node.DocString != "" {
		// Truncate long docstrings
		doc := node.DocString
		if len(doc) > 500 {
			doc = doc[:500] + "..."
		}
		parts = append(parts, fmt.Sprintf("Description: %s", doc))
	}

	// Add path for file context
	if node.Path != "" {
		parts = append(parts, fmt.Sprintf("Location: %s", node.Path))
	}

	return strings.Join(parts, "\n")
}

// NodeToTextWithSource converts a node to text including source code.
// This provides richer embeddings but uses more tokens.
func NodeToTextWithSource(projectRoot string, node *graph.Node) string {
	baseText := NodeToText(node)

	// Try to read source code
	source, err := ReadSymbolSource(projectRoot, node)
	if err != nil || source == nil {
		return baseText
	}

	// Truncate source if too long
	code := source.Source
	if len(code) > 1000 {
		code = code[:1000] + "\n// ... truncated"
	}

	return fmt.Sprintf("%s\n\nCode:\n```%s\n%s\n```", baseText, source.Language, code)
}

// EmbedNodes generates embeddings for a slice of nodes.
// Returns a map of NodeID -> embedding vector.
func EmbedNodes(
	ctx context.Context,
	client LLMClient,
	nodes []*graph.Node,
	config EmbeddingConfig,
) (map[graph.NodeID][]float64, *EmbeddingStats, error) {
	stats := &EmbeddingStats{
		Total: len(nodes),
	}
	start := time.Now()

	if len(nodes) == 0 {
		stats.Duration = time.Since(start)
		return nil, stats, nil
	}

	// Prepare texts for embedding
	type nodeText struct {
		node *graph.Node
		text string
	}
	nodesToEmbed := make([]nodeText, 0, len(nodes))

	for _, node := range nodes {
		text := NodeToText(node)
		if text == "" {
			stats.Skipped++
			continue
		}
		nodesToEmbed = append(nodesToEmbed, nodeText{node: node, text: text})
	}

	if len(nodesToEmbed) == 0 {
		stats.Duration = time.Since(start)
		return nil, stats, nil
	}

	results := make(map[graph.NodeID][]float64)
	batchSize := config.BatchSize
	if batchSize <= 0 {
		batchSize = 10
	}

	// Process in batches
	for i := 0; i < len(nodesToEmbed); i += batchSize {
		end := i + batchSize
		if end > len(nodesToEmbed) {
			end = len(nodesToEmbed)
		}
		batch := nodesToEmbed[i:end]

		// Embed each item in batch (most APIs don't support batch embedding)
		for _, item := range batch {
			// Check context cancellation
			select {
			case <-ctx.Done():
				stats.Duration = time.Since(start)
				return results, stats, ctx.Err()
			default:
			}

			// Embed with retries
			var embedding []float64
			var err error
			for attempt := 0; attempt <= config.MaxRetries; attempt++ {
				resp, embedErr := client.Embed(ctx, &EmbeddingRequest{
					Text: item.text,
				})
				if embedErr == nil {
					embedding = resp.Embedding
					stats.TokensIn += resp.Usage.PromptTokens
					break
				}
				err = embedErr

				if attempt < config.MaxRetries {
					delay := config.RetryDelay * time.Duration(1<<attempt)
					select {
					case <-ctx.Done():
						stats.Duration = time.Since(start)
						return results, stats, ctx.Err()
					case <-time.After(delay):
					}
				}
			}

			if embedding != nil {
				results[item.node.ID] = embedding
				stats.Embedded++
			} else {
				stats.Failed++
				// Log error but continue with other nodes
				if err != nil {
					fmt.Printf("Warning: failed to embed %s: %v\n", item.node.Name, err)
				}
			}
		}

		// Report progress
		if config.ProgressFn != nil {
			config.ProgressFn(stats.Embedded+stats.Failed+stats.Skipped, stats.Total)
		}
	}

	stats.Duration = time.Since(start)
	return results, stats, nil
}

// EmbedGraph generates embeddings for all function/method nodes in a graph.
// It updates the vector index in place.
func EmbedGraph(
	ctx context.Context,
	client LLMClient,
	cg *graph.CodeGraph,
	index *graph.InMemoryVectorIndex,
	config EmbeddingConfig,
) (*EmbeddingStats, error) {
	// Collect nodes to embed (functions and methods are most useful)
	var nodes []*graph.Node
	for _, node := range cg.Nodes {
		// Skip if already has embedding and config says to skip
		if config.SkipExisting && index.Has(node.ID) {
			continue
		}

		// Only embed functions, methods, and types for now
		switch node.Kind {
		case graph.KindFunction, graph.KindMethod, graph.KindType:
			nodes = append(nodes, node)
		}
	}

	if len(nodes) == 0 {
		return &EmbeddingStats{Total: 0, Skipped: len(cg.Nodes)}, nil
	}

	// Generate embeddings
	embeddings, stats, err := EmbedNodes(ctx, client, nodes, config)
	if err != nil {
		return stats, err
	}

	// Add to index
	for _, node := range nodes {
		if embedding, ok := embeddings[node.ID]; ok {
			text := NodeToText(node)
			if err := index.Add(node.ID, embedding, text); err != nil {
				return stats, fmt.Errorf("adding to index: %w", err)
			}
		}
	}

	return stats, nil
}

// EmbedQuery generates an embedding for a search query
func EmbedQuery(ctx context.Context, client LLMClient, query string) ([]float64, error) {
	resp, err := client.Embed(ctx, &EmbeddingRequest{
		Text: query,
	})
	if err != nil {
		return nil, fmt.Errorf("embedding query: %w", err)
	}
	return resp.Embedding, nil
}
</file>
<file path="analyze/factory.go">
package analyze

import (
	"fmt"
	"time"

	"codemap/config"
)

// NewClient creates an LLMClient based on the configuration.
func NewClient(cfg *config.Config) (LLMClient, error) {
	timeout := time.Duration(cfg.LLM.Timeout) * time.Second
	if timeout == 0 {
		timeout = DefaultClientConfig().Timeout
	}

	clientCfg := ClientConfig{
		Model:          cfg.LLM.Model,
		EmbeddingModel: cfg.LLM.EmbeddingModel,
		Timeout:        timeout,
		MaxRetries:     cfg.LLM.MaxRetries,
		Temperature:    cfg.LLM.Temperature,
		MaxTokens:      cfg.LLM.MaxTokens,
		Debug:          cfg.Debug,
	}

	switch cfg.LLM.Provider {
	case config.ProviderOllama:
		return NewOllamaClient(cfg.LLM.OllamaURL, clientCfg), nil

	case config.ProviderOpenAI:
		return NewOpenAIClient(cfg.LLM.OpenAIAPIKey, cfg.LLM.OpenAIBaseURL, clientCfg)

	case config.ProviderAnthropic:
		return NewAnthropicClient(cfg.LLM.AnthropicAPIKey, clientCfg)

	case config.ProviderGemini:
		return NewGeminiClient(cfg.LLM.GeminiAPIKey, cfg.LLM.GeminiBaseURL, clientCfg)

	case "mock":
		return NewMockClient(clientCfg), nil

	default:
		return nil, fmt.Errorf("unknown provider: %s", cfg.LLM.Provider)
	}
}

// NewEmbeddingClient creates a client suitable for embedding operations.
// If the main provider doesn't support embeddings (e.g., Anthropic),
// it falls back to Ollama.
func NewEmbeddingClient(cfg *config.Config) (LLMClient, error) {
	// Check if embedding provider is explicitly set
	if cfg.LLM.EmbeddingProvider != "" {
		// Create a modified config with the embedding provider
		embCfg := *cfg
		embCfg.LLM.Provider = config.Provider(cfg.LLM.EmbeddingProvider)
		return NewClient(&embCfg)
	}

	// If using Anthropic (which doesn't have embeddings), default to Ollama
	if cfg.LLM.Provider == config.ProviderAnthropic {
		clientCfg := ClientConfig{
			EmbeddingModel: cfg.LLM.EmbeddingModel,
			Timeout:        DefaultClientConfig().Timeout,
			Debug:          cfg.Debug,
		}
		return NewOllamaClient(cfg.LLM.OllamaURL, clientCfg), nil
	}

	// Otherwise use the main provider
	return NewClient(cfg)
}
</file>
<file path="analyze/gemini.go">
package analyze

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"time"
)

// GeminiClient implements LLMClient for Google's Gemini API.
type GeminiClient struct {
	apiKey     string
	baseURL    string
	httpClient *http.Client
	config     ClientConfig
}

// NewGeminiClient creates a new Gemini client.
func NewGeminiClient(apiKey, baseURL string, cfg ClientConfig) (*GeminiClient, error) {
	if apiKey == "" {
		return nil, ErrNoAPIKey
	}
	if baseURL == "" {
		baseURL = "https://generativelanguage.googleapis.com/v1beta"
	}
	if cfg.Model == "" {
		cfg.Model = "gemini-2.0-flash"
	}
	if cfg.EmbeddingModel == "" {
		cfg.EmbeddingModel = "gemini-embedding-001"
	}
	if cfg.Timeout == 0 {
		cfg.Timeout = 60 * time.Second
	}

	return &GeminiClient{
		apiKey:  apiKey,
		baseURL: baseURL,
		httpClient: &http.Client{
			Timeout: cfg.Timeout,
		},
		config: cfg,
	}, nil
}

// Name returns the provider name.
func (c *GeminiClient) Name() string {
	return "gemini"
}

// Ping checks if the Gemini API is reachable.
func (c *GeminiClient) Ping(ctx context.Context) error {
	req, err := http.NewRequestWithContext(ctx, "GET", c.baseURL+"/models", nil)
	if err != nil {
		return err
	}
	req.Header.Set("x-goog-api-key", c.apiKey)

	resp, err := c.httpClient.Do(req)
	if err != nil {
		return fmt.Errorf("gemini not reachable: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode == http.StatusUnauthorized || resp.StatusCode == http.StatusForbidden {
		return ErrNoAPIKey
	}
	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("gemini returned status %d", resp.StatusCode)
	}

	return nil
}

// Models returns a list of available Gemini models.
func (c *GeminiClient) Models(ctx context.Context) ([]string, error) {
	return []string{
		// Completion models
		"gemini-2.5-flash",
		"gemini-2.5-pro",
		"gemini-2.0-flash",
		"gemini-2.0-flash-exp",
		// Embedding models
		"gemini-embedding-001",
	}, nil
}

// geminiPart represents a content part in Gemini API.
type geminiPart struct {
	Text string `json:"text"`
}

// geminiContent represents a message in Gemini API.
type geminiContent struct {
	Role  string       `json:"role,omitempty"`
	Parts []geminiPart `json:"parts"`
}

// geminiGenerationConfig holds generation parameters.
type geminiGenerationConfig struct {
	Temperature     float64  `json:"temperature,omitempty"`
	MaxOutputTokens int      `json:"maxOutputTokens,omitempty"`
	StopSequences   []string `json:"stopSequences,omitempty"`
}

// geminiRequest is the request format for Gemini's generateContent API.
type geminiRequest struct {
	SystemInstruction *geminiContent          `json:"systemInstruction,omitempty"`
	Contents          []geminiContent         `json:"contents"`
	GenerationConfig  *geminiGenerationConfig `json:"generationConfig,omitempty"`
}

// geminiResponse is the response format from Gemini's generateContent API.
type geminiResponse struct {
	Candidates []struct {
		Content      geminiContent `json:"content"`
		FinishReason string        `json:"finishReason"`
		Index        int           `json:"index"`
	} `json:"candidates"`
	UsageMetadata struct {
		PromptTokenCount     int `json:"promptTokenCount"`
		CandidatesTokenCount int `json:"candidatesTokenCount"`
		TotalTokenCount      int `json:"totalTokenCount"`
	} `json:"usageMetadata"`
	ModelVersion string `json:"modelVersion"`
}

// geminiErrorResponse is the error format from Gemini API.
type geminiErrorResponse struct {
	Error struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
		Status  string `json:"status"`
	} `json:"error"`
}

// geminiEmbedRequest is the request format for Gemini's embedContent API.
// For gemini-embedding-001, supports output_dimensionality to control vector size.
type geminiEmbedRequest struct {
	Content              geminiContent `json:"content"`
	OutputDimensionality int           `json:"output_dimensionality,omitempty"` // 768, 1536, or 3072 (default)
	TaskType             string        `json:"task_type,omitempty"`             // SEMANTIC_SIMILARITY, RETRIEVAL_DOCUMENT, etc.
}

// Embedding dimension constants for gemini-embedding-001
const (
	GeminiEmbedDim768  = 768  // Compatible with most other providers
	GeminiEmbedDim1536 = 1536 // Medium quality/size tradeoff
	GeminiEmbedDim3072 = 3072 // Full quality (default for gemini-embedding-001)
)

// geminiEmbedResponse is the response format from Gemini's embedContent API.
type geminiEmbedResponse struct {
	Embedding struct {
		Values []float64 `json:"values"`
	} `json:"embedding"`
}

// Complete generates a completion using Gemini's generateContent API.
func (c *GeminiClient) Complete(ctx context.Context, req *CompletionRequest) (*CompletionResponse, error) {
	start := time.Now()

	model := req.Model
	if model == "" {
		model = c.config.Model
	}

	temp := req.Temperature
	if temp == 0 {
		temp = c.config.Temperature
	}

	maxTokens := req.MaxTokens
	if maxTokens == 0 {
		maxTokens = c.config.MaxTokens
	}

	// Convert messages to Gemini format
	// Extract system messages into systemInstruction
	var systemInstruction *geminiContent
	contents := make([]geminiContent, 0, len(req.Messages))

	for _, m := range req.Messages {
		switch m.Role {
		case "system":
			// Gemini uses a separate systemInstruction field
			if systemInstruction == nil {
				systemInstruction = &geminiContent{
					Parts: []geminiPart{{Text: m.Content}},
				}
			} else {
				// Append to existing system instruction
				systemInstruction.Parts = append(systemInstruction.Parts, geminiPart{Text: m.Content})
			}
		case "assistant":
			// Gemini uses "model" instead of "assistant"
			contents = append(contents, geminiContent{
				Role:  "model",
				Parts: []geminiPart{{Text: m.Content}},
			})
		default:
			// "user" role stays the same
			contents = append(contents, geminiContent{
				Role:  m.Role,
				Parts: []geminiPart{{Text: m.Content}},
			})
		}
	}

	geminiReq := geminiRequest{
		SystemInstruction: systemInstruction,
		Contents:          contents,
		GenerationConfig: &geminiGenerationConfig{
			Temperature:     temp,
			MaxOutputTokens: maxTokens,
			StopSequences:   req.Stop,
		},
	}

	body, err := json.Marshal(geminiReq)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	endpoint := fmt.Sprintf("%s/models/%s:generateContent", c.baseURL, model)

	// Retry logic with exponential backoff
	var resp *http.Response
	var lastErr error
	for attempt := 0; attempt <= c.config.MaxRetries; attempt++ {
		if attempt > 0 {
			time.Sleep(time.Duration(attempt*attempt) * time.Second)
		}

		// Check context before each attempt
		if ctx.Err() != nil {
			return nil, ErrTimeout
		}

		// Create new request for each attempt (body reader is consumed after each request)
		httpReq, err := http.NewRequestWithContext(ctx, "POST", endpoint, bytes.NewReader(body))
		if err != nil {
			return nil, err
		}
		httpReq.Header.Set("Content-Type", "application/json")
		httpReq.Header.Set("x-goog-api-key", c.apiKey)

		resp, lastErr = c.httpClient.Do(httpReq)
		if lastErr != nil {
			// Network error - retry
			if ctx.Err() != nil {
				return nil, ErrTimeout
			}
			continue
		}

		// Check response status
		switch {
		case resp.StatusCode == http.StatusOK:
			// Success - break out of retry loop
			lastErr = nil
		case resp.StatusCode == http.StatusNotFound:
			// Model not found - don't retry
			resp.Body.Close()
			return nil, ErrModelNotFound
		case resp.StatusCode == http.StatusTooManyRequests:
			// Rate limited - close body and retry
			resp.Body.Close()
			lastErr = ErrRateLimited
			continue
		case resp.StatusCode >= 500:
			// Server error - close body and retry
			resp.Body.Close()
			lastErr = fmt.Errorf("server error: %d", resp.StatusCode)
			continue
		default:
			// Client error (4xx) - don't retry, break to handle error with body
			lastErr = nil
		}
		break
	}

	if lastErr != nil {
		return nil, fmt.Errorf("request failed after %d retries: %w", c.config.MaxRetries, lastErr)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		bodyBytes, _ := io.ReadAll(resp.Body)
		var errResp geminiErrorResponse
		if json.Unmarshal(bodyBytes, &errResp) == nil && errResp.Error.Message != "" {
			return nil, fmt.Errorf("gemini error: %s", errResp.Error.Message)
		}
		return nil, fmt.Errorf("gemini error (status %d): %s", resp.StatusCode, string(bodyBytes))
	}

	var geminiResp geminiResponse
	if err := json.NewDecoder(resp.Body).Decode(&geminiResp); err != nil {
		return nil, fmt.Errorf("failed to parse response: %w", err)
	}

	if len(geminiResp.Candidates) == 0 {
		return nil, fmt.Errorf("no candidates returned")
	}

	// Extract text from parts
	var content string
	for _, part := range geminiResp.Candidates[0].Content.Parts {
		content += part.Text
	}

	// Map finish reason
	finishReason := mapGeminiFinishReason(geminiResp.Candidates[0].FinishReason)

	return &CompletionResponse{
		Content:      content,
		Model:        geminiResp.ModelVersion,
		FinishReason: finishReason,
		Duration:     time.Since(start),
		Usage: TokenUsage{
			PromptTokens:     geminiResp.UsageMetadata.PromptTokenCount,
			CompletionTokens: geminiResp.UsageMetadata.CandidatesTokenCount,
			TotalTokens:      geminiResp.UsageMetadata.TotalTokenCount,
		},
	}, nil
}

// mapGeminiFinishReason converts Gemini finish reasons to standard format.
func mapGeminiFinishReason(reason string) string {
	switch reason {
	case "STOP":
		return "stop"
	case "MAX_TOKENS":
		return "length"
	case "SAFETY", "RECITATION":
		return "content_filter"
	default:
		return "other"
	}
}

// Embed generates an embedding vector using Gemini's embedContent API.
func (c *GeminiClient) Embed(ctx context.Context, req *EmbeddingRequest) (*EmbeddingResponse, error) {
	start := time.Now()

	model := req.Model
	if model == "" {
		model = c.config.EmbeddingModel
	}

	geminiReq := geminiEmbedRequest{
		Content: geminiContent{
			Parts: []geminiPart{{Text: req.Text}},
		},
		// Use 768 dimensions for compatibility with other providers (OpenAI, Ollama)
		// gemini-embedding-001 defaults to 3072 but supports truncation
		OutputDimensionality: GeminiEmbedDim768,
		TaskType:             "SEMANTIC_SIMILARITY",
	}

	body, err := json.Marshal(geminiReq)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	endpoint := fmt.Sprintf("%s/models/%s:embedContent", c.baseURL, model)
	httpReq, err := http.NewRequestWithContext(ctx, "POST", endpoint, bytes.NewReader(body))
	if err != nil {
		return nil, err
	}
	httpReq.Header.Set("Content-Type", "application/json")
	httpReq.Header.Set("x-goog-api-key", c.apiKey)

	resp, err := c.httpClient.Do(httpReq)
	if err != nil {
		return nil, fmt.Errorf("embed request failed: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		if resp.StatusCode == http.StatusNotFound {
			return nil, ErrModelNotFound
		}
		bodyBytes, _ := io.ReadAll(resp.Body)
		return nil, fmt.Errorf("gemini embed error (status %d): %s", resp.StatusCode, string(bodyBytes))
	}

	var geminiResp geminiEmbedResponse
	if err := json.NewDecoder(resp.Body).Decode(&geminiResp); err != nil {
		return nil, fmt.Errorf("failed to parse response: %w", err)
	}

	if len(geminiResp.Embedding.Values) == 0 {
		return nil, fmt.Errorf("no embedding returned")
	}

	return &EmbeddingResponse{
		Embedding: geminiResp.Embedding.Values,
		Model:     model,
		Duration:  time.Since(start),
		// Note: Gemini embed endpoint doesn't return token usage
	}, nil
}
</file>
<file path="analyze/mock.go">
package analyze

import (
	"context"
	"fmt"
	"time"
)

// MockClient is a mock LLM client for testing.
type MockClient struct {
	// Responses maps prompt prefixes to responses
	Responses map[string]string

	// DefaultResponse is returned when no matching response is found
	DefaultResponse string

	// EmbeddingDimension is the dimension of mock embeddings
	EmbeddingDimension int

	// SimulateLatency adds artificial latency to simulate real requests
	SimulateLatency time.Duration

	// Error can be set to simulate errors
	Error error

	// RecordedRequests stores all requests for verification
	RecordedRequests []MockRequest

	config ClientConfig
}

// MockRequest records a request for testing verification.
type MockRequest struct {
	Type     string // "complete" or "embed"
	Messages []Message
	Text     string
}

// NewMockClient creates a new mock client for testing.
func NewMockClient(cfg ClientConfig) *MockClient {
	return &MockClient{
		Responses: make(map[string]string),
		DefaultResponse: `This is a mock response for testing.

**Purpose**: The code performs its intended functionality.

**Key Logic**: The implementation follows standard patterns.

**Parameters**: Accepts typical inputs and produces expected outputs.`,
		EmbeddingDimension: 768, // Common embedding dimension
		config:             cfg,
	}
}

// WithResponse adds a response mapping for a prompt prefix.
func (c *MockClient) WithResponse(promptPrefix, response string) *MockClient {
	c.Responses[promptPrefix] = response
	return c
}

// WithError configures the mock to return an error.
func (c *MockClient) WithError(err error) *MockClient {
	c.Error = err
	return c
}

// WithLatency configures simulated latency.
func (c *MockClient) WithLatency(d time.Duration) *MockClient {
	c.SimulateLatency = d
	return c
}

// Name returns the provider name.
func (c *MockClient) Name() string {
	return "mock"
}

// Ping always succeeds for mock client.
func (c *MockClient) Ping(ctx context.Context) error {
	if c.Error != nil {
		return c.Error
	}
	return nil
}

// Models returns mock models.
func (c *MockClient) Models(ctx context.Context) ([]string, error) {
	return []string{"mock-model", "mock-embed"}, nil
}

// Complete generates a mock completion.
func (c *MockClient) Complete(ctx context.Context, req *CompletionRequest) (*CompletionResponse, error) {
	start := time.Now()

	// Record request
	c.RecordedRequests = append(c.RecordedRequests, MockRequest{
		Type:     "complete",
		Messages: req.Messages,
	})

	// Check for configured error
	if c.Error != nil {
		return nil, c.Error
	}

	// Simulate latency
	if c.SimulateLatency > 0 {
		select {
		case <-time.After(c.SimulateLatency):
		case <-ctx.Done():
			return nil, ErrTimeout
		}
	}

	// Find matching response
	response := c.DefaultResponse
	for prefix, resp := range c.Responses {
		for _, msg := range req.Messages {
			if len(msg.Content) >= len(prefix) && msg.Content[:len(prefix)] == prefix {
				response = resp
				break
			}
		}
	}

	// Estimate tokens
	promptTokens := 0
	for _, msg := range req.Messages {
		promptTokens += EstimateTokens(msg.Content)
	}
	completionTokens := EstimateTokens(response)

	return &CompletionResponse{
		Content:      response,
		Model:        "mock-model",
		FinishReason: "stop",
		Duration:     time.Since(start),
		Usage: TokenUsage{
			PromptTokens:     promptTokens,
			CompletionTokens: completionTokens,
			TotalTokens:      promptTokens + completionTokens,
		},
	}, nil
}

// Embed generates a mock embedding vector.
func (c *MockClient) Embed(ctx context.Context, req *EmbeddingRequest) (*EmbeddingResponse, error) {
	start := time.Now()

	// Record request
	c.RecordedRequests = append(c.RecordedRequests, MockRequest{
		Type: "embed",
		Text: req.Text,
	})

	// Check for configured error
	if c.Error != nil {
		return nil, c.Error
	}

	// Simulate latency
	if c.SimulateLatency > 0 {
		select {
		case <-time.After(c.SimulateLatency):
		case <-ctx.Done():
			return nil, ErrTimeout
		}
	}

	// Generate deterministic mock embedding based on text content
	embedding := make([]float64, c.EmbeddingDimension)
	for i := range embedding {
		// Simple hash-based value for deterministic results
		h := 0
		for j, r := range req.Text {
			h += int(r) * (i + j + 1)
		}
		embedding[i] = float64(h%1000) / 1000.0
	}

	return &EmbeddingResponse{
		Embedding: embedding,
		Model:     "mock-embed",
		Duration:  time.Since(start),
		Usage: TokenUsage{
			PromptTokens: EstimateTokens(req.Text),
			TotalTokens:  EstimateTokens(req.Text),
		},
	}, nil
}

// GetRequests returns all recorded requests.
func (c *MockClient) GetRequests() []MockRequest {
	return c.RecordedRequests
}

// ClearRequests clears recorded requests.
func (c *MockClient) ClearRequests() {
	c.RecordedRequests = nil
}

// AssertCalled checks if a specific type of request was made.
func (c *MockClient) AssertCalled(requestType string) error {
	for _, req := range c.RecordedRequests {
		if req.Type == requestType {
			return nil
		}
	}
	return fmt.Errorf("no %s request was made", requestType)
}

// RequestCount returns the number of requests of a given type.
func (c *MockClient) RequestCount(requestType string) int {
	count := 0
	for _, req := range c.RecordedRequests {
		if req.Type == requestType {
			count++
		}
	}
	return count
}
</file>
<file path="analyze/ollama.go">
package analyze

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"time"
)

// OllamaClient implements LLMClient for Ollama local inference.
type OllamaClient struct {
	baseURL    string
	httpClient *http.Client
	config     ClientConfig
}

// NewOllamaClient creates a new Ollama client.
func NewOllamaClient(baseURL string, cfg ClientConfig) *OllamaClient {
	if baseURL == "" {
		baseURL = "http://localhost:11434"
	}
	if cfg.Model == "" {
		cfg.Model = "llama3"
	}
	if cfg.EmbeddingModel == "" {
		cfg.EmbeddingModel = "nomic-embed-text"
	}
	if cfg.Timeout == 0 {
		cfg.Timeout = 60 * time.Second
	}

	return &OllamaClient{
		baseURL: baseURL,
		httpClient: &http.Client{
			Timeout: cfg.Timeout,
		},
		config: cfg,
	}
}

// Name returns the provider name.
func (c *OllamaClient) Name() string {
	return "ollama"
}

// Ping checks if Ollama is reachable.
func (c *OllamaClient) Ping(ctx context.Context) error {
	req, err := http.NewRequestWithContext(ctx, "GET", c.baseURL+"/api/tags", nil)
	if err != nil {
		return err
	}

	resp, err := c.httpClient.Do(req)
	if err != nil {
		return fmt.Errorf("ollama not reachable at %s: %w", c.baseURL, err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("ollama returned status %d", resp.StatusCode)
	}

	return nil
}

// Models returns a list of available models.
func (c *OllamaClient) Models(ctx context.Context) ([]string, error) {
	req, err := http.NewRequestWithContext(ctx, "GET", c.baseURL+"/api/tags", nil)
	if err != nil {
		return nil, err
	}

	resp, err := c.httpClient.Do(req)
	if err != nil {
		return nil, fmt.Errorf("failed to list models: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return nil, fmt.Errorf("ollama returned status %d", resp.StatusCode)
	}

	var result struct {
		Models []struct {
			Name string `json:"name"`
		} `json:"models"`
	}

	if err := json.NewDecoder(resp.Body).Decode(&result); err != nil {
		return nil, fmt.Errorf("failed to parse response: %w", err)
	}

	models := make([]string, len(result.Models))
	for i, m := range result.Models {
		models[i] = m.Name
	}

	return models, nil
}

// ollamaChatRequest is the request format for Ollama's chat API.
type ollamaChatRequest struct {
	Model    string          `json:"model"`
	Messages []ollamaMessage `json:"messages"`
	Stream   bool            `json:"stream"`
	Options  ollamaOptions   `json:"options,omitempty"`
}

type ollamaMessage struct {
	Role    string `json:"role"`
	Content string `json:"content"`
}

type ollamaOptions struct {
	Temperature float64  `json:"temperature,omitempty"`
	NumPredict  int      `json:"num_predict,omitempty"`
	Stop        []string `json:"stop,omitempty"`
}

// ollamaChatResponse is the response format from Ollama's chat API.
type ollamaChatResponse struct {
	Model     string `json:"model"`
	CreatedAt string `json:"created_at"`
	Message   struct {
		Role    string `json:"role"`
		Content string `json:"content"`
	} `json:"message"`
	Done            bool   `json:"done"`
	DoneReason      string `json:"done_reason,omitempty"`
	TotalDuration   int64  `json:"total_duration,omitempty"`
	PromptEvalCount int    `json:"prompt_eval_count,omitempty"`
	EvalCount       int    `json:"eval_count,omitempty"`
}

// Complete generates a completion using Ollama's chat API.
func (c *OllamaClient) Complete(ctx context.Context, req *CompletionRequest) (*CompletionResponse, error) {
	start := time.Now()

	model := req.Model
	if model == "" {
		model = c.config.Model
	}

	temp := req.Temperature
	if temp == 0 {
		temp = c.config.Temperature
	}

	maxTokens := req.MaxTokens
	if maxTokens == 0 {
		maxTokens = c.config.MaxTokens
	}

	// Convert messages to Ollama format
	messages := make([]ollamaMessage, len(req.Messages))
	for i, m := range req.Messages {
		messages[i] = ollamaMessage{
			Role:    m.Role,
			Content: m.Content,
		}
	}

	ollamaReq := ollamaChatRequest{
		Model:    model,
		Messages: messages,
		Stream:   false,
		Options: ollamaOptions{
			Temperature: temp,
			NumPredict:  maxTokens,
			Stop:        req.Stop,
		},
	}

	body, err := json.Marshal(ollamaReq)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	// Retry logic with exponential backoff
	var resp *http.Response
	var lastErr error
	for attempt := 0; attempt <= c.config.MaxRetries; attempt++ {
		if attempt > 0 {
			time.Sleep(time.Duration(attempt*attempt) * time.Second)
		}

		// Check context before each attempt
		if ctx.Err() != nil {
			return nil, ErrTimeout
		}

		// Create new request for each attempt (body reader is consumed after each request)
		httpReq, err := http.NewRequestWithContext(ctx, "POST", c.baseURL+"/api/chat", bytes.NewReader(body))
		if err != nil {
			return nil, err
		}
		httpReq.Header.Set("Content-Type", "application/json")

		resp, lastErr = c.httpClient.Do(httpReq)
		if lastErr != nil {
			// Network error - retry
			if ctx.Err() != nil {
				return nil, ErrTimeout
			}
			continue
		}

		// Check response status
		switch {
		case resp.StatusCode == http.StatusOK:
			// Success - break out of retry loop
			lastErr = nil
		case resp.StatusCode == http.StatusTooManyRequests:
			// Rate limited - close body and retry
			resp.Body.Close()
			lastErr = ErrRateLimited
			continue
		case resp.StatusCode >= 500:
			// Server error - close body and retry
			resp.Body.Close()
			lastErr = fmt.Errorf("server error: %d", resp.StatusCode)
			continue
		default:
			// Client error (4xx) - don't retry, break to handle error with body
			lastErr = nil
		}
		break
	}

	if lastErr != nil {
		return nil, fmt.Errorf("request failed after %d retries: %w", c.config.MaxRetries, lastErr)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return nil, fmt.Errorf("ollama error (status %d): %s", resp.StatusCode, string(body))
	}

	var ollamaResp ollamaChatResponse
	if err := json.NewDecoder(resp.Body).Decode(&ollamaResp); err != nil {
		return nil, fmt.Errorf("failed to parse response: %w", err)
	}

	return &CompletionResponse{
		Content:      ollamaResp.Message.Content,
		Model:        ollamaResp.Model,
		FinishReason: ollamaResp.DoneReason,
		Duration:     time.Since(start),
		Usage: TokenUsage{
			PromptTokens:     ollamaResp.PromptEvalCount,
			CompletionTokens: ollamaResp.EvalCount,
			TotalTokens:      ollamaResp.PromptEvalCount + ollamaResp.EvalCount,
		},
	}, nil
}

// ollamaEmbedRequest is the request format for Ollama's embed API.
type ollamaEmbedRequest struct {
	Model string `json:"model"`
	Input string `json:"input"`
}

// ollamaEmbedResponse is the response format from Ollama's embed API.
type ollamaEmbedResponse struct {
	Model      string      `json:"model"`
	Embeddings [][]float64 `json:"embeddings"`
}

// Embed generates an embedding vector using Ollama.
func (c *OllamaClient) Embed(ctx context.Context, req *EmbeddingRequest) (*EmbeddingResponse, error) {
	start := time.Now()

	model := req.Model
	if model == "" {
		model = c.config.EmbeddingModel
	}

	ollamaReq := ollamaEmbedRequest{
		Model: model,
		Input: req.Text,
	}

	body, err := json.Marshal(ollamaReq)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	httpReq, err := http.NewRequestWithContext(ctx, "POST", c.baseURL+"/api/embed", bytes.NewReader(body))
	if err != nil {
		return nil, err
	}
	httpReq.Header.Set("Content-Type", "application/json")

	resp, err := c.httpClient.Do(httpReq)
	if err != nil {
		return nil, fmt.Errorf("embed request failed: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return nil, fmt.Errorf("ollama embed error (status %d): %s", resp.StatusCode, string(body))
	}

	var ollamaResp ollamaEmbedResponse
	if err := json.NewDecoder(resp.Body).Decode(&ollamaResp); err != nil {
		return nil, fmt.Errorf("failed to parse response: %w", err)
	}

	if len(ollamaResp.Embeddings) == 0 {
		return nil, fmt.Errorf("no embeddings returned")
	}

	return &EmbeddingResponse{
		Embedding: ollamaResp.Embeddings[0],
		Model:     ollamaResp.Model,
		Duration:  time.Since(start),
	}, nil
}
</file>
<file path="analyze/openai.go">
package analyze

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"time"
)

// OpenAIClient implements LLMClient for OpenAI's API.
type OpenAIClient struct {
	apiKey     string
	baseURL    string
	httpClient *http.Client
	config     ClientConfig
}

// NewOpenAIClient creates a new OpenAI client.
func NewOpenAIClient(apiKey, baseURL string, cfg ClientConfig) (*OpenAIClient, error) {
	if apiKey == "" {
		return nil, ErrNoAPIKey
	}
	if baseURL == "" {
		baseURL = "https://api.openai.com/v1"
	}
	if cfg.Model == "" {
		cfg.Model = "gpt-4o-mini"
	}
	if cfg.EmbeddingModel == "" {
		cfg.EmbeddingModel = "text-embedding-3-small"
	}
	if cfg.Timeout == 0 {
		cfg.Timeout = 60 * time.Second
	}

	return &OpenAIClient{
		apiKey:  apiKey,
		baseURL: baseURL,
		httpClient: &http.Client{
			Timeout: cfg.Timeout,
		},
		config: cfg,
	}, nil
}

// Name returns the provider name.
func (c *OpenAIClient) Name() string {
	return "openai"
}

// Ping checks if OpenAI is reachable.
func (c *OpenAIClient) Ping(ctx context.Context) error {
	req, err := http.NewRequestWithContext(ctx, "GET", c.baseURL+"/models", nil)
	if err != nil {
		return err
	}
	req.Header.Set("Authorization", "Bearer "+c.apiKey)

	resp, err := c.httpClient.Do(req)
	if err != nil {
		return fmt.Errorf("openai not reachable: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode == http.StatusUnauthorized {
		return ErrNoAPIKey
	}
	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("openai returned status %d", resp.StatusCode)
	}

	return nil
}

// Models returns a list of available models.
func (c *OpenAIClient) Models(ctx context.Context) ([]string, error) {
	req, err := http.NewRequestWithContext(ctx, "GET", c.baseURL+"/models", nil)
	if err != nil {
		return nil, err
	}
	req.Header.Set("Authorization", "Bearer "+c.apiKey)

	resp, err := c.httpClient.Do(req)
	if err != nil {
		return nil, fmt.Errorf("failed to list models: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return nil, fmt.Errorf("openai returned status %d", resp.StatusCode)
	}

	var result struct {
		Data []struct {
			ID string `json:"id"`
		} `json:"data"`
	}

	if err := json.NewDecoder(resp.Body).Decode(&result); err != nil {
		return nil, fmt.Errorf("failed to parse response: %w", err)
	}

	models := make([]string, len(result.Data))
	for i, m := range result.Data {
		models[i] = m.ID
	}

	return models, nil
}

// openaiChatRequest is the request format for OpenAI's chat API.
type openaiChatRequest struct {
	Model       string          `json:"model"`
	Messages    []openaiMessage `json:"messages"`
	Temperature float64         `json:"temperature,omitempty"`
	MaxTokens   int             `json:"max_tokens,omitempty"`
	Stop        []string        `json:"stop,omitempty"`
}

type openaiMessage struct {
	Role    string `json:"role"`
	Content string `json:"content"`
}

// openaiChatResponse is the response format from OpenAI's chat API.
type openaiChatResponse struct {
	ID      string `json:"id"`
	Object  string `json:"object"`
	Created int64  `json:"created"`
	Model   string `json:"model"`
	Choices []struct {
		Index   int `json:"index"`
		Message struct {
			Role    string `json:"role"`
			Content string `json:"content"`
		} `json:"message"`
		FinishReason string `json:"finish_reason"`
	} `json:"choices"`
	Usage struct {
		PromptTokens     int `json:"prompt_tokens"`
		CompletionTokens int `json:"completion_tokens"`
		TotalTokens      int `json:"total_tokens"`
	} `json:"usage"`
}

// openaiErrorResponse is the error format from OpenAI.
type openaiErrorResponse struct {
	Error struct {
		Message string `json:"message"`
		Type    string `json:"type"`
		Code    string `json:"code"`
	} `json:"error"`
}

// Complete generates a completion using OpenAI's chat API.
func (c *OpenAIClient) Complete(ctx context.Context, req *CompletionRequest) (*CompletionResponse, error) {
	start := time.Now()

	model := req.Model
	if model == "" {
		model = c.config.Model
	}

	temp := req.Temperature
	if temp == 0 {
		temp = c.config.Temperature
	}

	maxTokens := req.MaxTokens
	if maxTokens == 0 {
		maxTokens = c.config.MaxTokens
	}

	// Convert messages to OpenAI format
	messages := make([]openaiMessage, len(req.Messages))
	for i, m := range req.Messages {
		messages[i] = openaiMessage{
			Role:    m.Role,
			Content: m.Content,
		}
	}

	openaiReq := openaiChatRequest{
		Model:       model,
		Messages:    messages,
		Temperature: temp,
		MaxTokens:   maxTokens,
		Stop:        req.Stop,
	}

	body, err := json.Marshal(openaiReq)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	// Retry logic with exponential backoff
	var resp *http.Response
	var lastErr error
	for attempt := 0; attempt <= c.config.MaxRetries; attempt++ {
		if attempt > 0 {
			time.Sleep(time.Duration(attempt*attempt) * time.Second)
		}

		// Check context before each attempt
		if ctx.Err() != nil {
			return nil, ErrTimeout
		}

		// Create new request for each attempt (body reader is consumed after each request)
		httpReq, err := http.NewRequestWithContext(ctx, "POST", c.baseURL+"/chat/completions", bytes.NewReader(body))
		if err != nil {
			return nil, err
		}
		httpReq.Header.Set("Content-Type", "application/json")
		httpReq.Header.Set("Authorization", "Bearer "+c.apiKey)

		resp, lastErr = c.httpClient.Do(httpReq)
		if lastErr != nil {
			// Network error - retry
			if ctx.Err() != nil {
				return nil, ErrTimeout
			}
			continue
		}

		// Check response status
		switch {
		case resp.StatusCode == http.StatusOK:
			// Success - break out of retry loop
			lastErr = nil
		case resp.StatusCode == http.StatusTooManyRequests:
			// Rate limited - close body and retry
			resp.Body.Close()
			lastErr = ErrRateLimited
			continue
		case resp.StatusCode >= 500:
			// Server error - close body and retry
			resp.Body.Close()
			lastErr = fmt.Errorf("server error: %d", resp.StatusCode)
			continue
		default:
			// Client error (4xx) - don't retry, break to handle error with body
			lastErr = nil
		}
		break
	}

	if lastErr != nil {
		return nil, fmt.Errorf("request failed after %d retries: %w", c.config.MaxRetries, lastErr)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		bodyBytes, _ := io.ReadAll(resp.Body)
		var errResp openaiErrorResponse
		if json.Unmarshal(bodyBytes, &errResp) == nil && errResp.Error.Message != "" {
			return nil, fmt.Errorf("openai error: %s", errResp.Error.Message)
		}
		return nil, fmt.Errorf("openai error (status %d): %s", resp.StatusCode, string(bodyBytes))
	}

	var openaiResp openaiChatResponse
	if err := json.NewDecoder(resp.Body).Decode(&openaiResp); err != nil {
		return nil, fmt.Errorf("failed to parse response: %w", err)
	}

	if len(openaiResp.Choices) == 0 {
		return nil, fmt.Errorf("no choices returned")
	}

	return &CompletionResponse{
		Content:      openaiResp.Choices[0].Message.Content,
		Model:        openaiResp.Model,
		FinishReason: openaiResp.Choices[0].FinishReason,
		Duration:     time.Since(start),
		Usage: TokenUsage{
			PromptTokens:     openaiResp.Usage.PromptTokens,
			CompletionTokens: openaiResp.Usage.CompletionTokens,
			TotalTokens:      openaiResp.Usage.TotalTokens,
		},
	}, nil
}

// openaiEmbedRequest is the request format for OpenAI's embeddings API.
type openaiEmbedRequest struct {
	Model string `json:"model"`
	Input string `json:"input"`
}

// openaiEmbedResponse is the response format from OpenAI's embeddings API.
type openaiEmbedResponse struct {
	Object string `json:"object"`
	Data   []struct {
		Object    string    `json:"object"`
		Index     int       `json:"index"`
		Embedding []float64 `json:"embedding"`
	} `json:"data"`
	Model string `json:"model"`
	Usage struct {
		PromptTokens int `json:"prompt_tokens"`
		TotalTokens  int `json:"total_tokens"`
	} `json:"usage"`
}

// Embed generates an embedding vector using OpenAI.
func (c *OpenAIClient) Embed(ctx context.Context, req *EmbeddingRequest) (*EmbeddingResponse, error) {
	start := time.Now()

	model := req.Model
	if model == "" {
		model = c.config.EmbeddingModel
	}

	openaiReq := openaiEmbedRequest{
		Model: model,
		Input: req.Text,
	}

	body, err := json.Marshal(openaiReq)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	httpReq, err := http.NewRequestWithContext(ctx, "POST", c.baseURL+"/embeddings", bytes.NewReader(body))
	if err != nil {
		return nil, err
	}
	httpReq.Header.Set("Content-Type", "application/json")
	httpReq.Header.Set("Authorization", "Bearer "+c.apiKey)

	resp, err := c.httpClient.Do(httpReq)
	if err != nil {
		return nil, fmt.Errorf("embed request failed: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		bodyBytes, _ := io.ReadAll(resp.Body)
		return nil, fmt.Errorf("openai embed error (status %d): %s", resp.StatusCode, string(bodyBytes))
	}

	var openaiResp openaiEmbedResponse
	if err := json.NewDecoder(resp.Body).Decode(&openaiResp); err != nil {
		return nil, fmt.Errorf("failed to parse response: %w", err)
	}

	if len(openaiResp.Data) == 0 {
		return nil, fmt.Errorf("no embeddings returned")
	}

	return &EmbeddingResponse{
		Embedding: openaiResp.Data[0].Embedding,
		Model:     openaiResp.Model,
		Duration:  time.Since(start),
		Usage: TokenUsage{
			PromptTokens: openaiResp.Usage.PromptTokens,
			TotalTokens:  openaiResp.Usage.TotalTokens,
		},
	}, nil
}
</file>
<file path="analyze/prompts.go">
package analyze

import (
	"fmt"
	"strings"

	"codemap/graph"
)

// Prompt templates for code explanation and summarization.
// These are designed to be token-efficient while producing useful summaries.

// ExplainSymbolPrompt generates a prompt for explaining a code symbol.
func ExplainSymbolPrompt(source *SymbolSource) []Message {
	systemPrompt := `You are a code documentation expert. Explain code clearly and concisely.
Focus on:
1. What the code does (purpose)
2. How it works (key logic)
3. Important parameters and return values
4. Any notable patterns or edge cases

Be brief but comprehensive. Use technical terms appropriately.`

	var userContent strings.Builder

	userContent.WriteString(fmt.Sprintf("Explain this %s %s", source.Language, kindName(source.Node)))

	if source.Node.Name != "" {
		userContent.WriteString(fmt.Sprintf(" named `%s`", source.Node.Name))
	}

	if source.Node.Package != "" {
		userContent.WriteString(fmt.Sprintf(" in package `%s`", source.Node.Package))
	}

	userContent.WriteString(":\n\n")
	userContent.WriteString("```" + source.Language + "\n")
	userContent.WriteString(source.Source)
	userContent.WriteString("\n```")

	// Add signature if available
	if source.Node.Signature != "" && !strings.Contains(source.Source, source.Node.Signature) {
		userContent.WriteString(fmt.Sprintf("\n\nSignature: `%s`", source.Node.Signature))
	}

	// Add docstring if available
	if source.Node.DocString != "" {
		userContent.WriteString(fmt.Sprintf("\n\nExisting documentation:\n%s", source.Node.DocString))
	}

	return []Message{
		{Role: "system", Content: systemPrompt},
		{Role: "user", Content: userContent.String()},
	}
}

// ExplainSymbolWithContextPrompt generates a prompt including surrounding context.
func ExplainSymbolWithContextPrompt(source *SymbolSource) []Message {
	if source.Context == nil {
		return ExplainSymbolPrompt(source)
	}

	systemPrompt := `You are a code documentation expert. Explain code clearly and concisely.
Focus on:
1. What the code does (purpose)
2. How it works (key logic)
3. How it relates to surrounding code
4. Important parameters and return values

Be brief but comprehensive. Use technical terms appropriately.`

	var userContent strings.Builder

	userContent.WriteString(fmt.Sprintf("Explain this %s %s", source.Language, kindName(source.Node)))

	if source.Node.Name != "" {
		userContent.WriteString(fmt.Sprintf(" named `%s`", source.Node.Name))
	}

	userContent.WriteString(":\n\n")

	// Add context before
	if source.Context.Before != "" {
		userContent.WriteString("Context (code before):\n```" + source.Language + "\n")
		userContent.WriteString(source.Context.Before)
		userContent.WriteString("\n```\n\n")
	}

	// Main source
	userContent.WriteString("Target code:\n```" + source.Language + "\n")
	userContent.WriteString(source.Source)
	userContent.WriteString("\n```")

	// Add context after
	if source.Context.After != "" {
		userContent.WriteString("\n\nContext (code after):\n```" + source.Language + "\n")
		userContent.WriteString(source.Context.After)
		userContent.WriteString("\n```")
	}

	return []Message{
		{Role: "system", Content: systemPrompt},
		{Role: "user", Content: userContent.String()},
	}
}

// SummarizeModulePrompt generates a prompt for summarizing a module/directory.
func SummarizeModulePrompt(modulePath string, sources []*SymbolSource) []Message {
	systemPrompt := `You are a code documentation expert. Summarize code modules clearly and concisely.
Provide:
1. Module purpose (what does this module do?)
2. Key components (main files, classes, functions)
3. Dependencies and relationships
4. Public API (exported functions/types)

Be structured and scannable. Use bullet points where appropriate.`

	var userContent strings.Builder

	userContent.WriteString(fmt.Sprintf("Summarize this module at `%s`:\n\n", modulePath))

	// List files with their key exports
	userContent.WriteString("**Files:**\n")
	for _, source := range sources {
		userContent.WriteString(fmt.Sprintf("\n### %s\n", source.Node.Name))
		userContent.WriteString("```" + source.Language + "\n")

		// Include first 100 lines or less
		lines := strings.Split(source.Source, "\n")
		if len(lines) > 100 {
			userContent.WriteString(strings.Join(lines[:100], "\n"))
			userContent.WriteString(fmt.Sprintf("\n// ... (%d more lines)\n", len(lines)-100))
		} else {
			userContent.WriteString(source.Source)
		}

		userContent.WriteString("\n```\n")
	}

	return []Message{
		{Role: "system", Content: systemPrompt},
		{Role: "user", Content: userContent.String()},
	}
}

// SummarizeFilesPrompt generates a prompt for summarizing multiple files concisely.
func SummarizeFilesPrompt(files []string, overviews []string) []Message {
	systemPrompt := `You are a code documentation expert. Create a brief module overview.
Output should be:
1. One-sentence module purpose
2. Key components (2-5 bullet points)
3. Main dependencies

Be extremely concise. Target 100-200 words total.`

	var userContent strings.Builder

	userContent.WriteString("Summarize this module based on file overviews:\n\n")

	for i, file := range files {
		userContent.WriteString(fmt.Sprintf("**%s:**\n%s\n\n", file, overviews[i]))
	}

	return []Message{
		{Role: "system", Content: systemPrompt},
		{Role: "user", Content: userContent.String()},
	}
}

// QuickExplainPrompt generates a minimal prompt for quick explanations.
func QuickExplainPrompt(source *SymbolSource) []Message {
	systemPrompt := "Explain code in one paragraph. Be concise."

	userContent := fmt.Sprintf("```%s\n%s\n```\n\nWhat does this do?",
		source.Language, source.Source)

	return []Message{
		{Role: "system", Content: systemPrompt},
		{Role: "user", Content: userContent},
	}
}

// CallGraphExplainPrompt explains a function in context of its callers and callees.
func CallGraphExplainPrompt(source *SymbolSource, callers, callees []string) []Message {
	systemPrompt := `You are a code documentation expert. Explain this function's role in the codebase.
Focus on:
1. What it does
2. Why callers might use it
3. What it depends on (callees)

Be concise but informative.`

	var userContent strings.Builder

	userContent.WriteString(fmt.Sprintf("Explain `%s`:\n\n", source.Node.Name))
	userContent.WriteString("```" + source.Language + "\n")
	userContent.WriteString(source.Source)
	userContent.WriteString("\n```\n")

	if len(callers) > 0 {
		userContent.WriteString(fmt.Sprintf("\nCalled by: %s\n", strings.Join(callers, ", ")))
	}

	if len(callees) > 0 {
		userContent.WriteString(fmt.Sprintf("\nCalls: %s\n", strings.Join(callees, ", ")))
	}

	return []Message{
		{Role: "system", Content: systemPrompt},
		{Role: "user", Content: userContent.String()},
	}
}

// kindName returns a human-readable name for a node kind.
func kindName(node *graph.Node) string {
	if node == nil {
		return "code"
	}

	switch node.Kind {
	case graph.KindFunction:
		return "function"
	case graph.KindMethod:
		return "method"
	case graph.KindType:
		return "type"
	case graph.KindFile:
		return "file"
	case graph.KindPackage:
		return "package"
	case graph.KindVariable:
		return "variable"
	default:
		return "code"
	}
}
</file>
<file path="analyze/retriever.go">
package analyze

import (
	"context"
	"sort"
	"strings"

	"codemap/graph"
)

// SearchMode controls how hybrid search operates
type SearchMode int

const (
	// SearchModeHybrid combines vector and graph search with rank fusion
	SearchModeHybrid SearchMode = iota
	// SearchModeVector uses only vector/semantic search
	SearchModeVector
	// SearchModeGraph uses only graph-based name matching
	SearchModeGraph
)

// SearchConfig configures the hybrid search behavior
type SearchConfig struct {
	// Mode controls the search strategy
	Mode SearchMode

	// Limit is the maximum number of results to return
	Limit int

	// VectorWeight is the weight for vector search results (0-1)
	VectorWeight float64

	// GraphWeight is the weight for graph search results (0-1)
	GraphWeight float64

	// ExpandContext if true, includes callers/callees in results
	ExpandContext bool

	// ExpandDepth is how many levels of context to expand
	ExpandDepth int

	// FuzzyMatch if true, uses substring matching for graph search
	FuzzyMatch bool
}

// DefaultSearchConfig returns sensible search defaults
func DefaultSearchConfig() SearchConfig {
	return SearchConfig{
		Mode:          SearchModeHybrid,
		Limit:         10,
		VectorWeight:  0.6, // Slightly favor semantic search
		GraphWeight:   0.4,
		ExpandContext: false,
		ExpandDepth:   1,
		FuzzyMatch:    true,
	}
}

// HybridResult represents a combined search result
type HybridResult struct {
	Node        *graph.Node
	VectorScore float64 // Similarity from vector search (0-1)
	GraphScore  float64 // Match score from graph search (0-1)
	FinalScore  float64 // Combined score after rank fusion
	MatchReason string  // Why this result matched

	// Context expansion (optional)
	Callers []*graph.Node
	Callees []*graph.Node
	Snippet string // Code snippet if available
}

// Retriever performs hybrid search over a codebase
type Retriever struct {
	graph       *graph.CodeGraph
	vectorIndex *graph.InMemoryVectorIndex
	llmClient   LLMClient
	projectRoot string
}

// NewRetriever creates a new hybrid retriever
func NewRetriever(
	cg *graph.CodeGraph,
	index *graph.InMemoryVectorIndex,
	client LLMClient,
	projectRoot string,
) *Retriever {
	return &Retriever{
		graph:       cg,
		vectorIndex: index,
		llmClient:   client,
		projectRoot: projectRoot,
	}
}

// Search performs hybrid search combining vector and graph search
func (r *Retriever) Search(ctx context.Context, query string, config SearchConfig) ([]HybridResult, error) {
	var vectorResults []graph.SearchResult
	var graphResults []graphMatch

	// Vector search
	if config.Mode == SearchModeHybrid || config.Mode == SearchModeVector {
		if r.vectorIndex != nil && r.vectorIndex.Count() > 0 && r.llmClient != nil {
			queryVec, err := EmbedQuery(ctx, r.llmClient, query)
			if err == nil && len(queryVec) > 0 {
				results, err := r.vectorIndex.Search(queryVec, config.Limit*2)
				if err == nil {
					vectorResults = results
				}
			}
		}
	}

	// Graph search (name matching)
	if config.Mode == SearchModeHybrid || config.Mode == SearchModeGraph {
		graphResults = r.graphSearch(query, config.Limit*2, config.FuzzyMatch)
	}

	// Combine results using Reciprocal Rank Fusion
	combined := r.rankFusion(vectorResults, graphResults, config)

	// Apply limit
	if len(combined) > config.Limit {
		combined = combined[:config.Limit]
	}

	// Expand context if requested
	if config.ExpandContext {
		for i := range combined {
			r.expandContext(&combined[i], config.ExpandDepth)
		}
	}

	return combined, nil
}

// graphMatch represents a graph-based search match
type graphMatch struct {
	node      *graph.Node
	score     float64
	matchType string // "exact", "prefix", "contains"
}

// graphSearch finds nodes by name matching
func (r *Retriever) graphSearch(query string, limit int, fuzzy bool) []graphMatch {
	if r.graph == nil {
		return nil
	}

	query = strings.ToLower(query)
	words := strings.Fields(query)

	var matches []graphMatch

	for _, node := range r.graph.Nodes {
		// Skip file nodes for search (usually not what users want)
		if node.Kind == graph.KindFile || node.Kind == graph.KindPackage {
			continue
		}

		score, matchType := r.matchScore(node, query, words, fuzzy)
		if score > 0 {
			matches = append(matches, graphMatch{
				node:      node,
				score:     score,
				matchType: matchType,
			})
		}
	}

	// Sort by score descending
	sort.Slice(matches, func(i, j int) bool {
		return matches[i].score > matches[j].score
	})

	if len(matches) > limit {
		matches = matches[:limit]
	}

	return matches
}

// matchScore calculates how well a node matches the query
func (r *Retriever) matchScore(node *graph.Node, query string, words []string, fuzzy bool) (float64, string) {
	name := strings.ToLower(node.Name)

	// Exact match (highest score)
	if name == query {
		return 1.0, "exact"
	}

	// Prefix match
	if strings.HasPrefix(name, query) {
		return 0.9, "prefix"
	}

	if fuzzy {
		// Contains match
		if strings.Contains(name, query) {
			return 0.7, "contains"
		}

		// Word matching (for multi-word queries)
		if len(words) > 1 {
			matchedWords := 0
			for _, word := range words {
				if strings.Contains(name, word) {
					matchedWords++
				}
				// Also check signature and docstring
				if node.Signature != "" && strings.Contains(strings.ToLower(node.Signature), word) {
					matchedWords++
				}
				if node.DocString != "" && strings.Contains(strings.ToLower(node.DocString), word) {
					matchedWords++
				}
			}
			if matchedWords > 0 {
				// Score based on percentage of words matched
				score := 0.5 * float64(matchedWords) / float64(len(words)*3) // *3 for name, sig, doc
				if score > 0.1 {
					return score, "words"
				}
			}
		}

		// Check path for package/module queries
		if node.Path != "" && strings.Contains(strings.ToLower(node.Path), query) {
			return 0.4, "path"
		}
	}

	return 0, ""
}

// rankFusion combines vector and graph results using Reciprocal Rank Fusion (RRF)
// RRF score = sum(1 / (k + rank)) for each list where the item appears
// k is a constant (typically 60) to prevent division by small numbers
func (r *Retriever) rankFusion(
	vectorResults []graph.SearchResult,
	graphResults []graphMatch,
	config SearchConfig,
) []HybridResult {
	const k = 60 // RRF constant

	// Map to track combined scores
	type fusedScore struct {
		node        *graph.Node
		vectorScore float64
		vectorRank  int
		graphScore  float64
		graphRank   int
		matchReason string
	}
	scores := make(map[graph.NodeID]*fusedScore)

	// Add vector results
	for rank, vr := range vectorResults {
		node := r.graph.GetNode(vr.NodeID)
		if node == nil {
			continue
		}

		if _, exists := scores[vr.NodeID]; !exists {
			scores[vr.NodeID] = &fusedScore{
				node:        node,
				vectorRank:  -1,
				graphRank:   -1,
				matchReason: "semantic match",
			}
		}
		scores[vr.NodeID].vectorScore = vr.Score
		scores[vr.NodeID].vectorRank = rank
	}

	// Add graph results
	for rank, gr := range graphResults {
		if _, exists := scores[gr.node.ID]; !exists {
			scores[gr.node.ID] = &fusedScore{
				node:       gr.node,
				vectorRank: -1,
				graphRank:  -1,
			}
		}
		scores[gr.node.ID].graphScore = gr.score
		scores[gr.node.ID].graphRank = rank
		if scores[gr.node.ID].matchReason == "" {
			scores[gr.node.ID].matchReason = gr.matchType + " name match"
		} else {
			scores[gr.node.ID].matchReason += ", " + gr.matchType + " name match"
		}
	}

	// Calculate RRF scores
	var results []HybridResult
	for _, fs := range scores {
		var rrfScore float64

		// Vector contribution (weighted)
		if fs.vectorRank >= 0 {
			rrfScore += config.VectorWeight * (1.0 / float64(k+fs.vectorRank))
		}

		// Graph contribution (weighted)
		if fs.graphRank >= 0 {
			rrfScore += config.GraphWeight * (1.0 / float64(k+fs.graphRank))
		}

		results = append(results, HybridResult{
			Node:        fs.node,
			VectorScore: fs.vectorScore,
			GraphScore:  fs.graphScore,
			FinalScore:  rrfScore,
			MatchReason: fs.matchReason,
		})
	}

	// Sort by final score descending
	sort.Slice(results, func(i, j int) bool {
		return results[i].FinalScore > results[j].FinalScore
	})

	return results
}

// expandContext adds callers and callees to a result
func (r *Retriever) expandContext(result *HybridResult, depth int) {
	if r.graph == nil || result.Node == nil || depth <= 0 {
		return
	}

	// Get callers (direct callers only)
	callers := r.graph.GetCallers(result.Node.ID)
	for _, caller := range callers {
		result.Callers = append(result.Callers, caller)
	}

	// Get callees (direct callees only)
	callees := r.graph.GetCallees(result.Node.ID)
	for _, callee := range callees {
		result.Callees = append(result.Callees, callee)
	}

	// Add code snippet if available
	if r.projectRoot != "" {
		source, err := ReadSymbolSource(r.projectRoot, result.Node)
		if err == nil && source != nil {
			// Truncate for readability
			snippet := source.Source
			if len(snippet) > 500 {
				snippet = snippet[:500] + "\n// ..."
			}
			result.Snippet = snippet
		}
	}
}

// SemanticSearch is a convenience function for pure semantic search
func (r *Retriever) SemanticSearch(ctx context.Context, query string, limit int) ([]HybridResult, error) {
	config := DefaultSearchConfig()
	config.Mode = SearchModeVector
	config.Limit = limit
	return r.Search(ctx, query, config)
}

// NameSearch is a convenience function for name-based search
func (r *Retriever) NameSearch(query string, limit int) ([]HybridResult, error) {
	config := DefaultSearchConfig()
	config.Mode = SearchModeGraph
	config.Limit = limit
	return r.Search(context.Background(), query, config)
}
</file>
<file path="analyze/source.go">
package analyze

import (
	"bufio"
	"crypto/sha256"
	"encoding/hex"
	"fmt"
	"os"
	"path/filepath"
	"strings"

	"codemap/graph"
)

// SymbolSource represents the extracted source code for a symbol.
type SymbolSource struct {
	// Node is the graph node this source belongs to
	Node *graph.Node

	// Source is the raw source code
	Source string

	// Language is the programming language
	Language string

	// ContentHash is a SHA256 hash of the source for cache invalidation
	ContentHash string

	// Context provides surrounding code for context
	Context *SourceContext
}

// SourceContext provides surrounding code context for a symbol.
type SourceContext struct {
	// Before contains lines before the symbol (e.g., imports, comments)
	Before string

	// After contains lines after the symbol
	After string

	// FileHeader contains the file-level documentation
	FileHeader string
}

// ReadSymbolSource extracts the source code for a symbol from its file.
// The projectRoot is used to resolve relative paths in the node.
func ReadSymbolSource(projectRoot string, node *graph.Node) (*SymbolSource, error) {
	if node == nil {
		return nil, fmt.Errorf("node is nil")
	}

	if node.Path == "" {
		return nil, fmt.Errorf("node has no path")
	}

	// Resolve full path
	fullPath := filepath.Join(projectRoot, node.Path)

	// Read the file
	content, err := os.ReadFile(fullPath)
	if err != nil {
		return nil, fmt.Errorf("reading file %s: %w", fullPath, err)
	}

	lines := strings.Split(string(content), "\n")

	// Extract source based on line numbers
	var source string
	if node.Line > 0 && node.EndLine > 0 {
		// Extract specific lines (1-indexed to 0-indexed)
		startLine := node.Line - 1
		endLine := node.EndLine

		if startLine < 0 {
			startLine = 0
		}
		if endLine > len(lines) {
			endLine = len(lines)
		}

		source = strings.Join(lines[startLine:endLine], "\n")
	} else if node.Line > 0 {
		// Only start line available, use heuristic to find end
		startLine := node.Line - 1
		if startLine < 0 {
			startLine = 0
		}

		// For files, use entire content
		if node.Kind == graph.KindFile {
			source = string(content)
		} else {
			// Use a reasonable chunk (50 lines) if we don't have end line
			endLine := startLine + 50
			if endLine > len(lines) {
				endLine = len(lines)
			}
			source = strings.Join(lines[startLine:endLine], "\n")
		}
	} else {
		// No line info, use entire file for file nodes
		if node.Kind == graph.KindFile {
			source = string(content)
		} else {
			return nil, fmt.Errorf("node has no line information")
		}
	}

	// Compute content hash
	hash := sha256.Sum256([]byte(source))
	contentHash := hex.EncodeToString(hash[:])

	// Detect language from file extension
	language := detectLanguage(node.Path)

	return &SymbolSource{
		Node:        node,
		Source:      source,
		Language:    language,
		ContentHash: contentHash,
	}, nil
}

// ReadSymbolSourceWithContext extracts source code with surrounding context.
func ReadSymbolSourceWithContext(projectRoot string, node *graph.Node, contextLines int) (*SymbolSource, error) {
	source, err := ReadSymbolSource(projectRoot, node)
	if err != nil {
		return nil, err
	}

	if contextLines <= 0 {
		return source, nil
	}

	// Read the file again for context
	fullPath := filepath.Join(projectRoot, node.Path)
	content, err := os.ReadFile(fullPath)
	if err != nil {
		return source, nil // Return source without context on error
	}

	lines := strings.Split(string(content), "\n")

	// Extract context before
	if node.Line > 1 {
		startContext := node.Line - 1 - contextLines
		if startContext < 0 {
			startContext = 0
		}
		endContext := node.Line - 1
		source.Context = &SourceContext{
			Before: strings.Join(lines[startContext:endContext], "\n"),
		}
	}

	// Extract context after
	if node.EndLine > 0 && node.EndLine < len(lines) {
		startContext := node.EndLine
		endContext := node.EndLine + contextLines
		if endContext > len(lines) {
			endContext = len(lines)
		}
		if source.Context == nil {
			source.Context = &SourceContext{}
		}
		source.Context.After = strings.Join(lines[startContext:endContext], "\n")
	}

	// Extract file header (first comment block)
	source.Context.FileHeader = extractFileHeader(lines)

	return source, nil
}

// ReadModuleSource reads all source files in a directory.
func ReadModuleSource(projectRoot, modulePath string) ([]*SymbolSource, error) {
	fullPath := filepath.Join(projectRoot, modulePath)

	info, err := os.Stat(fullPath)
	if err != nil {
		return nil, fmt.Errorf("stat %s: %w", fullPath, err)
	}

	if !info.IsDir() {
		// Single file
		node := &graph.Node{
			Kind: graph.KindFile,
			Name: filepath.Base(modulePath),
			Path: modulePath,
		}
		source, err := ReadSymbolSource(projectRoot, node)
		if err != nil {
			return nil, err
		}
		return []*SymbolSource{source}, nil
	}

	// Directory - read all source files
	var sources []*SymbolSource
	entries, err := os.ReadDir(fullPath)
	if err != nil {
		return nil, fmt.Errorf("reading directory %s: %w", fullPath, err)
	}

	for _, entry := range entries {
		if entry.IsDir() {
			continue
		}

		name := entry.Name()
		if !isSourceFile(name) {
			continue
		}

		relPath := filepath.Join(modulePath, name)
		node := &graph.Node{
			Kind: graph.KindFile,
			Name: name,
			Path: relPath,
		}

		source, err := ReadSymbolSource(projectRoot, node)
		if err != nil {
			continue // Skip files we can't read
		}

		sources = append(sources, source)
	}

	return sources, nil
}

// detectLanguage returns the programming language based on file extension.
func detectLanguage(path string) string {
	ext := strings.ToLower(filepath.Ext(path))
	switch ext {
	case ".go":
		return "go"
	case ".py":
		return "python"
	case ".js":
		return "javascript"
	case ".ts":
		return "typescript"
	case ".tsx":
		return "typescript"
	case ".jsx":
		return "javascript"
	case ".rs":
		return "rust"
	case ".java":
		return "java"
	case ".c", ".h":
		return "c"
	case ".cpp", ".cc", ".cxx", ".hpp":
		return "cpp"
	case ".rb":
		return "ruby"
	case ".php":
		return "php"
	case ".swift":
		return "swift"
	case ".kt", ".kts":
		return "kotlin"
	case ".scala":
		return "scala"
	case ".cs":
		return "csharp"
	case ".md":
		return "markdown"
	case ".json":
		return "json"
	case ".yaml", ".yml":
		return "yaml"
	case ".xml":
		return "xml"
	case ".html":
		return "html"
	case ".css":
		return "css"
	case ".sh", ".bash":
		return "bash"
	default:
		return "text"
	}
}

// isSourceFile checks if a filename is a recognized source file.
func isSourceFile(name string) bool {
	if strings.HasPrefix(name, ".") {
		return false
	}

	ext := strings.ToLower(filepath.Ext(name))
	switch ext {
	case ".go", ".py", ".js", ".ts", ".tsx", ".jsx", ".rs", ".java",
		".c", ".h", ".cpp", ".cc", ".cxx", ".hpp", ".rb", ".php",
		".swift", ".kt", ".kts", ".scala", ".cs":
		return true
	default:
		return false
	}
}

// extractFileHeader extracts the file-level documentation comment.
func extractFileHeader(lines []string) string {
	var header []string
	inComment := false

	for _, line := range lines {
		trimmed := strings.TrimSpace(line)

		// Package declarations end header
		if strings.HasPrefix(trimmed, "package ") ||
			strings.HasPrefix(trimmed, "import ") ||
			strings.HasPrefix(trimmed, "from ") ||
			strings.HasPrefix(trimmed, "use ") ||
			strings.HasPrefix(trimmed, "require ") {
			break
		}

		// Track comment blocks
		if strings.HasPrefix(trimmed, "//") || strings.HasPrefix(trimmed, "#") {
			header = append(header, line)
			continue
		}

		if strings.HasPrefix(trimmed, "/*") {
			inComment = true
			header = append(header, line)
			continue
		}

		if inComment {
			header = append(header, line)
			if strings.Contains(trimmed, "*/") {
				inComment = false
			}
			continue
		}

		// Empty lines are ok in header
		if trimmed == "" && len(header) > 0 {
			header = append(header, line)
			continue
		}

		// Non-empty, non-comment line ends header
		if trimmed != "" {
			break
		}
	}

	return strings.TrimSpace(strings.Join(header, "\n"))
}

// ContentHash computes a SHA256 hash of the given content.
func ContentHash(content string) string {
	hash := sha256.Sum256([]byte(content))
	return hex.EncodeToString(hash[:])
}

// ReadFileLines reads specific lines from a file.
func ReadFileLines(path string, startLine, endLine int) (string, error) {
	file, err := os.Open(path)
	if err != nil {
		return "", err
	}
	defer file.Close()

	var lines []string
	scanner := bufio.NewScanner(file)
	lineNum := 0

	for scanner.Scan() {
		lineNum++
		if lineNum >= startLine && lineNum <= endLine {
			lines = append(lines, scanner.Text())
		}
		if lineNum > endLine {
			break
		}
	}

	if err := scanner.Err(); err != nil {
		return "", err
	}

	return strings.Join(lines, "\n"), nil
}
</file>
<file path="analyze/tokens.go">
package analyze

import (
	"strings"
	"unicode"
)

// EstimateTokens provides a rough estimate of token count for text.
// This is a simple heuristic based on typical tokenization patterns.
// For accurate counts, use the provider's actual tokenizer.
//
// Typical ratios:
// - English: ~4 characters per token
// - Code: ~3-4 characters per token
// - Mixed: ~3.5 characters per token
func EstimateTokens(text string) int {
	if text == "" {
		return 0
	}

	// Count words and special tokens
	words := 0
	specialTokens := 0
	inWord := false

	for _, r := range text {
		if unicode.IsSpace(r) {
			if inWord {
				words++
				inWord = false
			}
		} else if unicode.IsPunct(r) || unicode.IsSymbol(r) {
			if inWord {
				words++
				inWord = false
			}
			specialTokens++
		} else {
			inWord = true
		}
	}

	if inWord {
		words++
	}

	// Rough estimate: each word is ~1.3 tokens, each special char is ~1 token
	return int(float64(words)*1.3) + specialTokens
}

// EstimateTokensForMessages estimates token count for a conversation.
// Includes overhead for message formatting.
func EstimateTokensForMessages(messages []Message) int {
	total := 0
	for _, m := range messages {
		// ~4 tokens overhead per message for role, formatting
		total += 4
		total += EstimateTokens(m.Content)
	}
	return total
}

// TruncateToTokenLimit truncates text to fit within a token limit.
// Returns the truncated text and whether truncation occurred.
func TruncateToTokenLimit(text string, maxTokens int) (string, bool) {
	estimated := EstimateTokens(text)
	if estimated <= maxTokens {
		return text, false
	}

	// Rough conversion: ~4 chars per token
	targetChars := maxTokens * 4

	// Find a good break point (end of line or sentence)
	if len(text) <= targetChars {
		return text, false
	}

	truncated := text[:targetChars]

	// Try to break at newline
	if idx := strings.LastIndex(truncated, "\n"); idx > targetChars/2 {
		return truncated[:idx] + "\n... (truncated)", true
	}

	// Try to break at sentence
	if idx := strings.LastIndex(truncated, ". "); idx > targetChars/2 {
		return truncated[:idx+1] + " ... (truncated)", true
	}

	// Break at word boundary
	if idx := strings.LastIndex(truncated, " "); idx > targetChars/2 {
		return truncated[:idx] + " ... (truncated)", true
	}

	return truncated + "... (truncated)", true
}

// TokenBudget helps manage token allocation across multiple content blocks.
type TokenBudget struct {
	Total     int
	Remaining int
}

// NewTokenBudget creates a new token budget.
func NewTokenBudget(total int) *TokenBudget {
	return &TokenBudget{
		Total:     total,
		Remaining: total,
	}
}

// Allocate reserves tokens for content, returning available tokens.
// Returns 0 if budget is exhausted.
func (b *TokenBudget) Allocate(content string) int {
	needed := EstimateTokens(content)
	if needed > b.Remaining {
		return 0
	}
	b.Remaining -= needed
	return needed
}

// Reserve sets aside tokens for later use (e.g., response).
func (b *TokenBudget) Reserve(tokens int) bool {
	if tokens > b.Remaining {
		return false
	}
	b.Remaining -= tokens
	return true
}

// Used returns how many tokens have been used.
func (b *TokenBudget) Used() int {
	return b.Total - b.Remaining
}

// Available returns how many tokens are available.
func (b *TokenBudget) Available() int {
	return b.Remaining
}
</file>
<file path="cache/cache.go">
// Package cache provides response caching for LLM interactions.
// Cache entries are keyed by content hash to ensure cache invalidation
// when source code changes.
package cache

import (
	"crypto/sha256"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"os"
	"path/filepath"
	"sync"
	"time"
)

// Entry represents a cached response.
type Entry struct {
	// Key is the cache key (typically content hash + operation)
	Key string `json:"key"`

	// ContentHash is the SHA256 hash of the source content
	ContentHash string `json:"content_hash"`

	// Response is the cached LLM response
	Response string `json:"response"`

	// Model is the model that generated the response
	Model string `json:"model"`

	// CreatedAt is when the entry was created
	CreatedAt time.Time `json:"created_at"`

	// Usage contains token counts
	Usage *TokenUsage `json:"usage,omitempty"`
}

// TokenUsage tracks token consumption.
type TokenUsage struct {
	PromptTokens     int `json:"prompt_tokens"`
	CompletionTokens int `json:"completion_tokens"`
	TotalTokens      int `json:"total_tokens"`
}

// Stats tracks cache performance.
type Stats struct {
	Hits       int64 `json:"hits"`
	Misses     int64 `json:"misses"`
	Writes     int64 `json:"writes"`
	Evictions  int64 `json:"evictions"`
	TotalBytes int64 `json:"total_bytes"`
}

// Cache is a file-backed cache for LLM responses.
type Cache struct {
	dir     string
	ttl     time.Duration
	stats   Stats
	mu      sync.RWMutex
	enabled bool
}

// Options configures the cache.
type Options struct {
	// Dir is the cache directory (default: .codemap/cache)
	Dir string

	// TTL is the cache entry TTL (0 = no expiry)
	TTL time.Duration

	// Enabled controls whether caching is active
	Enabled bool
}

// DefaultOptions returns default cache options.
func DefaultOptions() Options {
	return Options{
		Dir:     ".codemap/cache",
		TTL:     0, // No expiry
		Enabled: true,
	}
}

// New creates a new cache.
func New(opts Options) (*Cache, error) {
	if !opts.Enabled {
		return &Cache{enabled: false}, nil
	}

	// Ensure cache directory exists
	if err := os.MkdirAll(opts.Dir, 0755); err != nil {
		return nil, fmt.Errorf("creating cache directory: %w", err)
	}

	return &Cache{
		dir:     opts.Dir,
		ttl:     opts.TTL,
		enabled: true,
	}, nil
}

// MakeKey creates a cache key from content hash and operation.
func MakeKey(contentHash, operation, model string) string {
	// Include operation and model in key to separate different uses
	combined := fmt.Sprintf("%s:%s:%s", contentHash, operation, model)
	hash := sha256.Sum256([]byte(combined))
	return hex.EncodeToString(hash[:16]) // Use first 16 bytes for shorter keys
}

// ContentHash computes a SHA256 hash of content.
func ContentHash(content string) string {
	hash := sha256.Sum256([]byte(content))
	return hex.EncodeToString(hash[:])
}

// Get retrieves a cached entry.
func (c *Cache) Get(key string) (*Entry, bool) {
	if !c.enabled {
		return nil, false
	}

	c.mu.RLock()
	path := c.keyPath(key)
	data, err := os.ReadFile(path)
	c.mu.RUnlock() // Release read lock before calling recordMiss/recordHit

	if err != nil {
		c.recordMiss()
		return nil, false
	}

	var entry Entry
	if err := json.Unmarshal(data, &entry); err != nil {
		c.recordMiss()
		return nil, false
	}

	// Check TTL
	if c.ttl > 0 && time.Since(entry.CreatedAt) > c.ttl {
		// Entry expired
		os.Remove(path)
		c.recordMiss()
		return nil, false
	}

	c.recordHit()
	return &entry, true
}

// GetByContentHash retrieves a cached entry by content hash and operation.
func (c *Cache) GetByContentHash(contentHash, operation, model string) (*Entry, bool) {
	key := MakeKey(contentHash, operation, model)
	return c.Get(key)
}

// Set stores an entry in the cache.
func (c *Cache) Set(entry *Entry) error {
	if !c.enabled {
		return nil
	}

	c.mu.Lock()
	defer c.mu.Unlock()

	if entry.CreatedAt.IsZero() {
		entry.CreatedAt = time.Now()
	}

	data, err := json.MarshalIndent(entry, "", "  ")
	if err != nil {
		return fmt.Errorf("marshaling entry: %w", err)
	}

	path := c.keyPath(entry.Key)
	if err := os.WriteFile(path, data, 0644); err != nil {
		return fmt.Errorf("writing cache file: %w", err)
	}

	c.stats.Writes++
	c.stats.TotalBytes += int64(len(data))

	return nil
}

// SetResponse is a convenience method to cache a response.
func (c *Cache) SetResponse(contentHash, operation, model, response string, usage *TokenUsage) error {
	key := MakeKey(contentHash, operation, model)
	entry := &Entry{
		Key:         key,
		ContentHash: contentHash,
		Response:    response,
		Model:       model,
		CreatedAt:   time.Now(),
		Usage:       usage,
	}
	return c.Set(entry)
}

// Delete removes an entry from the cache.
func (c *Cache) Delete(key string) error {
	if !c.enabled {
		return nil
	}

	c.mu.Lock()
	defer c.mu.Unlock()

	path := c.keyPath(key)
	if err := os.Remove(path); err != nil && !os.IsNotExist(err) {
		return err
	}

	c.stats.Evictions++
	return nil
}

// Clear removes all entries from the cache.
func (c *Cache) Clear() error {
	if !c.enabled {
		return nil
	}

	c.mu.Lock()
	defer c.mu.Unlock()

	entries, err := os.ReadDir(c.dir)
	if err != nil {
		return err
	}

	for _, entry := range entries {
		if entry.IsDir() {
			continue
		}
		if filepath.Ext(entry.Name()) == ".json" {
			path := filepath.Join(c.dir, entry.Name())
			os.Remove(path)
			c.stats.Evictions++
		}
	}

	c.stats.TotalBytes = 0
	return nil
}

// Stats returns cache statistics.
func (c *Cache) Stats() Stats {
	c.mu.RLock()
	defer c.mu.RUnlock()
	return c.stats
}

// HitRate returns the cache hit rate.
func (c *Cache) HitRate() float64 {
	c.mu.RLock()
	defer c.mu.RUnlock()

	total := c.stats.Hits + c.stats.Misses
	if total == 0 {
		return 0
	}
	return float64(c.stats.Hits) / float64(total)
}

// Size returns the number of cached entries.
func (c *Cache) Size() int {
	if !c.enabled {
		return 0
	}

	entries, err := os.ReadDir(c.dir)
	if err != nil {
		return 0
	}

	count := 0
	for _, entry := range entries {
		if !entry.IsDir() && filepath.Ext(entry.Name()) == ".json" {
			count++
		}
	}
	return count
}

// Enabled returns whether caching is enabled.
func (c *Cache) Enabled() bool {
	return c.enabled
}

// keyPath returns the file path for a cache key.
func (c *Cache) keyPath(key string) string {
	return filepath.Join(c.dir, key+".json")
}

// recordHit increments the hit counter.
func (c *Cache) recordHit() {
	c.mu.Lock()
	c.stats.Hits++
	c.mu.Unlock()
}

// recordMiss increments the miss counter.
func (c *Cache) recordMiss() {
	c.mu.Lock()
	c.stats.Misses++
	c.mu.Unlock()
}

// Cleanup removes expired entries.
func (c *Cache) Cleanup() error {
	if !c.enabled || c.ttl == 0 {
		return nil
	}

	c.mu.Lock()
	defer c.mu.Unlock()

	entries, err := os.ReadDir(c.dir)
	if err != nil {
		return err
	}

	now := time.Now()
	for _, entry := range entries {
		if entry.IsDir() || filepath.Ext(entry.Name()) != ".json" {
			continue
		}

		path := filepath.Join(c.dir, entry.Name())
		data, err := os.ReadFile(path)
		if err != nil {
			continue
		}

		var cached Entry
		if err := json.Unmarshal(data, &cached); err != nil {
			continue
		}

		if now.Sub(cached.CreatedAt) > c.ttl {
			os.Remove(path)
			c.stats.Evictions++
		}
	}

	return nil
}
</file>
<file path="config/config.go">
// Package config handles configuration loading and management for codemap.
// Configuration is loaded from:
// 1. ~/.config/codemap/config.yaml (user-level)
// 2. .codemap/config.yaml (project-level override)
// 3. Environment variables from .env files (user-level and project-level)
// 4. Environment variables (highest priority)
package config

import (
	"errors"
	"fmt"
	"os"
	"path/filepath"
	"strings"

	"github.com/joho/godotenv"
	"gopkg.in/yaml.v3"
)

// Provider represents an LLM provider type.
type Provider string

const (
	ProviderOllama    Provider = "ollama"
	ProviderOpenAI    Provider = "openai"
	ProviderAnthropic Provider = "anthropic"
	ProviderGemini    Provider = "gemini"
	ProviderMock      Provider = "mock"
)

// LLMConfig holds settings for LLM integration.
type LLMConfig struct {
	// Provider is the LLM provider to use (ollama, openai, anthropic, gemini)
	Provider Provider `yaml:"provider"`

	// Model is the model name/ID to use (e.g., "llama3", "gpt-4", "claude-3-sonnet")
	Model string `yaml:"model"`

	// Ollama-specific settings
	OllamaURL string `yaml:"ollama_url"` // Default: http://localhost:11434

	// OpenAI-specific settings
	OpenAIAPIKey  string `yaml:"openai_api_key"`  // Can also use OPENAI_API_KEY env var
	OpenAIBaseURL string `yaml:"openai_base_url"` // For Azure or compatible APIs

	// Anthropic-specific settings
	AnthropicAPIKey string `yaml:"anthropic_api_key"` // Can also use ANTHROPIC_API_KEY env var

	// Gemini-specific settings
	GeminiAPIKey  string `yaml:"gemini_api_key"`  // Can also use GEMINI_API_KEY env var
	GeminiBaseURL string `yaml:"gemini_base_url"` // Default: https://generativelanguage.googleapis.com/v1beta

	// Embedding settings
	EmbeddingModel    string `yaml:"embedding_model"`    // Model for embeddings
	EmbeddingProvider string `yaml:"embedding_provider"` // Provider for embeddings (can differ from main provider)

	// Request settings
	Timeout        int     `yaml:"timeout"`          // Request timeout in seconds (default: 60)
	MaxRetries     int     `yaml:"max_retries"`      // Max retry attempts (default: 3)
	Temperature    float64 `yaml:"temperature"`      // Temperature for generation (default: 0.1)
	MaxTokens      int     `yaml:"max_tokens"`       // Max tokens for response (default: 2048)
	RequestsPerMin int     `yaml:"requests_per_min"` // Rate limit (default: 60)
}

// CacheConfig holds settings for response caching.
type CacheConfig struct {
	// Enabled controls whether caching is active
	Enabled bool `yaml:"enabled"`

	// Dir is the cache directory (default: .codemap/cache)
	Dir string `yaml:"dir"`

	// TTLDays is the cache TTL in days (0 = no expiry)
	TTLDays int `yaml:"ttl_days"`
}

// Config is the main configuration structure.
type Config struct {
	// LLM holds LLM integration settings
	LLM LLMConfig `yaml:"llm"`

	// Cache holds caching settings
	Cache CacheConfig `yaml:"cache"`

	// Debug enables verbose logging
	Debug bool `yaml:"debug"`
}

// DefaultConfig returns a configuration with sensible defaults.
func DefaultConfig() *Config {
	return &Config{
		LLM: LLMConfig{
			Provider:       ProviderOllama,
			Model:          "llama3",
			OllamaURL:      "http://localhost:11434",
			EmbeddingModel: "nomic-embed-text",
			Timeout:        60,
			MaxRetries:     3,
			Temperature:    0.1,
			MaxTokens:      2048,
			RequestsPerMin: 60,
		},
		Cache: CacheConfig{
			Enabled: true,
			Dir:     ".codemap/cache",
			TTLDays: 0, // No expiry by default
		},
		Debug: false,
	}
}

// Load reads configuration from standard locations and merges with defaults.
// Priority (highest to lowest):
// 1. Environment variables (shell)
// 2. Project .env file (.env)
// 3. User .env file (~/.config/codemap/.env)
// 4. Project config (.codemap/config.yaml)
// 5. User config (~/.config/codemap/config.yaml)
// 6. Defaults
func Load() (*Config, error) {
	cfg := DefaultConfig()

	// Try user config first
	userConfigPath, err := userConfigPath()
	if err == nil {
		if data, err := os.ReadFile(userConfigPath); err == nil {
			if err := yaml.Unmarshal(data, cfg); err != nil {
				return nil, fmt.Errorf("parsing user config %s: %w", userConfigPath, err)
			}
		}
	}

	// Try project config (overrides user config)
	projectConfigPath := filepath.Join(".codemap", "config.yaml")
	if data, err := os.ReadFile(projectConfigPath); err == nil {
		if err := yaml.Unmarshal(data, cfg); err != nil {
			return nil, fmt.Errorf("parsing project config %s: %w", projectConfigPath, err)
		}
	}

	// Load .env files (user-level first, then project-level to override)
	loadEnvFiles()

	// Apply environment variable overrides
	applyEnvOverrides(cfg)

	// Validate the configuration
	if err := cfg.Validate(); err != nil {
		return nil, fmt.Errorf("config validation failed: %w", err)
	}

	return cfg, nil
}

// LoadFromPath reads configuration from a specific file path.
func LoadFromPath(path string) (*Config, error) {
	cfg := DefaultConfig()

	data, err := os.ReadFile(path)
	if err != nil {
		return nil, fmt.Errorf("reading config file: %w", err)
	}

	if err := yaml.Unmarshal(data, cfg); err != nil {
		return nil, fmt.Errorf("parsing config file: %w", err)
	}

	// Load .env files
	loadEnvFiles()

	applyEnvOverrides(cfg)

	if err := cfg.Validate(); err != nil {
		return nil, fmt.Errorf("config validation failed: %w", err)
	}

	return cfg, nil
}

// Validate checks that the configuration is valid.
func (c *Config) Validate() error {
	var errs []string

	// Validate provider
	switch c.LLM.Provider {
	case ProviderOllama:
		if c.LLM.OllamaURL == "" {
			errs = append(errs, "ollama_url required for ollama provider")
		}
	case ProviderOpenAI:
		if c.LLM.OpenAIAPIKey == "" {
			errs = append(errs, "openai_api_key required for openai provider (or set OPENAI_API_KEY env var)")
		}
	case ProviderAnthropic:
		if c.LLM.AnthropicAPIKey == "" {
			errs = append(errs, "anthropic_api_key required for anthropic provider (or set ANTHROPIC_API_KEY env var)")
		}
	case ProviderGemini:
		if c.LLM.GeminiAPIKey == "" {
			errs = append(errs, "gemini_api_key required for gemini provider (or set GEMINI_API_KEY env var)")
		}
	case ProviderMock:
		// No validation needed for mock
	case "":
		// Will use default
	default:
		errs = append(errs, fmt.Sprintf("unknown provider: %s", c.LLM.Provider))
	}

	// Validate model
	if c.LLM.Model == "" {
		errs = append(errs, "model is required")
	}

	// Validate timeouts
	if c.LLM.Timeout < 0 {
		errs = append(errs, "timeout must be non-negative")
	}
	if c.LLM.MaxRetries < 0 {
		errs = append(errs, "max_retries must be non-negative")
	}

	if len(errs) > 0 {
		return errors.New(strings.Join(errs, "; "))
	}

	return nil
}

// userConfigPath returns the path to the user configuration file.
func userConfigPath() (string, error) {
	// Check XDG_CONFIG_HOME first
	if xdgConfig := os.Getenv("XDG_CONFIG_HOME"); xdgConfig != "" {
		return filepath.Join(xdgConfig, "codemap", "config.yaml"), nil
	}

	// Fall back to ~/.config
	home, err := os.UserHomeDir()
	if err != nil {
		return "", err
	}

	return filepath.Join(home, ".config", "codemap", "config.yaml"), nil
}

// loadEnvFiles loads environment variables from .env files.
// Files are loaded in order of precedence (later files override earlier):
// 1. User-level: ~/.config/codemap/.env
// 2. Project-level: .env (current directory)
//
// Note: godotenv does NOT override existing environment variables,
// so shell-set variables always take precedence.
func loadEnvFiles() {
	// Load user-level .env first (lower priority)
	if home, err := os.UserHomeDir(); err == nil {
		userEnvPath := filepath.Join(home, ".config", "codemap", ".env")
		_ = godotenv.Load(userEnvPath) // Ignore errors (file may not exist)
	}

	// Check XDG_CONFIG_HOME
	if xdgConfig := os.Getenv("XDG_CONFIG_HOME"); xdgConfig != "" {
		xdgEnvPath := filepath.Join(xdgConfig, "codemap", ".env")
		_ = godotenv.Load(xdgEnvPath)
	}

	// Load project-level .env (higher priority, overrides user-level)
	_ = godotenv.Overload(".env")
}

// applyEnvOverrides applies environment variable overrides to the config.
func applyEnvOverrides(cfg *Config) {
	// Provider
	if v := os.Getenv("CODEMAP_LLM_PROVIDER"); v != "" {
		cfg.LLM.Provider = Provider(strings.ToLower(v))
	}

	// Model
	if v := os.Getenv("CODEMAP_LLM_MODEL"); v != "" {
		cfg.LLM.Model = v
	}

	// Ollama
	if v := os.Getenv("OLLAMA_HOST"); v != "" {
		cfg.LLM.OllamaURL = v
	}
	if v := os.Getenv("CODEMAP_OLLAMA_URL"); v != "" {
		cfg.LLM.OllamaURL = v
	}

	// OpenAI
	if v := os.Getenv("OPENAI_API_KEY"); v != "" {
		cfg.LLM.OpenAIAPIKey = v
	}
	if v := os.Getenv("OPENAI_BASE_URL"); v != "" {
		cfg.LLM.OpenAIBaseURL = v
	}

	// Anthropic
	if v := os.Getenv("ANTHROPIC_API_KEY"); v != "" {
		cfg.LLM.AnthropicAPIKey = v
	}

	// Gemini
	if v := os.Getenv("GEMINI_API_KEY"); v != "" {
		cfg.LLM.GeminiAPIKey = v
	}
	if v := os.Getenv("GEMINI_BASE_URL"); v != "" {
		cfg.LLM.GeminiBaseURL = v
	}

	// Embedding settings
	if v := os.Getenv("CODEMAP_EMBEDDING_MODEL"); v != "" {
		cfg.LLM.EmbeddingModel = v
	}
	if v := os.Getenv("CODEMAP_EMBEDDING_PROVIDER"); v != "" {
		cfg.LLM.EmbeddingProvider = v
	}

	// Debug
	if v := os.Getenv("CODEMAP_DEBUG"); v == "1" || strings.ToLower(v) == "true" {
		cfg.Debug = true
	}
}

// WriteDefault creates a default config file at the specified path.
func WriteDefault(path string) error {
	cfg := DefaultConfig()

	// Create parent directory if needed
	dir := filepath.Dir(path)
	if err := os.MkdirAll(dir, 0755); err != nil {
		return fmt.Errorf("creating config directory: %w", err)
	}

	data, err := yaml.Marshal(cfg)
	if err != nil {
		return fmt.Errorf("marshaling config: %w", err)
	}

	// Add header comment
	content := "# Codemap Configuration\n# See https://github.com/owner/codemap for documentation\n\n" + string(data)

	if err := os.WriteFile(path, []byte(content), 0644); err != nil {
		return fmt.Errorf("writing config file: %w", err)
	}

	return nil
}
</file>
<file path="development-docs/plans/01_knowledge_graph.md">
# Phase 1: Knowledge Graph Foundation

**Goal**: Transform `codemap` from a file-tree viewer to a graph-aware analysis tool.
**Focus**: Structural relationships, call graph extraction, and persistence.
**No LLM dependency in this phase.**

## 1. Core Data Structures (`graph/`)

- [ ] **Define Types**: Create `Node`, `Edge`, `NodeKind`, `EdgeKind` structs.
    - *Correction*: Ensure `NodeID` generation is strictly deterministic (`sha256(path + symbol)`).
- [ ] **Graph Storage**: Implement `CodeGraph` struct with map-based indexes (`nodesByID`, `edgesByFrom`).
- [ ] **Persistence**:
    - Implement `SaveBinary(path)` / `LoadBinary(path)` using `encoding/gob` for fast startup.
    - *Note*: Skip SQLite for now if `gob` is fast enough (<100ms for 100k nodes).

## 2. Scanner Enhancements (`scanner/`)

- [ ] **Refactor `walker.go`**: Decouple file scanning from data generation.
- [ ] **Call Extraction (`calls.go`)**:
    - Update Tree-sitter queries (`queries/*.scm`) to capture `call_expression`.
    - Implement `SyntacticCallGraph` extractor.
- [ ] **Edge Validation**:
    - Implement `ImportGraphFilter`: Eliminate calls to packages not imported by the file.
    - Implement `ArityFilter`: Eliminate calls where arg count mismatches (high precision).

## 3. Graph Builder (`graph/`)

- [ ] **Builder Logic**: Convert `FileAnalysis` (from scanner) into `Node` and `Edge` objects.
- [ ] **Delta Analysis**: Implement `IsGraphStale(root)` using file modification times to avoid unnecessary re-indexing.

## 4. CLI & MCP Tools

- [ ] **Command**: `codemap index` (Builds `.codemap/graph.gob`).
- [ ] **Command**: `codemap query --from <symbol> --to <symbol>` (Path tracing).
- [ ] **MCP Tool**: `get_structure` (Update to use Graph if available).
- [ ] **MCP Tool**: `trace_path` (New tool for finding connections).

## 5. Testing Strategy

- [ ] **Golden Corpus**: Create `scanner/testdata/corpus/` with polyglot files (Go, Py, TS).
- [ ] **Snapshot Tests**: Verify that the generated graph nodes/edges match a JSON snapshot exactly.
</file>
<file path="development-docs/plans/02_llm_integration.md">
# Phase 2: Semantic Intelligence (LLM)

**Goal**: Add semantic understanding and explanation capabilities.
**Focus**: LLM client integration, summarization pipeline, and configuration.
**Prerequisite**: Phase 1 completed.

## 1. Configuration System (`config/`)

- [ ] **Config Manager**: Implement loading from `~/.config/codemap/config.yaml`.
- [ ] **Secrets**: Handle API keys safely (Ollama URL, OpenAI Key).

## 2. LLM Client Layer (`analyze/`)

- [ ] **Interfaces**: Define `LLMClient` (Complete, Embed).
- [ ] **Ollama**: Implement client for local inference (preferred default).
- [ ] **OpenAI/Anthropic**: Implement clients for cloud inference.

## 3. Summarization Engine

- [ ] **Code Reader**: Implement `ReadSymbolSource(node)` to extract raw code efficiently.
- [ ] **Prompt Engineering**: Create robust prompts for "Explain this symbol" and "Summarize this module".
- [ ] **Caching Strategy**:
    - Implement `cache/` system storing JSON responses.
    - **Critical**: Use `ContentHash` of the symbol's source code as the cache invalidation key.

## 4. CLI & MCP Tools

- [ ] **Command**: `codemap explain <symbol>` (CLI wrapper for explanation).
- [ ] **MCP Tool**: `explain_symbol` (Returns structured summary + code context).
- [ ] **MCP Tool**: `summarize_module` (Aggregates summaries of files in a folder).

## 5. Testing

- [ ] **Mock LLM**: Create a mock client that returns fixed strings to test the pipeline without spending tokens/time.
</file>
<file path="development-docs/plans/03_hybrid_retrieval.md">
# Phase 3: Hybrid Retrieval & Search

**Goal**: Enable natural language search over the codebase.
**Focus**: Embeddings, Vector Search, and merging Structural + Semantic results.
**Prerequisite**: Phase 2 completed.

## 1. Vector Storage (`graph/vectors.go`)

- [ ] **Design Decision**: Use **pure Go** vector logic or a lightweight library (e.g., `github.com/philippgille/gokv`) instead of CGO SQLite extensions to maintain portability.
- [ ] **Storage**: Implement `VectorIndex` interface.
    - Simple flat file with `gob` serialization is likely sufficient for <50k vectors.
- [ ] **Search**: Implement Cosine Similarity search in memory.

## 2. Embedding Pipeline

- [ ] **Batch Processing**: Implement `EmbedNodes(nodes)` to batch calls to the embedding model.
- [ ] **Text Representation**: Define `NodeToText(node)` strategy (Signature + Docstring + Summary).

## 3. Hybrid Search Engine (`analyze/retriever.go`)

- [ ] **Algorithm**:
    1.  Get `top_k` from Vector Search (Semantic).
    2.  Get `top_k` from Graph Query (Name matching/fuzzy search).
    3.  Rank fusion (Reciprocal Rank Fusion or weighted score).
- [ ] **Context Expansion**: Retrieve connected nodes (Callers/Callees) for the top results.

## 4. CLI & MCP Tools

- [ ] **Command**: `codemap search "natural language query"`
- [ ] **MCP Tool**: `semantic_search` (The "Killer Feature" for agents).

## 5. Performance Tuning

- [ ] **Profile**: Ensure embedding generation doesn't block the UI/CLI.
- [ ] **Progress Bars**: Add TUI feedback for long-running embedding tasks.
</file>
<file path="development-docs/0001-enhanced-code-analysis-plan.md">
# PLAN.md - Enhanced Code Analysis Features

## Executive Summary

Este plano descreve a implementação de recursos avançados de análise de código para o `codemap`, focando em:
1. **Assinaturas de funções completas** (parâmetros e retornos)
2. **Extração de tipos** (structs, classes, interfaces, traits, enums)
3. **Modo API Surface** para visualização compacta de APIs públicas

O objetivo é fornecer um mapa mais rico para LLMs sem comprometer a token-efficiency que é o diferencial do codemap.

---

## Princípios de Design

### 1. Backward Compatibility
- Output atual deve permanecer inalterado por padrão
- Novas features ativadas via flags opcionais
- JSON schema deve ser extensível sem quebrar consumers

### 2. Token Efficiency
- Modo padrão continua mostrando apenas nomes
- Detalhes extras são opt-in via `--detail`
- Modo `--api` oferece visualização ultra-compacta

### 3. Consistência Semântica
- Tipos são categorizados por `Kind` semântico normalizado
- Cada linguagem mapeia seus constructos para Kinds universais
- Evita falsos cognatos (Rust `trait` ≠ Java `interface`)

---

## Arquitetura de Mudanças

```
┌─────────────────────────────────────────────────────────────────┐
│                         main.go                                  │
│  + --detail flag (0, 1, 2)                                      │
│  + --api flag                                                    │
└─────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────┐
│                      scanner/types.go                            │
│  + FuncInfo struct (Name, Signature, Receiver, IsExported)      │
│  + TypeInfo struct (Name, Kind, Fields, IsExported)             │
│  ~ FileAnalysis (Functions: []string → []FuncInfo)              │
│  ~ FileAnalysis (+ Types: []TypeInfo)                           │
└─────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────┐
│                     scanner/grammar.go                           │
│  ~ AnalyzeFile() - extended capture handling                    │
│  + buildSignature() - reconstructs signature from parts         │
│  + parseTypeInfo() - extracts type metadata                     │
│  + DetailLevel parameter propagation                            │
└─────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────┐
│                    scanner/queries/*.scm                         │
│  ~ All 16 language queries updated with:                        │
│    - @func.name, @func.params, @func.result, @func.receiver     │
│    - @type.name, @type.kind, @type.fields                       │
└─────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────┐
│                      render/depgraph.go                          │
│  ~ Depgraph() - conditional signature display                   │
│  + renderTypes() - type listing in output                       │
└─────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────┐
│                       render/api.go (NEW)                        │
│  + APIView() - compact public API visualization                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## Fase 1: Estruturas de Dados Base

### 1.1 Novos Tipos em `scanner/types.go`

```go
// DetailLevel controls how much information is extracted
type DetailLevel int

const (
    DetailNone      DetailLevel = 0 // Only names (current behavior)
    DetailSignature DetailLevel = 1 // Names + signatures
    DetailFull      DetailLevel = 2 // Signatures + type fields
)

// FuncInfo represents a function/method with optional detail
type FuncInfo struct {
    Name       string `json:"name"`
    Signature  string `json:"signature,omitempty"`  // Full signature when detail >= 1
    Receiver   string `json:"receiver,omitempty"`   // For methods (Go, Rust, etc.)
    IsExported bool   `json:"exported,omitempty"`   // Public visibility
}

// TypeKind represents normalized type categories across languages
type TypeKind string

const (
    KindStruct    TypeKind = "struct"    // Go struct, C struct, Rust struct
    KindClass     TypeKind = "class"     // Python/Java/C#/TS class
    KindInterface TypeKind = "interface" // Go interface, Java/C#/TS interface
    KindTrait     TypeKind = "trait"     // Rust trait
    KindEnum      TypeKind = "enum"      // All languages
    KindTypeAlias TypeKind = "alias"     // Go type alias, TS type
    KindProtocol  TypeKind = "protocol"  // Swift protocol
)

// TypeInfo represents a type definition
type TypeInfo struct {
    Name       string   `json:"name"`
    Kind       TypeKind `json:"kind"`
    Fields     []string `json:"fields,omitempty"`     // Field names when detail = 2
    Methods    []string `json:"methods,omitempty"`    // Method names (for classes)
    IsExported bool     `json:"exported,omitempty"`
}

// FileAnalysis holds extracted info about a single file for deps mode.
// UPDATED: Functions now use FuncInfo, added Types field
type FileAnalysis struct {
    Path      string     `json:"path"`
    Language  string     `json:"language"`
    Functions []FuncInfo `json:"functions"`           // CHANGED from []string
    Types     []TypeInfo `json:"types,omitempty"`     // NEW
    Imports   []string   `json:"imports"`
}
```

### 1.2 Compatibilidade JSON

Para manter backward compatibility na serialização JSON:

```go
// MarshalJSON customizes JSON output based on detail level
func (f FuncInfo) MarshalJSON() ([]byte, error) {
    // Se Signature está vazio, serializa apenas como string (comportamento antigo)
    if f.Signature == "" && f.Receiver == "" {
        return json.Marshal(f.Name)
    }
    // Caso contrário, serializa objeto completo
    type Alias FuncInfo
    return json.Marshal(Alias(f))
}
```

---

## Fase 2: Queries Tree-sitter Expandidas

### 2.1 Estratégia de Captura

**Problema:** Capturar `(function_declaration) @function` inclui o corpo inteiro.

**Solução:** Usar captures nomeados para componentes específicos:
- `@func.name` - nome da função
- `@func.params` - lista de parâmetros (texto do nó)
- `@func.result` - tipo de retorno
- `@func.receiver` - receiver para métodos

O código Go reconstruirá a signature a partir dos componentes.

### 2.2 Query Go (`scanner/queries/go.scm`)

```scheme
; ============================================
; GO QUERY - Enhanced for signatures and types
; ============================================

; --- FUNCTIONS ---

; Function declarations with full signature components
(function_declaration
  name: (identifier) @func.name
  parameters: (parameter_list) @func.params
  result: (_)? @func.result)

; Method declarations (functions with receivers)
(method_declaration
  receiver: (parameter_list) @func.receiver
  name: (field_identifier) @func.name
  parameters: (parameter_list) @func.params
  result: (_)? @func.result)

; --- TYPES ---

; Struct type definitions
(type_declaration
  (type_spec
    name: (type_identifier) @type.name
    type: (struct_type
      (field_declaration_list)? @type.fields))) @type.struct

; Interface type definitions
(type_declaration
  (type_spec
    name: (type_identifier) @type.name
    type: (interface_type
      (method_spec_list)? @type.methods))) @type.interface

; Type aliases
(type_declaration
  (type_spec
    name: (type_identifier) @type.name
    type: (type_identifier) @type.alias.target)) @type.alias

; --- IMPORTS (unchanged) ---
(import_spec
  path: (interpreted_string_literal) @import)
```

### 2.3 Query Python (`scanner/queries/python.scm`)

```scheme
; ============================================
; PYTHON QUERY - Enhanced for signatures and types
; ============================================

; --- FUNCTIONS ---

; Function definitions with parameters
(function_definition
  name: (identifier) @func.name
  parameters: (parameters) @func.params
  return_type: (type)? @func.result)

; Async function definitions
(function_definition
  "async"
  name: (identifier) @func.name
  parameters: (parameters) @func.params
  return_type: (type)? @func.result) @func.async

; --- CLASSES ---

; Class definitions
(class_definition
  name: (identifier) @type.name
  superclasses: (argument_list)? @type.bases) @type.class

; --- IMPORTS (unchanged) ---
(import_statement
  name: (dotted_name) @import)

(import_from_statement
  module_name: (dotted_name) @import)

(import_from_statement
  module_name: (relative_import) @import)
```

### 2.4 Query TypeScript (`scanner/queries/typescript.scm`)

```scheme
; ============================================
; TYPESCRIPT QUERY - Enhanced for signatures and types
; ============================================

; --- FUNCTIONS ---

; Function declarations
(function_declaration
  name: (identifier) @func.name
  parameters: (formal_parameters) @func.params
  return_type: (type_annotation)? @func.result)

; Arrow functions assigned to const/let
(lexical_declaration
  (variable_declarator
    name: (identifier) @func.name
    type: (type_annotation)? @func.result
    value: (arrow_function
      parameters: (formal_parameters) @func.params)))

; Method definitions in classes
(method_definition
  name: (property_identifier) @func.name
  parameters: (formal_parameters) @func.params
  return_type: (type_annotation)? @func.result)

; --- TYPES ---

; Interface declarations
(interface_declaration
  name: (type_identifier) @type.name
  body: (interface_body) @type.fields) @type.interface

; Class declarations
(class_declaration
  name: (type_identifier) @type.name
  body: (class_body) @type.fields) @type.class

; Type aliases
(type_alias_declaration
  name: (type_identifier) @type.name
  value: (_) @type.alias.target) @type.alias

; Enum declarations
(enum_declaration
  name: (identifier) @type.name
  body: (enum_body) @type.fields) @type.enum

; --- IMPORTS (unchanged) ---
(import_statement
  source: (string) @import)
```

### 2.5 Query Rust (`scanner/queries/rust.scm`)

```scheme
; ============================================
; RUST QUERY - Enhanced for signatures and types
; ============================================

; --- FUNCTIONS ---

; Function definitions
(function_item
  name: (identifier) @func.name
  parameters: (parameters) @func.params
  return_type: (_)? @func.result)

; Methods in impl blocks (capture self for receiver detection)
(impl_item
  type: (_) @func.receiver.type
  body: (declaration_list
    (function_item
      name: (identifier) @func.name
      parameters: (parameters) @func.params
      return_type: (_)? @func.result)))

; --- TYPES ---

; Struct definitions
(struct_item
  name: (type_identifier) @type.name
  body: (field_declaration_list)? @type.fields) @type.struct

; Enum definitions
(enum_item
  name: (type_identifier) @type.name
  body: (enum_variant_list) @type.fields) @type.enum

; Trait definitions
(trait_item
  name: (type_identifier) @type.name
  body: (declaration_list) @type.methods) @type.trait

; Type aliases
(type_item
  name: (type_identifier) @type.name
  type: (_) @type.alias.target) @type.alias

; --- IMPORTS (unchanged) ---
(use_declaration
  argument: (scoped_identifier) @import)

(use_declaration
  argument: (identifier) @import)

(mod_item
  name: (identifier) @module)
```

### 2.6 Query Java (`scanner/queries/java.scm`)

```scheme
; ============================================
; JAVA QUERY - Enhanced for signatures and types
; ============================================

; --- FUNCTIONS ---

; Method declarations
(method_declaration
  type: (_) @func.result
  name: (identifier) @func.name
  parameters: (formal_parameters) @func.params)

; Constructor declarations
(constructor_declaration
  name: (identifier) @func.name
  parameters: (formal_parameters) @func.params)

; --- TYPES ---

; Class declarations
(class_declaration
  name: (identifier) @type.name
  interfaces: (super_interfaces)? @type.implements
  body: (class_body) @type.fields) @type.class

; Interface declarations
(interface_declaration
  name: (identifier) @type.name
  body: (interface_body) @type.methods) @type.interface

; Enum declarations
(enum_declaration
  name: (identifier) @type.name
  body: (enum_body) @type.fields) @type.enum

; --- IMPORTS (unchanged) ---
(import_declaration
  (scoped_identifier) @import)
```

### 2.7 Query C# (`scanner/queries/c_sharp.scm`)

```scheme
; ============================================
; C# QUERY - Enhanced for signatures and types
; ============================================

; --- FUNCTIONS ---

; Method declarations
(method_declaration
  returns: (_) @func.result
  name: (identifier) @func.name
  parameters: (parameter_list) @func.params)

; Constructor declarations
(constructor_declaration
  name: (identifier) @func.name
  parameters: (parameter_list) @func.params)

; --- TYPES ---

; Class declarations
(class_declaration
  name: (identifier) @type.name
  bases: (base_list)? @type.bases
  body: (declaration_list) @type.fields) @type.class

; Interface declarations
(interface_declaration
  name: (identifier) @type.name
  body: (declaration_list) @type.methods) @type.interface

; Struct declarations
(struct_declaration
  name: (identifier) @type.name
  body: (declaration_list) @type.fields) @type.struct

; Enum declarations
(enum_declaration
  name: (identifier) @type.name
  body: (enum_member_declaration_list) @type.fields) @type.enum

; --- IMPORTS (unchanged) ---
(using_directive
  (qualified_name) @import)

(using_directive
  (identifier) @import)
```

---

## Fase 3: Lógica de Análise Expandida

### 3.1 Modificação em `scanner/grammar.go`

```go
// AnalyzeFile extracts functions, types, and imports
// detailLevel controls depth of extraction:
//   0 = names only (current behavior)
//   1 = names + signatures
//   2 = names + signatures + type fields
func (l *GrammarLoader) AnalyzeFile(filePath string, detailLevel DetailLevel) (*FileAnalysis, error) {
    lang := DetectLanguage(filePath)
    if lang == "" {
        return nil, nil
    }

    if err := l.LoadLanguage(lang); err != nil {
        return nil, nil
    }

    config := l.configs[lang]
    content, err := os.ReadFile(filePath)
    if err != nil {
        return nil, err
    }

    parser := tree_sitter.NewParser()
    defer parser.Close()
    parser.SetLanguage(config.Language)

    tree := parser.Parse(content, nil)
    defer tree.Close()

    cursor := tree_sitter.NewQueryCursor()
    defer cursor.Close()

    analysis := &FileAnalysis{Path: filePath, Language: lang}

    // Temporary storage for building composite captures
    funcBuilder := make(map[uint32]*funcCapture) // match_id -> components
    typeBuilder := make(map[uint32]*typeCapture)

    matches := cursor.Matches(config.Query, tree.RootNode(), content)
    for match := matches.Next(); match != nil; match = matches.Next() {
        for _, capture := range match.Captures {
            name := config.Query.CaptureNames()[capture.Index]
            text := strings.Trim(capture.Node.Utf8Text(content), `"'`)

            // Route to appropriate handler based on capture name prefix
            switch {
            case strings.HasPrefix(name, "func."):
                handleFuncCapture(funcBuilder, match.ID, name, text, capture.Node)
            case strings.HasPrefix(name, "type."):
                handleTypeCapture(typeBuilder, match.ID, name, text, capture.Node, detailLevel)
            case name == "import" || name == "module":
                analysis.Imports = append(analysis.Imports, text)
            // Legacy support: plain @function capture
            case name == "function" || name == "method":
                analysis.Functions = append(analysis.Functions, FuncInfo{Name: text})
            }
        }
    }

    // Build final function list from captured components
    for _, fc := range funcBuilder {
        funcInfo := fc.Build(detailLevel, lang)
        analysis.Functions = append(analysis.Functions, funcInfo)
    }

    // Build final type list
    for _, tc := range typeBuilder {
        typeInfo := tc.Build(detailLevel)
        analysis.Types = append(analysis.Types, typeInfo)
    }

    analysis.Functions = dedupeFuncs(analysis.Functions)
    analysis.Types = dedupeTypes(analysis.Types)
    analysis.Imports = dedupe(analysis.Imports)

    return analysis, nil
}

// funcCapture collects components of a function signature
type funcCapture struct {
    name     string
    params   string
    result   string
    receiver string
    node     *tree_sitter.Node
}

// Build constructs FuncInfo from captured components
func (fc *funcCapture) Build(detail DetailLevel, lang string) FuncInfo {
    info := FuncInfo{
        Name:       fc.name,
        IsExported: isExported(fc.name, lang),
    }

    if detail >= DetailSignature && fc.params != "" {
        info.Signature = buildSignature(fc, lang)
    }

    if fc.receiver != "" {
        info.Receiver = fc.receiver
    }

    return info
}

// buildSignature reconstructs function signature from components
func buildSignature(fc *funcCapture, lang string) string {
    var sig strings.Builder

    switch lang {
    case "go":
        sig.WriteString("func ")
        if fc.receiver != "" {
            sig.WriteString(fc.receiver)
            sig.WriteString(" ")
        }
        sig.WriteString(fc.name)
        sig.WriteString(fc.params)
        if fc.result != "" {
            sig.WriteString(" ")
            sig.WriteString(fc.result)
        }

    case "python":
        sig.WriteString("def ")
        sig.WriteString(fc.name)
        sig.WriteString(fc.params)
        if fc.result != "" {
            sig.WriteString(" -> ")
            sig.WriteString(fc.result)
        }

    case "typescript", "javascript":
        sig.WriteString("function ")
        sig.WriteString(fc.name)
        sig.WriteString(fc.params)
        if fc.result != "" {
            sig.WriteString(": ")
            sig.WriteString(fc.result)
        }

    case "rust":
        sig.WriteString("fn ")
        sig.WriteString(fc.name)
        sig.WriteString(fc.params)
        if fc.result != "" {
            sig.WriteString(" -> ")
            sig.WriteString(fc.result)
        }

    case "java", "c_sharp":
        if fc.result != "" {
            sig.WriteString(fc.result)
            sig.WriteString(" ")
        }
        sig.WriteString(fc.name)
        sig.WriteString(fc.params)

    default:
        sig.WriteString(fc.name)
        sig.WriteString(fc.params)
    }

    return sig.String()
}

// isExported checks if a symbol is publicly visible
func isExported(name, lang string) bool {
    if name == "" {
        return false
    }

    switch lang {
    case "go":
        // Go: exported if starts with uppercase
        r := []rune(name)
        return unicode.IsUpper(r[0])

    case "python":
        // Python: exported if doesn't start with _
        return !strings.HasPrefix(name, "_")

    case "rust":
        // Rust: would need `pub` keyword analysis
        // For now, assume all are potentially public
        return true

    default:
        // Most languages: assume public unless private keyword
        return true
    }
}

// typeCapture collects components of a type definition
type typeCapture struct {
    name   string
    kind   TypeKind
    fields string // Raw text of fields block
    node   *tree_sitter.Node
}

// Build constructs TypeInfo from captured components
func (tc *typeCapture) Build(detail DetailLevel) TypeInfo {
    info := TypeInfo{
        Name: tc.name,
        Kind: tc.kind,
    }

    if detail >= DetailFull && tc.fields != "" {
        info.Fields = parseFieldNames(tc.fields)
    }

    return info
}

// parseFieldNames extracts field/member names from raw block text
func parseFieldNames(fieldsText string) []string {
    // Simple extraction: find identifiers at start of lines
    var fields []string
    lines := strings.Split(fieldsText, "\n")

    for _, line := range lines {
        line = strings.TrimSpace(line)
        if line == "" || line == "{" || line == "}" {
            continue
        }

        // Extract first identifier (field name)
        parts := strings.Fields(line)
        if len(parts) > 0 {
            name := strings.TrimSuffix(parts[0], ":")
            name = strings.TrimSuffix(name, ",")
            if name != "" && !strings.HasPrefix(name, "//") {
                fields = append(fields, name)
            }
        }
    }

    return fields
}
```

### 3.2 Handler para Captures

```go
// handleFuncCapture routes function-related captures to builder
func handleFuncCapture(builders map[uint32]*funcCapture, matchID uint32, name, text string, node *tree_sitter.Node) {
    if builders[matchID] == nil {
        builders[matchID] = &funcCapture{}
    }
    fc := builders[matchID]

    switch name {
    case "func.name":
        fc.name = text
    case "func.params":
        fc.params = text
    case "func.result":
        fc.result = text
    case "func.receiver":
        fc.receiver = text
    }
    fc.node = node
}

// handleTypeCapture routes type-related captures to builder
func handleTypeCapture(builders map[uint32]*typeCapture, matchID uint32, name, text string, node *tree_sitter.Node, detail DetailLevel) {
    if builders[matchID] == nil {
        builders[matchID] = &typeCapture{}
    }
    tc := builders[matchID]

    switch name {
    case "type.name":
        tc.name = text
    case "type.fields", "type.methods":
        if detail >= DetailFull {
            tc.fields = text
        }
    case "type.struct":
        tc.kind = KindStruct
    case "type.class":
        tc.kind = KindClass
    case "type.interface":
        tc.kind = KindInterface
    case "type.trait":
        tc.kind = KindTrait
    case "type.enum":
        tc.kind = KindEnum
    case "type.alias":
        tc.kind = KindTypeAlias
    case "type.protocol":
        tc.kind = KindProtocol
    }
    tc.node = node
}
```

---

## Fase 4: CLI e Flags

### 4.1 Modificação em `main.go`

```go
func main() {
    // Existing flags...
    skylineMode := flag.Bool("skyline", false, "Enable skyline visualization mode")
    animateMode := flag.Bool("animate", false, "Enable animation (use with --skyline)")
    depsMode := flag.Bool("deps", false, "Enable dependency graph mode (function/import analysis)")
    diffMode := flag.Bool("diff", false, "Only show files changed vs main (or use --ref to specify branch)")
    diffRef := flag.String("ref", "main", "Branch/ref to compare against (use with --diff)")
    jsonMode := flag.Bool("json", false, "Output JSON (for Python renderer compatibility)")
    debugMode := flag.Bool("debug", false, "Show debug info (gitignore loading, paths, etc.)")
    helpMode := flag.Bool("help", false, "Show help")

    // NEW FLAGS
    detailLevel := flag.Int("detail", 0, "Detail level: 0=names, 1=signatures, 2=full (use with --deps)")
    apiMode := flag.Bool("api", false, "Show public API surface only (compact view)")

    flag.Parse()

    // ... rest of main
}

func runDepsMode(absRoot, root string, gitignore *ignore.GitIgnore, jsonMode bool, diffRef string, changedFiles map[string]bool, detailLevel int, apiMode bool) {
    loader := scanner.NewGrammarLoader()

    // ... grammar check ...

    analyses, err := scanner.ScanForDeps(root, gitignore, loader, scanner.DetailLevel(detailLevel))
    if err != nil {
        fmt.Fprintf(os.Stderr, "Error scanning for deps: %v\n", err)
        os.Exit(1)
    }

    // ... filter changed files ...

    depsProject := scanner.DepsProject{
        Root:         absRoot,
        Mode:         "deps",
        Files:        analyses,
        ExternalDeps: scanner.ReadExternalDeps(absRoot),
        DiffRef:      diffRef,
        DetailLevel:  detailLevel, // NEW field
    }

    if jsonMode {
        json.NewEncoder(os.Stdout).Encode(depsProject)
    } else if apiMode {
        render.APIView(depsProject) // NEW renderer
    } else {
        render.Depgraph(depsProject)
    }
}
```

### 4.2 Help Text Atualizado

```go
if *helpMode {
    fmt.Println("codemap - Generate a brain map of your codebase for LLM context")
    fmt.Println()
    fmt.Println("Usage: codemap [options] [path]")
    fmt.Println()
    fmt.Println("Options:")
    fmt.Println("  --help              Show this help message")
    fmt.Println("  --skyline           City skyline visualization")
    fmt.Println("  --animate           Animated skyline (use with --skyline)")
    fmt.Println("  --deps              Dependency flow map (functions & imports)")
    fmt.Println("  --detail <level>    Detail level for --deps: 0=names, 1=signatures, 2=full")
    fmt.Println("  --api               Show public API surface (compact, use with --deps)")
    fmt.Println("  --diff              Only show files changed vs main")
    fmt.Println("  --ref <branch>      Branch to compare against (default: main)")
    fmt.Println()
    fmt.Println("Examples:")
    fmt.Println("  codemap .                       # Basic tree view")
    fmt.Println("  codemap --deps .                # Dependency flow (names only)")
    fmt.Println("  codemap --deps --detail 1 .    # Dependencies with signatures")
    fmt.Println("  codemap --deps --api .         # Public API surface view")
    fmt.Println("  codemap --deps --detail 2 .    # Full detail with type fields")
    os.Exit(0)
}
```

---

## Fase 5: Renderização

### 5.1 Modificação em `render/depgraph.go`

```go
// Depgraph renders the dependency flow visualization
func Depgraph(project scanner.DepsProject) {
    files := project.Files
    detailLevel := project.DetailLevel

    // ... existing code ...

    // Render each system
    for _, system := range systemNames {
        sysFiles := systems[system]
        systemName := getSystemName(system)

        // ... existing header code ...

        for _, f := range sysFiles {
            basename := filepath.Base(f.Path)
            nameNoExt := extPattern.ReplaceAllString(basename, "")

            // NEW: Show types if present
            if len(f.Types) > 0 && detailLevel >= 1 {
                renderTypes(f.Types, detailLevel)
            }

            // ... existing dependency rendering ...

            // NEW: Show function signatures if detail level warrants
            if detailLevel >= 1 {
                renderFunctionsDetailed(f.Functions)
            }
        }
    }

    // ... existing summary code ...
}

// renderTypes displays type definitions
func renderTypes(types []scanner.TypeInfo, detail int) {
    for _, t := range types {
        icon := typeIcon(t.Kind)
        if detail >= 2 && len(t.Fields) > 0 {
            fmt.Printf("    %s %s { %s }\n", icon, t.Name, strings.Join(t.Fields, ", "))
        } else {
            fmt.Printf("    %s %s\n", icon, t.Name)
        }
    }
}

// typeIcon returns an ASCII icon for the type kind
func typeIcon(kind scanner.TypeKind) string {
    switch kind {
    case scanner.KindStruct:
        return "[S]"
    case scanner.KindClass:
        return "[C]"
    case scanner.KindInterface:
        return "[I]"
    case scanner.KindTrait:
        return "[T]"
    case scanner.KindEnum:
        return "[E]"
    case scanner.KindTypeAlias:
        return "[A]"
    case scanner.KindProtocol:
        return "[P]"
    default:
        return "[?]"
    }
}

// renderFunctionsDetailed shows functions with signatures
func renderFunctionsDetailed(funcs []scanner.FuncInfo) {
    for _, f := range funcs {
        if f.Signature != "" {
            fmt.Printf("      ƒ %s\n", f.Signature)
        } else {
            fmt.Printf("      ƒ %s()\n", f.Name)
        }
    }
}
```

### 5.2 Novo arquivo `render/api.go`

```go
package render

import (
    "fmt"
    "path/filepath"
    "sort"
    "strings"

    "codemap/scanner"
)

// APIView renders a compact public API surface view
func APIView(project scanner.DepsProject) {
    files := project.Files
    projectName := filepath.Base(project.Root)

    if len(files) == 0 {
        fmt.Println("  No source files found.")
        return
    }

    fmt.Println()
    fmt.Printf("=== API Surface: %s ===\n", projectName)
    fmt.Println()

    // Group by directory
    packages := make(map[string][]scanner.FileAnalysis)
    for _, f := range files {
        dir := filepath.Dir(f.Path)
        if dir == "." {
            dir = projectName
        }
        packages[dir] = append(packages[dir], f)
    }

    // Sort package names
    var pkgNames []string
    for name := range packages {
        pkgNames = append(pkgNames, name)
    }
    sort.Strings(pkgNames)

    for _, pkg := range pkgNames {
        pkgFiles := packages[pkg]

        // Collect all exported types and functions
        var exportedTypes []scanner.TypeInfo
        var exportedFuncs []scanner.FuncInfo

        for _, f := range pkgFiles {
            for _, t := range f.Types {
                if t.IsExported {
                    exportedTypes = append(exportedTypes, t)
                }
            }
            for _, fn := range f.Functions {
                if fn.IsExported {
                    exportedFuncs = append(exportedFuncs, fn)
                }
            }
        }

        // Skip packages with no exports
        if len(exportedTypes) == 0 && len(exportedFuncs) == 0 {
            continue
        }

        fmt.Printf("%s/\n", pkg)

        // Group methods by receiver type
        methodsByType := make(map[string][]scanner.FuncInfo)
        var standaloneFuncs []scanner.FuncInfo

        for _, fn := range exportedFuncs {
            if fn.Receiver != "" {
                // Extract type name from receiver
                typeName := extractTypeName(fn.Receiver)
                methodsByType[typeName] = append(methodsByType[typeName], fn)
            } else {
                standaloneFuncs = append(standaloneFuncs, fn)
            }
        }

        // Print types with their methods
        for _, t := range exportedTypes {
            icon := typeIcon(t.Kind)

            if len(t.Fields) > 0 {
                fmt.Printf("  %s %s {%s}\n", icon, t.Name, strings.Join(t.Fields, ", "))
            } else {
                fmt.Printf("  %s %s\n", icon, t.Name)
            }

            // Print methods for this type
            if methods, ok := methodsByType[t.Name]; ok {
                for _, m := range methods {
                    if m.Signature != "" {
                        fmt.Printf("    + %s\n", m.Signature)
                    } else {
                        fmt.Printf("    + (%s) %s()\n", m.Receiver, m.Name)
                    }
                }
            }
        }

        // Print standalone functions
        if len(standaloneFuncs) > 0 {
            for _, fn := range standaloneFuncs {
                if fn.Signature != "" {
                    fmt.Printf("  + %s\n", fn.Signature)
                } else {
                    fmt.Printf("  + %s()\n", fn.Name)
                }
            }
        }

        fmt.Println()
    }

    // Summary
    totalTypes := 0
    totalFuncs := 0
    for _, f := range files {
        for _, t := range f.Types {
            if t.IsExported {
                totalTypes++
            }
        }
        for _, fn := range f.Functions {
            if fn.IsExported {
                totalFuncs++
            }
        }
    }

    fmt.Printf("─────────────────────────────────────────────────────────────\n")
    fmt.Printf("Exported: %d types · %d functions\n", totalTypes, totalFuncs)
    fmt.Println()
}

// extractTypeName extracts type name from receiver like "(l *GrammarLoader)"
func extractTypeName(receiver string) string {
    // Remove parentheses
    r := strings.Trim(receiver, "()")

    // Split by space, take last part (the type)
    parts := strings.Fields(r)
    if len(parts) == 0 {
        return ""
    }

    typePart := parts[len(parts)-1]
    // Remove pointer asterisk
    return strings.TrimPrefix(typePart, "*")
}
```

---

## Fase 6: MCP Server Updates

### 6.1 Modificação em `mcp/main.go`

Atualizar as tools MCP para suportar o novo detail level:

```go
// get_dependencies tool - add detail parameter
case "get_dependencies":
    path := getStringArg(args, "path", ".")
    detail := getIntArg(args, "detail", 0) // NEW

    absPath, _ := filepath.Abs(path)
    gitignore := scanner.LoadGitignore(absPath)
    loader := scanner.NewGrammarLoader()

    if !loader.HasGrammars() {
        return mcp.NewToolResultError("No tree-sitter grammars found")
    }

    analyses, err := scanner.ScanForDeps(absPath, gitignore, loader, scanner.DetailLevel(detail))
    if err != nil {
        return mcp.NewToolResultError(err.Error())
    }

    project := scanner.DepsProject{
        Root:         absPath,
        Mode:         "deps",
        Files:        analyses,
        ExternalDeps: scanner.ReadExternalDeps(absPath),
        DetailLevel:  detail,
    }

    result, _ := json.MarshalIndent(project, "", "  ")
    return mcp.NewToolResultText(string(result))
```

---

## Fase 7: Testes

### 7.1 Estrutura de Testes

```
scanner/
  grammar_test.go      # Test signature extraction
  types_test.go        # Test type parsing

render/
  api_test.go          # Test API view rendering

testdata/
  go/
    sample.go          # Test Go parsing
  python/
    sample.py          # Test Python parsing
  typescript/
    sample.ts          # Test TS parsing
  rust/
    sample.rs          # Test Rust parsing
```

### 7.2 Test Cases para Go

```go
// scanner/grammar_test.go
func TestAnalyzeGoSignatures(t *testing.T) {
    loader := NewGrammarLoader()

    // Create temp file with test content
    content := `package test

type Config struct {
    Name string
    Port int
}

type Handler interface {
    Handle(ctx context.Context, req Request) (Response, error)
}

func NewConfig(name string, port int) *Config {
    return &Config{Name: name, Port: port}
}

func (c *Config) Validate() error {
    if c.Port < 0 {
        return errors.New("invalid port")
    }
    return nil
}
`
    tmpFile := createTempFile(t, "test.go", content)
    defer os.Remove(tmpFile)

    // Test detail level 0 (names only)
    analysis, err := loader.AnalyzeFile(tmpFile, DetailNone)
    require.NoError(t, err)
    require.Len(t, analysis.Functions, 2)
    assert.Equal(t, "NewConfig", analysis.Functions[0].Name)
    assert.Empty(t, analysis.Functions[0].Signature)

    // Test detail level 1 (signatures)
    analysis, err = loader.AnalyzeFile(tmpFile, DetailSignature)
    require.NoError(t, err)
    require.Len(t, analysis.Functions, 2)
    assert.Equal(t, "func NewConfig(name string, port int) *Config", analysis.Functions[0].Signature)
    assert.Equal(t, "func (c *Config) Validate() error", analysis.Functions[1].Signature)

    // Test types
    require.Len(t, analysis.Types, 2)
    assert.Equal(t, "Config", analysis.Types[0].Name)
    assert.Equal(t, KindStruct, analysis.Types[0].Kind)
    assert.Equal(t, "Handler", analysis.Types[1].Name)
    assert.Equal(t, KindInterface, analysis.Types[1].Kind)

    // Test detail level 2 (fields)
    analysis, err = loader.AnalyzeFile(tmpFile, DetailFull)
    require.NoError(t, err)
    assert.Contains(t, analysis.Types[0].Fields, "Name")
    assert.Contains(t, analysis.Types[0].Fields, "Port")
}
```

### 7.3 Test Cases para Python

```go
func TestAnalyzePythonSignatures(t *testing.T) {
    content := `
class UserService:
    def __init__(self, db: Database):
        self.db = db

    async def get_user(self, user_id: int) -> User:
        return await self.db.get(user_id)

    def _private_method(self):
        pass

def create_app(config: Config) -> Application:
    return Application(config)
`
    tmpFile := createTempFile(t, "test.py", content)
    defer os.Remove(tmpFile)

    loader := NewGrammarLoader()
    analysis, err := loader.AnalyzeFile(tmpFile, DetailSignature)
    require.NoError(t, err)

    // Check class
    require.Len(t, analysis.Types, 1)
    assert.Equal(t, "UserService", analysis.Types[0].Name)
    assert.Equal(t, KindClass, analysis.Types[0].Kind)

    // Check functions
    require.Len(t, analysis.Functions, 4)

    // Check exported detection
    var exported []string
    for _, f := range analysis.Functions {
        if f.IsExported {
            exported = append(exported, f.Name)
        }
    }
    assert.Contains(t, exported, "__init__")
    assert.Contains(t, exported, "get_user")
    assert.Contains(t, exported, "create_app")
    assert.NotContains(t, exported, "_private_method")
}
```

---

## Cronograma de Implementação

### Sprint 1: Fundação (Types + Basic Signatures)
- [ ] Implementar `FuncInfo` e `TypeInfo` em `types.go`
- [ ] Atualizar `FileAnalysis` struct
- [ ] Implementar JSON marshaling customizado
- [ ] Adicionar flag `--detail` em `main.go`
- [ ] Atualizar `ScanForDeps` para aceitar `DetailLevel`

### Sprint 2: Go + Python Queries
- [ ] Reescrever `go.scm` com captures granulares
- [ ] Reescrever `python.scm` com captures granulares
- [ ] Implementar `handleFuncCapture` e `handleTypeCapture`
- [ ] Implementar `buildSignature` para Go e Python
- [ ] Escrever testes unitários

### Sprint 3: TypeScript/JavaScript + Rust
- [ ] Reescrever `typescript.scm`
- [ ] Reescrever `javascript.scm`
- [ ] Reescrever `rust.scm`
- [ ] Implementar `buildSignature` para TS/JS/Rust
- [ ] Adicionar testes

### Sprint 4: Java + C# + Demais
- [ ] Reescrever `java.scm`
- [ ] Reescrever `c_sharp.scm`
- [ ] Atualizar queries restantes (C, C++, Swift, etc.)
- [ ] Implementar `buildSignature` para linguagens restantes

### Sprint 5: Renderização + API View
- [ ] Atualizar `depgraph.go` para signatures
- [ ] Implementar `render/api.go`
- [ ] Adicionar flag `--api`
- [ ] Testes de integração

### Sprint 6: MCP + Polish
- [ ] Atualizar MCP tools com `detail` parameter
- [ ] Atualizar documentação
- [ ] Performance testing
- [ ] Bug fixes finais

---

## Riscos e Mitigações

| Risco | Probabilidade | Impacto | Mitigação |
|-------|--------------|---------|-----------|
| Tree-sitter query syntax errors | Alta | Médio | Testar cada query isoladamente com playground |
| Quebra de backward compatibility | Média | Alto | Custom JSON marshaling mantém output antigo |
| Performance degradation | Baixa | Médio | DetailLevel 0 usa código otimizado atual |
| Queries inconsistentes entre linguagens | Alta | Médio | Definir spec clara de captures obrigatórios |

---

## Definições de Pronto

### Feature "Completa" quando:
1. Todas as 16 linguagens suportam o feature
2. Testes unitários cobrem casos de borda
3. JSON output é backward compatible
4. Documentação atualizada no README
5. MCP server atualizado

### Aceitação:
- `codemap --deps .` funciona identicamente ao atual
- `codemap --deps --detail 1 .` mostra signatures
- `codemap --deps --api .` mostra API surface compacta
- JSON output é parseável por consumers existentes

---

## Notas de Implementação

### Ordem de Prioridade das Linguagens
1. **Go** - Linguagem do projeto, mais fácil de testar
2. **Python** - Alta demanda, syntax clara
3. **TypeScript** - Muito usado, tipos explícitos
4. **Rust** - Syntax complexa mas bem definida
5. **Java/C#** - Similar entre si
6. **C/C++** - Mais complexo, menor prioridade

### Padrões de Código
- Preferir funções puras para transformações
- Usar table-driven tests
- Manter cada query file sob 100 linhas se possível
- Documentar edge cases nas queries com comentários

---

*Documento criado em: 2024-12-03*
*Última atualização: 2024-12-03*
*Autor: Claude Code Analysis*
</file>
<file path="development-docs/0001-enhanced-code-analysis.md">
# TASKS.md - Enhanced Code Analysis Features

## Project Briefing

**Objective:** Implement advanced code analysis features for `codemap` including:
1. Full function signatures (parameters and return types)
2. Type extraction (structs, classes, interfaces, traits, enums)
3. API Surface mode for compact public API visualization

**Design Principles:**
- Backward compatibility - existing output unchanged by default
- Token efficiency - new details are opt-in via `--detail` flag
- Semantic consistency - types normalized across languages

**Key Files Modified:**
- `scanner/types.go` - New data structures
- `scanner/grammar.go` - Enhanced analysis logic
- `scanner/queries/*.scm` - Updated tree-sitter queries
- `render/depgraph.go` - Updated rendering
- `render/api.go` - New API surface view
- `main.go` - New CLI flags
- `mcp/main.go` - MCP server updates

---

## Phase 1: Base Data Structures

### 1.1 New Types in scanner/types.go
- [x] Add `DetailLevel` type and constants (DetailNone=0, DetailSignature=1, DetailFull=2)
- [x] Add `FuncInfo` struct (Name, Signature, Receiver, IsExported)
- [x] Add `TypeKind` type and constants (struct, class, interface, trait, enum, alias, protocol)
- [x] Add `TypeInfo` struct (Name, Kind, Fields, Methods, IsExported)
- [x] Update `FileAnalysis` struct (Functions: []FuncInfo, add Types: []TypeInfo)
- [x] Add `DetailLevel` field to `DepsProject` struct

### 1.2 JSON Backward Compatibility
- [x] Implement custom `MarshalJSON` for `FuncInfo` (serialize as string when no signature)
- [x] Implement custom `UnmarshalJSON` for backward compatibility
- [x] Ensure existing JSON consumers still work

### 1.3 CLI Flags in main.go
- [x] Add `--detail` flag (int, 0-2)
- [x] Add `--api` flag (bool)
- [x] Update help text with new flags
- [x] Wire flags through to `runDepsMode`

### 1.4 Scanner Integration
- [x] Update `ScanForDeps` to accept `DetailLevel` parameter
- [x] Update `AnalyzeFile` to accept `DetailLevel` parameter
- [x] Propagate detail level through the analysis pipeline

### 1.5 Validation
- [x] Build succeeds: `go build -o codemap .`
- [x] `./codemap --deps .` still works (backward compatibility)
- [x] `./codemap --help` shows new flags

---

## Phase 2: Go + Python Queries

### 2.1 Go Query (scanner/queries/go.scm)
- [x] Rewrite query with granular captures:
  - [x] `@func.name`, `@func.params` for functions
  - [x] `@func.receiver` for methods
  - [x] `@type.name`, `@type.struct`, `@type.interface`
- [x] Keep existing `@import` captures

### 2.2 Python Query (scanner/queries/python.scm)
- [x] Rewrite query with granular captures:
  - [x] `@func.name`, `@func.params` for functions
  - [x] `@type.name`, `@type.class`
- [x] Keep existing `@import` captures

### 2.3 Analysis Logic in scanner/grammar.go
- [x] Add `funcCapture` struct for collecting function components
- [x] Add `typeCapture` struct for collecting type components
- [x] Implement `handleFuncCapture` function
- [x] Implement `handleTypeCapture` function
- [x] Implement `buildSignature` function for Go and Python
- [x] Implement `IsExportedName` function for Go and Python
- [x] Add deduplication helpers: `dedupeFuncs`, `dedupeTypes`

### 2.4 Validation
- [x] Build succeeds
- [x] Test Go file with `--detail 0` (names only)
- [x] Test Go file with `--detail 1` (shows signatures)
- [x] JSON output is valid and backward compatible

---

## Phase 3: TypeScript + JavaScript + Rust Queries

### 3.1 TypeScript Query (scanner/queries/typescript.scm)
- [x] Rewrite query with granular captures for:
  - [x] Functions with parameters
  - [x] Arrow functions
  - [x] Method definitions
  - [x] Interfaces, classes, type aliases, enums

### 3.2 JavaScript Query (scanner/queries/javascript.scm)
- [x] Rewrite query with granular captures

### 3.3 Rust Query (scanner/queries/rust.scm)
- [x] Rewrite query with granular captures for:
  - [x] Functions with parameters
  - [x] Structs, enums, traits

---

## Phase 4: Java + C# Queries

### 4.1 Java Query (scanner/queries/java.scm)
- [x] Rewrite query with granular captures:
  - [x] Methods and constructors with params
  - [x] Classes, interfaces, enums

### 4.2 C# Query (scanner/queries/c_sharp.scm)
- [x] Rewrite query with granular captures:
  - [x] Methods and constructors
  - [x] Classes, interfaces, structs, enums

---

## Phase 5: Rendering Updates

### 5.1 Update render/depgraph.go
- [x] Update summary to show type counts

### 5.2 Create render/api.go
- [x] Implement `APIView` function:
  - [x] Group files by package/directory
  - [x] Filter to exported types and functions only
  - [x] Group methods by their receiver type
  - [x] Render compact view with type icons
- [x] Implement `typeIcon` helper for ASCII type icons ([S], [C], [I], etc.)
- [x] Implement `extractTypeName` helper for receiver parsing
- [x] Add summary statistics (exported types/functions count)

### 5.3 Wire API Mode in main.go
- [x] Call `render.APIView` when `--api` flag is set

---

## Phase 6: MCP Server Updates

### 6.1 Update mcp/main.go
- [x] Add `DepsInput` type with `detail` parameter
- [x] Update `get_dependencies` tool to use new input type
- [x] Pass detail level to `ScanForDeps`
- [x] Update tool description for new parameter

### 6.2 Validation
- [x] Build MCP server: `go build -o codemap-mcp ./mcp/`

---

## Phase 7: Testing & Documentation

### 7.1 Manual Testing
- [x] Test on codemap itself with all flags
- [x] Test tree mode (basic view)
- [x] Test deps mode (detail=0)
- [x] Test deps mode with signatures (detail=1)
- [x] Test API surface mode

### 7.2 Final Validation
- [x] All existing modes still work (tree, deps, diff, skyline)
- [x] Build both binaries successfully
- [x] Run `go fmt ./...`

---

## Completion Summary

**Status:** [x] Completed

| Phase | Description | Status |
|-------|-------------|--------|
| 1 | Base Data Structures | [x] Complete |
| 2 | Go + Python Queries | [x] Complete |
| 3 | TypeScript + Rust Queries | [x] Complete |
| 4 | Java + C# Queries | [x] Complete |
| 5 | Rendering Updates | [x] Complete |
| 6 | MCP Server Updates | [x] Complete |
| 7 | Testing & Documentation | [x] Complete |

### Implementation Notes

1. **Function Signatures**: Function parameters are captured via `@func.params` and methods include receivers via `@func.receiver`. Signatures are built using the `buildSignature` function in `grammar.go`.

2. **Type Extraction**: Types are captured with pattern markers (`@type.struct`, `@type.class`, etc.) that set the `Kind` field in `TypeInfo`. Type fields are not extracted in this implementation to keep queries simple.

3. **Return Types**: Due to tree-sitter query syntax limitations with optional fields, function return types are not captured. Signatures show function name and parameters only.

4. **Backward Compatibility**: `FuncInfo.MarshalJSON()` serializes as plain string when no extended info is present, ensuring existing JSON consumers continue to work.

5. **Queries Updated**: Go, Python, TypeScript, JavaScript, Rust, Java, and C# queries have been updated with the new capture patterns. Other languages (C, C++, Swift, etc.) continue to use legacy `@function` captures.

---

*Created: 2024-12-03*
*Completed: 2024-12-03*
</file>
<file path="development-docs/0002-token-heuristics-symbol-search-plan.md">
# Refactoring/Design Plan: Token Heuristics, `get_symbol` Tool e Melhorias MCP

> **Versão:** 2.0 (Otimizado)
> **Data:** 2025-12-03
> **Status:** Pronto para implementação

---

## 1. Executive Summary & Goals

O objetivo principal é **melhorar a utilidade do `codemap` para agentes LLM**, focando em:

1. **Context Management (Tokens):** Estimativa de tokens por arquivo com alertas visuais (P0)
2. **Semantic Search (`get_symbol`):** Busca precisa de funções/tipos por nome (P0)
3. **MCP Enhancement:** Exposição do modo `--api` e refatoração de path validation (P1)

### Mudanças-Chave vs. Plano Original

| Aspecto | Plano Original | Plano Otimizado |
|---------|----------------|-----------------|
| Token Heuristics | Map `TokenRatios` por linguagem | Ratio fixo universal (3.5 chars/token) |
| Captura de Linha | Mencionada como "questão aberta" | **Fase 0 obrigatória** - Prerequisito |
| `SymbolInfo` | Novo struct com campos duplicados | Reutiliza `FuncInfo`/`TypeInfo` + campo `Line` |
| Formato de Saída | "ASCII formatado" (vago) | Especificação exata definida |

---

## 2. Current Situation Analysis

### Arquitetura Existente

```
scanner/types.go     → FileInfo, FuncInfo, TypeInfo, FileAnalysis
scanner/walker.go    → ScanFiles(), ScanForDeps()
scanner/grammar.go   → AnalyzeFile(), funcCapture, typeCapture
render/tree.go       → Tree()
render/api.go        → APIView()
mcp/main.go          → 7 handlers existentes
```

### Gap Crítico Identificado

**`funcCapture` e `typeCapture` NÃO capturam linha de definição:**

```go
// scanner/grammar.go:256-261 - ATUAL
type funcCapture struct {
    name     string
    params   string
    result   string
    receiver string
    // FALTA: line int
}
```

Isso **bloqueia** a implementação de `get_symbol` com localização.

---

## 3. Proposed Solution

### 3.1. High-Level Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                         CLI / MCP Layer                          │
├─────────────────────────────────────────────────────────────────┤
│  main.go         mcp/main.go                                    │
│     │               │                                            │
│     │               ├── handleGetSymbol (NEW)                   │
│     │               ├── handleGetDependencies (mode param)      │
│     │               └── validatePath (refactored)               │
└─────────────────────────────────────────────────────────────────┘
                              │
┌─────────────────────────────────────────────────────────────────┐
│                        Scanner Layer                             │
├─────────────────────────────────────────────────────────────────┤
│  types.go                                                        │
│     ├── FileInfo { ..., Tokens int }  (modified)                │
│     ├── FuncInfo { ..., Line int }    (modified)                │
│     └── TypeInfo { ..., Line int }    (modified)                │
│                                                                  │
│  walker.go                                                       │
│     └── ScanFiles() → calcula Tokens                            │
│                                                                  │
│  grammar.go                                                      │
│     ├── funcCapture { ..., line int } (modified)                │
│     └── typeCapture { ..., line int } (modified)                │
│                                                                  │
│  symbol.go (NEW)                                                 │
│     └── SearchSymbols(analyses, query) → []SymbolMatch          │
└─────────────────────────────────────────────────────────────────┘
                              │
┌─────────────────────────────────────────────────────────────────┐
│                        Render Layer                              │
├─────────────────────────────────────────────────────────────────┤
│  tree.go                                                         │
│     └── Tree() → exibe tokens + warnings                        │
└─────────────────────────────────────────────────────────────────┘
```

### 3.2. Data Model Changes

#### `scanner/types.go`

```go
// MODIFICAÇÃO: FileInfo
type FileInfo struct {
    Path    string `json:"path"`
    Size    int64  `json:"size"`
    Ext     string `json:"ext"`
    Tokens  int    `json:"tokens,omitempty"`  // NEW: Estimativa de tokens
    IsNew   bool   `json:"is_new,omitempty"`
    Added   int    `json:"added,omitempty"`
    Removed int    `json:"removed,omitempty"`
}

// MODIFICAÇÃO: FuncInfo
type FuncInfo struct {
    Name       string `json:"name"`
    Signature  string `json:"signature,omitempty"`
    Receiver   string `json:"receiver,omitempty"`
    IsExported bool   `json:"exported,omitempty"`
    Line       int    `json:"line,omitempty"`  // NEW: Linha de definição
}

// MODIFICAÇÃO: TypeInfo
type TypeInfo struct {
    Name       string   `json:"name"`
    Kind       TypeKind `json:"kind"`
    Fields     []string `json:"fields,omitempty"`
    Methods    []string `json:"methods,omitempty"`
    IsExported bool     `json:"exported,omitempty"`
    Line       int      `json:"line,omitempty"`  // NEW: Linha de definição
}

// NEW: Constante para estimativa de tokens
const CharsPerToken = 3.5  // Conservador para maioria dos tokenizers BPE

// NEW: Threshold para warning
const LargeFileTokens = 8000

// NEW: Função helper
func EstimateTokens(size int64) int {
    return int(float64(size) / CharsPerToken)
}
```

#### `scanner/grammar.go`

```go
// MODIFICAÇÃO: funcCapture
type funcCapture struct {
    name     string
    params   string
    result   string
    receiver string
    line     int  // NEW
}

// MODIFICAÇÃO: typeCapture
type typeCapture struct {
    name   string
    kind   TypeKind
    fields string
    line   int  // NEW
}
```

#### `scanner/symbol.go` (NEW)

```go
package scanner

// SymbolQuery representa os filtros de busca
type SymbolQuery struct {
    Name string   // Substring match (case-insensitive)
    Kind string   // "function", "type", "all"
    File string   // Filtrar por arquivo específico (opcional)
}

// SymbolMatch representa um símbolo encontrado
type SymbolMatch struct {
    Name      string `json:"name"`
    Kind      string `json:"kind"`      // "function" ou "type"
    Signature string `json:"signature"` // Para funções
    TypeKind  string `json:"type_kind"` // Para tipos (struct, class, etc.)
    File      string `json:"file"`
    Line      int    `json:"line"`
    Exported  bool   `json:"exported"`
}

// SearchSymbols busca símbolos nas análises existentes
func SearchSymbols(analyses []FileAnalysis, query SymbolQuery) []SymbolMatch
```

#### `mcp/main.go`

```go
// MODIFICAÇÃO: DepsInput
type DepsInput struct {
    Path   string `json:"path" jsonschema:"Path to the project directory"`
    Detail int    `json:"detail,omitempty" jsonschema:"0=names, 1=signatures, 2=full"`
    Mode   string `json:"mode,omitempty" jsonschema:"deps (default), api"`  // NEW
}

// NEW: SymbolInput
type SymbolInput struct {
    Path string `json:"path" jsonschema:"Path to the project directory"`
    Name string `json:"name" jsonschema:"Symbol name to search (substring match)"`
    Kind string `json:"kind,omitempty" jsonschema:"function, type, or all (default)"`
    File string `json:"file,omitempty" jsonschema:"Filter to specific file"`
}
```

---

## 4. Detailed Action Plan

### Phase 0: Line Capture Fix (BLOCKER)

**Objetivo:** Corrigir o gap crítico que impede `get_symbol` de reportar localização.

| Task | Arquivo | Mudança | Critério de Conclusão |
|------|---------|---------|----------------------|
| 0.1 | `scanner/grammar.go` | Adicionar campo `line int` em `funcCapture` e `typeCapture` | Structs atualizados |
| 0.2 | `scanner/grammar.go` | Em `AnalyzeFile`, capturar linha via `node.StartPosition().Row + 1` | Linha capturada durante parsing |
| 0.3 | `scanner/types.go` | Adicionar campo `Line int` em `FuncInfo` e `TypeInfo` | Structs atualizados |
| 0.4 | `scanner/grammar.go` | Propagar linha em `Build()` de `funcCapture` e `typeCapture` | `FuncInfo.Line` e `TypeInfo.Line` preenchidos |

**Implementação `0.2`:**
```go
// Em AnalyzeFile, no loop de captures:
for i, capture := range match.Captures {
    captureName := query.CaptureNames()[capture.Index]
    text := capture.Node.Utf8Text(content)
    startLine := int(capture.Node.StartPosition().Row) + 1  // 1-indexed

    if strings.HasPrefix(captureName, "func.") {
        handleFuncCapture(funcBuilders, match.ID(), captureName, text, startLine)
    }
    // similar para types
}
```

---

### Phase 1: Token Estimation (P0)

**Objetivo:** Fornecer visibilidade de context window para LLMs.

| Task | Arquivo | Mudança | Critério de Conclusão |
|------|---------|---------|----------------------|
| 1.1 | `scanner/types.go` | Adicionar `Tokens int` em `FileInfo`, `CharsPerToken`, `LargeFileTokens`, `EstimateTokens()` | Constantes e helper definidos |
| 1.2 | `scanner/walker.go` | Em `ScanFiles`, calcular e preencher `Tokens` para cada arquivo | `FileInfo.Tokens` preenchido |
| 1.3 | `render/tree.go` | Exibir tokens no formato `~Xk` e `⚠` para arquivos > 8k tokens | Output visual atualizado |
| 1.4 | `render/tree.go` | Adicionar total de tokens no header | Header exibe `Tokens: ~Xk` |

**Formato de Saída (Task 1.3):**
```
├── scanner/
│   ├── grammar.go        (~4.2k tokens)
│   ├── walker.go         (~1.8k tokens)
│   └── huge_file.go      (~12k tokens) ⚠
```

**Formato Header (Task 1.4):**
```
╭──────────────────── codemap ────────────────────╮
│ Files: 42 | Size: 156KB | Tokens: ~45k          │
│ Top Extensions: .go (28), .md (8), .yaml (4)    │
╰─────────────────────────────────────────────────╯
```

---

### Phase 2: Symbol Search - `get_symbol` (P0)

**Objetivo:** Permitir busca semântica precisa de símbolos.

| Task | Arquivo | Mudança | Critério de Conclusão |
|------|---------|---------|----------------------|
| 2.1 | `scanner/symbol.go` | Criar arquivo com `SymbolQuery`, `SymbolMatch`, `SearchSymbols()` | Arquivo criado e compila |
| 2.2 | `scanner/symbol.go` | Implementar `SearchSymbols()` que filtra `[]FileAnalysis` | Retorna matches filtrados |
| 2.3 | `mcp/main.go` | Adicionar `SymbolInput` struct | Struct definido |
| 2.4 | `mcp/main.go` | Implementar `handleGetSymbol()` | Handler funcional |
| 2.5 | `mcp/main.go` | Registrar tool `get_symbol` no servidor MCP | Tool aparece em `status` |

**Formato de Saída `get_symbol`:**
```
=== Symbol Search: "Config" ===

Found 3 matches:

  scanner/types.go:18
  ├─ type Config struct
  └─ Fields: Root, Mode, Files

  scanner/grammar.go:45
  ├─ func NewConfig(path string) *Config
  └─ Signature: NewConfig(path string) *Config

  mcp/main.go:120
  ├─ func (c *Config) Validate() error
  └─ Receiver: *Config

───────────────────────────────────
Matches: 3 (1 type, 2 functions)
```

---

### Phase 3: MCP Enhancements (P1)

**Objetivo:** Expor modo API e centralizar validação.

| Task | Arquivo | Mudança | Critério de Conclusão |
|------|---------|---------|----------------------|
| 3.1 | `mcp/main.go` | Adicionar `Mode string` em `DepsInput` | Struct atualizado |
| 3.2 | `mcp/main.go` | Em `handleGetDependencies`, usar `render.APIView` quando `mode="api"` | Modo API funcional |
| 3.3 | `mcp/main.go` | Criar `validatePath(path string) (string, error)` | Helper definido |
| 3.4 | `mcp/main.go` | Refatorar handlers para usar `validatePath` | Zero duplicação de código |

**Implementação `validatePath`:**
```go
func validatePath(path string) (string, error) {
    if path == "" {
        return "", fmt.Errorf("path is required")
    }
    absPath, err := filepath.Abs(path)
    if err != nil {
        return "", fmt.Errorf("invalid path: %w", err)
    }
    if _, err := os.Stat(absPath); os.IsNotExist(err) {
        return "", fmt.Errorf("path does not exist: %s", absPath)
    }
    return absPath, nil
}
```

---

## 5. Dependency Graph

```
Phase 0 (Line Capture)
    │
    ├──────────────────────┐
    ▼                      ▼
Phase 1 (Tokens)     Phase 2 (get_symbol)
    │                      │
    └──────────┬───────────┘
               ▼
         Phase 3 (MCP)
```

**Nota:** Phases 1 e 2 podem ser desenvolvidas em paralelo após Phase 0.

---

## 6. Risk Mitigation

| Risco | Probabilidade | Impacto | Mitigação |
|-------|---------------|---------|-----------|
| R1: Captura de linha quebra parsing existente | Baixa | Alto | Testes manuais em múltiplas linguagens antes de merge |
| R2: Token ratio impreciso | Alta | Baixo | Usar `~` (aproximação) e focar no warning > 8k |
| R3: `SearchSymbols` lento em projetos grandes | Média | Médio | Filtrar por arquivo primeiro se `query.File` especificado |
| R4: Breaking change em JSON output | Média | Alto | Campos novos são `omitempty`, não quebra consumers existentes |

---

## 7. Validation Criteria

### Phase 0
- [ ] `./codemap --deps .` exibe funções/tipos com linha no JSON (`--json`)
- [ ] Linha é 1-indexed e corresponde à definição real

### Phase 1
- [ ] `./codemap .` exibe `(~Xk tokens)` após cada arquivo
- [ ] Arquivos > 8k tokens mostram `⚠`
- [ ] Header mostra total de tokens do projeto

### Phase 2
- [ ] `codemap-mcp` lista `get_symbol` em `status`
- [ ] `get_symbol(path=".", name="Scan")` retorna todas funções/tipos com "Scan"
- [ ] Output inclui `file:line` para cada match

### Phase 3
- [ ] `get_dependencies(path=".", mode="api")` retorna output idêntico a `render.APIView`
- [ ] Nenhum handler em `mcp/main.go` tem `filepath.Abs` duplicado

---

## 8. Questões Resolvidas

| Questão Original | Resolução |
|------------------|-----------|
| `get_symbol` como função separada ou modo em `ScanForDeps`? | **Função separada** em `scanner/symbol.go` que consome output de `ScanForDeps` |
| Onde extrair linha de definição? | Em `handleFuncCapture`/`handleTypeCapture` via `node.StartPosition().Row + 1` |
| Test Mapping (P2): nome de arquivo ou conteúdo? | **Apenas nome de arquivo** (removido do escopo atual - P2 postergado) |
| Token ratio por linguagem ou fixo? | **Fixo (3.5)** - Simplificação justificada por baixa variação em tokenizers BPE |

---

## 9. Out of Scope (P2+)

Removido do escopo atual para manter foco:

- Test Mapping (`TestPattern`, `TestMapping`)
- JSON output format para MCP (`--format=json`)
- CLI `--format=json` para tree mode
- Cross-file reference tracking (`ImportedBy`)

Estes itens podem ser implementados em iteração futura.
</file>
<file path="development-docs/0002-token-heuristics-symbol-search.md">
# TASKS.md - Token Heuristics, `get_symbol` Tool & MCP Improvements

> **Feature:** Token estimation, semantic symbol search, and MCP enhancements
> **Created:** 2025-12-03
> **Status:** Completed

---

## Project Briefing

This implementation adds three key capabilities to codemap for better LLM support:

1. **Token Estimation**: Display estimated token counts per file with warnings for large files (>8k tokens)
2. **Symbol Search (`get_symbol`)**: New MCP tool for precise function/type lookup by name
3. **MCP Enhancements**: Add API mode to `get_dependencies` and refactor path validation

### Key Technical Changes

| Component | Current | After |
|-----------|---------|-------|
| `FileInfo` | No token count | `Tokens int` field |
| `FuncInfo`/`TypeInfo` | No line numbers | `Line int` field |
| `funcCapture`/`typeCapture` | No line tracking | `line int` field |
| MCP handlers | 7 tools | 8 tools (`get_symbol`) |

---

## Phase 0: Line Capture Fix (BLOCKER)

> **Objective:** Enable line number capture for symbols - prerequisite for `get_symbol`

### Task 0.1: Add `line` field to `funcCapture` struct
- [x] **File:** `scanner/grammar.go:256-261`
- [x] Add `line int` field to `funcCapture` struct
- [x] Verify struct compiles

### Task 0.2: Add `line` field to `typeCapture` struct
- [x] **File:** `scanner/grammar.go:301-305`
- [x] Add `line int` field to `typeCapture` struct
- [x] Verify struct compiles

### Task 0.3: Add `Line` field to `FuncInfo` struct
- [x] **File:** `scanner/types.go:38-43`
- [x] Add `Line int json:"line,omitempty"` field
- [x] Verify struct compiles

### Task 0.4: Add `Line` field to `TypeInfo` struct
- [x] **File:** `scanner/types.go:87-93`
- [x] Add `Line int json:"line,omitempty"` field
- [x] Verify struct compiles

### Task 0.5: Modify `handleFuncCapture` to capture line numbers
- [x] **File:** `scanner/grammar.go:282-298`
- [x] Add `line int` parameter to `handleFuncCapture`
- [x] Set `fc.line` when capturing `func.name` (first significant capture)
- [x] Handle line in the switch statement

### Task 0.6: Modify `handleTypeCapture` to capture line numbers
- [x] **File:** `scanner/grammar.go:323-351`
- [x] Add `line int` parameter to `handleTypeCapture`
- [x] Set `tc.line` when capturing `type.name` (first significant capture)
- [x] Handle line in the switch statement

### Task 0.7: Update `AnalyzeFile` to pass line numbers
- [x] **File:** `scanner/grammar.go:183-253`
- [x] Extract line from `capture.Node.StartPosition().Row + 1` (1-indexed)
- [x] Pass line to `handleFuncCapture` calls
- [x] Pass line to `handleTypeCapture` calls

### Task 0.8: Update `funcCapture.Build` to include Line
- [x] **File:** `scanner/grammar.go:264-279`
- [x] Set `info.Line = fc.line` in Build method
- [x] Verify FuncInfo includes line in output

### Task 0.9: Update `typeCapture.Build` to include Line
- [x] **File:** `scanner/grammar.go:308-320`
- [x] Set `info.Line = tc.line` in Build method
- [x] Verify TypeInfo includes line in output

### Task 0.10: Validate Phase 0
- [x] Run `go build -o codemap .`
- [x] Run `./codemap --deps --json . | head -50` and verify lines appear
- [x] Verify line numbers are 1-indexed and correct

---

## Phase 1: Token Estimation (P0)

> **Objective:** Add token count visibility to help LLMs manage context windows

### Task 1.1: Add token estimation constants and helper
- [x] **File:** `scanner/types.go`
- [x] Add `const CharsPerToken = 3.5`
- [x] Add `const LargeFileTokens = 8000`
- [x] Add `func EstimateTokens(size int64) int`

### Task 1.2: Add `Tokens` field to `FileInfo`
- [x] **File:** `scanner/types.go:18-25`
- [x] Add `Tokens int json:"tokens,omitempty"` field

### Task 1.3: Calculate tokens in `ScanFiles`
- [x] **File:** `scanner/walker.go`
- [x] In `ScanFiles`, set `file.Tokens = EstimateTokens(file.Size)` for each file

### Task 1.4: Update `Tree` to display tokens per file
- [x] **File:** `render/tree.go`
- [x] Modify `printTreeNode` to show `[!]` warning for files > 8k tokens
- [x] Add warning indicator `[!]` for files > 8k tokens

### Task 1.5: Update header to show total tokens
- [x] **File:** `render/tree.go:106-195`
- [x] Calculate total tokens across all files
- [x] Add `Tokens: ~Xk` to stats line in header

### Task 1.6: Validate Phase 1
- [x] Run `go build -o codemap .`
- [x] Run `./codemap .` and verify tokens appear in output
- [x] Verify warning indicator appears for large files
- [x] Verify header shows total tokens

---

## Phase 2: Symbol Search - `get_symbol` (P0)

> **Objective:** Enable precise semantic search for functions and types

### Task 2.1: Create `scanner/symbol.go` with types
- [x] Create new file `scanner/symbol.go`
- [x] Define `SymbolQuery` struct with `Name`, `Kind`, `File` fields
- [x] Define `SymbolMatch` struct with `Name`, `Kind`, `Signature`, `TypeKind`, `File`, `Line`, `Exported` fields

### Task 2.2: Implement `SearchSymbols` function
- [x] **File:** `scanner/symbol.go`
- [x] Implement search logic that filters `[]FileAnalysis`
- [x] Support case-insensitive substring matching on Name
- [x] Support filtering by Kind ("function", "type", "all")
- [x] Support filtering by File (optional)

### Task 2.3: Add `SymbolInput` struct to MCP
- [x] **File:** `mcp/main.go`
- [x] Add `SymbolInput` struct with `Path`, `Name`, `Kind`, `File` fields
- [x] Add JSON schema annotations

### Task 2.4: Implement `handleGetSymbol` handler
- [x] **File:** `mcp/main.go`
- [x] Validate path using existing pattern
- [x] Call `ScanForDeps` with appropriate detail level
- [x] Call `SearchSymbols` with query parameters
- [x] Format output as ASCII table with file:line references

### Task 2.5: Register `get_symbol` tool in MCP server
- [x] **File:** `mcp/main.go`
- [x] Add tool registration with description and schema
- [x] Update tool count comment

### Task 2.6: Validate Phase 2
- [x] Run `go build -o codemap-mcp ./mcp/`
- [x] Test `get_symbol` tool with various queries
- [x] Verify file:line format in output

---

## Phase 3: MCP Enhancements (P1)

> **Objective:** Add API mode and centralize path validation

### Task 3.1: Add `Mode` field to `DepsInput`
- [x] **File:** `mcp/main.go`
- [x] Add `Mode string json:"mode,omitempty"` to `DepsInput`
- [x] Document valid values: "deps" (default), "api"

### Task 3.2: Implement API mode in `handleGetDependencies`
- [x] **File:** `mcp/main.go`
- [x] Check if `input.Mode == "api"`
- [x] Call `render.APIView` instead of `render.Depgraph` when API mode
- [x] Ensure proper output capture

### Task 3.3: Create `validatePath` helper function
- [x] **File:** `mcp/main.go`
- [x] Create function: `func validatePath(path string) (string, error)`
- [x] Handle empty path, filepath.Abs, and os.Stat checks
- [x] Return absolute path or error

### Task 3.4: Refactor handlers to use `validatePath`
- [x] **File:** `mcp/main.go`
- [x] Update `handleGetStructure` to use `validatePath`
- [x] Update `handleGetDependencies` to use `validatePath`
- [x] Update `handleGetDiff` to use `validatePath`
- [x] Update `handleFindFile` to use `validatePath`
- [x] Update `handleGetImporters` to use `validatePath`
- [x] Update `handleListProjects` to use `validatePath`
- [x] Verify no duplicate `filepath.Abs` calls remain

### Task 3.5: Validate Phase 3
- [x] Run `go build -o codemap-mcp ./mcp/`
- [x] Test `get_dependencies` with `mode=api`
- [x] Verify path validation works correctly

---

## Final Validation & Testing

### Integration Tests
- [x] `./codemap .` - Verify tree with tokens
- [x] `./codemap --deps .` - Verify functions/types have line numbers
- [x] `./codemap --deps --json .` - Verify JSON includes `line` and `tokens` fields
- [x] MCP: `get_symbol(path=".", name="Scan")` - Verify results
- [x] MCP: `get_dependencies(path=".", mode="api")` - Verify API output

### Manual Verification Checklist
- [x] Line numbers are 1-indexed and match actual file lines
- [x] Token estimates use `~` prefix for approximation
- [x] Warning indicator `[!]` appears for files > 8k tokens
- [x] Header shows aggregated token count
- [x] `get_symbol` returns results with `file:line` format
- [x] API mode produces same output as `render.APIView`
- [x] No duplicate path validation code in MCP handlers

---

## Completion Notes

### Summary
- [x] All phases completed
- [x] All validation criteria passed

**Implementation completed on 2025-12-03**

### Files Modified
- `scanner/types.go` - Added `Tokens`, `Line` fields, token estimation constants and helper
- `scanner/grammar.go` - Added line capture in `funcCapture`, `typeCapture`, handlers, and Build methods
- `scanner/walker.go` - Added token calculation in `ScanFiles`
- `scanner/symbol.go` - New file with `SearchSymbols` function
- `render/tree.go` - Added token display in header and `[!]` warning for large files
- `mcp/main.go` - Added `get_symbol` tool, `validatePath` helper, API mode support

### Deviations from Plan
- Token display format: Changed from `(~Xk tokens)` after each file to `[!]` warning indicator only for files > 8k tokens (simpler, less cluttered output)
- The `validatePath` helper also handles `~/` expansion (bonus feature)

### Follow-up Items
- Consider adding token display per-file in verbose mode
- Test `get_symbol` with various languages to ensure line numbers are accurate
- Consider adding sorting options to `get_symbol` results
</file>
<file path="development-docs/0003-graphrag-implementation-plan.md">
# Master Plan: Codemap GraphRAG

**Status**: ✅ COMPLETE
**Architecture**: Monolithic CLI + MCP Server
**Completed**: 2024-12-04

## Vision
Transform `codemap` from a structural mapper into a **GraphRAG (Graph-based Retrieval Augmented Generation)** system. This will allow LLM agents to "understand" the codebase through pre-computed summaries, relationship mapping (Call Graph), and semantic search, rather than just reading raw file trees.

## Implementation Roadmap

The implementation is broken down into three sequential phases to ensure stability and testability.

### ⚠️ Critical Constraints
1.  **Portability**: Must remain easily cross-compilable (Linux/Mac/Windows). Avoid CGO where possible (e.g., use `modernc.org/sqlite` or pure Go equivalents).
2.  **Performance**: `codemap` is a CLI tool. Startup time and index speed are paramount.
3.  **Token Efficiency**: All outputs must be optimized to save context window space for Agents.

### [Phase 1: Knowledge Graph Foundation](./development-docs/plans/01_knowledge_graph.md)
*   **Goal**: Build the "Skeleton". Syntactic Call Graph, Persistence, and Graph Querying.
*   **Key Tech**: `encoding/gob` (Storage), Tree-sitter (Parsing).
*   **New Commands**: `index`, `query`, `trace`.

### [Phase 2: Semantic Intelligence](./development-docs/plans/02_llm_integration.md)
*   **Goal**: Add the "Brain". Connect to LLMs (Ollama/OpenAI) to explain and summarize.
*   **Key Tech**: LLM Clients, Caching (ContentHash), Configuration.
*   **New Commands**: `explain`, `summarize`.

### [Phase 3: Hybrid Retrieval](./development-docs/plans/03_hybrid_retrieval.md)
*   **Goal**: Add the "Eyes". Natural language search merging Graph + Embeddings.
*   **Key Tech**: Vector Search (In-Memory/Pure Go), Rank Fusion.
*   **New Commands**: `search`.

## Architecture Overview

```mermaid
graph TD
    CLI[CLI / MCP] --> Orchestrator
    Orchestrator --> Graph[Knowledge Graph]
    Orchestrator --> Analyzer[LLM Analyzer]

    subgraph "Phase 1: Structure"
    Graph --> Store[(Graph.gob)]
    Graph --> Scanner[Scanner/Tree-sitter]
    Scanner --> FS[File System]
    end

    subgraph "Phase 2: Semantics"
    Analyzer --> Cache[(JSON Cache)]
    Analyzer --> LLM[Ollama / OpenAI]
    end

    subgraph "Phase 3: Search"
    Analyzer --> Vector[(Vector Index)]
    end
```

## Implementation Complete

All three phases have been successfully implemented:

1. **Phase 1**: Knowledge Graph with gob persistence, call graph extraction for 6 languages
2. **Phase 2**: LLM integration with Ollama/OpenAI/Anthropic, caching, explain/summarize commands
3. **Phase 3**: Hybrid search with vector index and Reciprocal Rank Fusion

See `TASKS.md` for detailed implementation notes and validation results.
</file>
<file path="development-docs/0003-graphrag-implementation.md">
# TASKS: Codemap GraphRAG Implementation

## Project Briefing

**Objective**: Transform `codemap` from a structural file-tree mapper into a **GraphRAG (Graph-based Retrieval Augmented Generation)** system enabling LLM agents to "understand" codebases through pre-computed summaries, call graphs, and semantic search.

**Current State**: CLI tool with tree view, dependency flow, diff mode, skyline visualization, and MCP server with 8 tools.

**Target State**: Full GraphRAG system with knowledge graph persistence, LLM-powered explanations, and hybrid semantic/structural search.

**Critical Constraints**:
1. **Portability**: Cross-compile support (Linux/Mac/Windows). Avoid CGO.
2. **Performance**: Fast CLI startup (<100ms) and indexing.
3. **Token Efficiency**: Optimize all outputs for LLM context windows.

---

## Phase 1: Knowledge Graph Foundation

### 1.1 Core Data Structures (`graph/` package)

- [x] **1.1.1** Create `graph/` package directory structure
- [x] **1.1.2** Define `NodeKind` enum (File, Package, Function, Type, Method, Variable)
- [x] **1.1.3** Define `EdgeKind` enum (Imports, Calls, Defines, Contains, References)
- [x] **1.1.4** Implement `NodeID` generation using deterministic `sha256(path + symbol)`
- [x] **1.1.5** Define `Node` struct with fields: ID, Kind, Name, Path, Line, Signature, DocString
- [x] **1.1.6** Define `Edge` struct with fields: From, To, Kind, Weight, Location
- [x] **1.1.7** Implement `CodeGraph` struct with map-based indexes (`nodesByID`, `edgesByFrom`, `edgesByTo`)
- [x] **1.1.8** Implement `AddNode()`, `AddEdge()`, `GetNode()`, `GetEdges()` methods
- [x] **1.1.9** Implement `SaveBinary(path)` using `encoding/gob` (with gzip compression)
- [x] **1.1.10** Implement `LoadBinary(path)` using `encoding/gob`
- [x] **1.1.11** Benchmark persistence for target <100ms with 100k nodes (277ms for 237 nodes)

### 1.2 Scanner Enhancements (`scanner/`)

- [x] **1.2.1** Refactor `walker.go` to decouple file scanning from data generation
- [x] **1.2.2** Create `scanner/calls.go` for call expression extraction
- [x] **1.2.3** Update `queries/go.scm` to capture `call_expression` nodes (inline patterns)
- [x] **1.2.4** Update `queries/python.scm` to capture `call` nodes (inline patterns)
- [x] **1.2.5** Update `queries/javascript.scm` to capture `call_expression` nodes (inline patterns)
- [x] **1.2.6** Update `queries/typescript.scm` to capture `call_expression` nodes (inline patterns)
- [x] **1.2.7** Update remaining language queries (rust, java) (inline patterns)
- [x] **1.2.8** Implement `ExtractCalls(file, source)` function returning call sites
- [x] **1.2.9** Implement `SyntacticCallGraph` extractor aggregating all files
- [x] **1.2.10** Implement `ImportGraphFilter` to eliminate calls to non-imported packages
- [x] **1.2.11** Implement `ArityFilter` to eliminate calls where arg count mismatches

### 1.3 Graph Builder (`graph/`)

- [x] **1.3.1** Create `graph/builder.go` for graph construction logic
- [x] **1.3.2** Implement `BuildFromScanner(root string)` orchestrating full graph build
- [x] **1.3.3** Convert `FileAnalysis` results into `Node` objects
- [x] **1.3.4** Convert import relationships into `Edge` objects (EdgeKind: Imports)
- [x] **1.3.5** Convert call relationships into `Edge` objects (EdgeKind: Calls)
- [x] **1.3.6** Implement `IsGraphStale(root)` using file modification times
- [x] **1.3.7** Implement incremental update logic for changed files only
- [x] **1.3.8** Add progress reporting callback for large codebases

### 1.4 CLI Commands

- [x] **1.4.1** Implement `codemap index` command in `main.go`
- [x] **1.4.2** Add `--force` flag to rebuild graph even if fresh
- [x] **1.4.3** Add `--output` flag to specify graph file location (default: `.codemap/graph.gob`)
- [x] **1.4.4** Implement `codemap query --from <symbol>` showing outgoing edges
- [x] **1.4.5** Implement `codemap query --to <symbol>` showing incoming edges
- [x] **1.4.6** Implement `codemap query --from <A> --to <B>` for path tracing
- [x] **1.4.7** Add `--json` output format for query results
- [x] **1.4.8** Add `--depth` flag to limit traversal depth

### 1.5 MCP Tools

- [x] **1.5.1** Update `get_structure` to use graph if `.codemap/graph.gob` exists
- [x] **1.5.2** Implement `trace_path` MCP tool (find connections between symbols)
- [x] **1.5.3** Implement `get_callers` MCP tool (who calls this symbol)
- [x] **1.5.4** Implement `get_callees` MCP tool (what does this symbol call)

### 1.6 Testing

- [x] **1.6.1** Create `scanner/testdata/corpus/` directory
- [x] **1.6.2** Add Go test files with known call patterns
- [x] **1.6.3** Add Python test files with known call patterns
- [x] **1.6.4** Add TypeScript test files with known call patterns
- [x] **1.6.5** Implement snapshot tests comparing generated graph to JSON baseline
- [x] **1.6.6** Manual verification: run against codemap itself and 2-3 real projects

---

## Phase 2: Semantic Intelligence (LLM Integration)

### 2.1 Configuration System (`config/`)

- [x] **2.1.1** Create `config/` package directory
- [x] **2.1.2** Define `Config` struct with LLM settings
- [x] **2.1.3** Implement loading from `~/.config/codemap/config.yaml`
- [x] **2.1.4** Implement loading from `.codemap/config.yaml` (project-level override)
- [x] **2.1.5** Handle API keys safely (env vars, file permissions)
- [x] **2.1.6** Add validation for required config fields

### 2.2 LLM Client Layer (`analyze/`)

- [x] **2.2.1** Create `analyze/` package directory
- [x] **2.2.2** Define `LLMClient` interface with `Complete(prompt)` and `Embed(text)` methods
- [x] **2.2.3** Implement `OllamaClient` for local inference (default)
- [x] **2.2.4** Implement `OpenAIClient` for cloud inference
- [x] **2.2.5** Implement `AnthropicClient` for Claude API
- [x] **2.2.6** Add retry logic and timeout handling
- [x] **2.2.7** Add token counting utilities

### 2.3 Summarization Engine

- [x] **2.3.1** Implement `ReadSymbolSource(node)` to extract raw code from file
- [x] **2.3.2** Design prompt template for "Explain this function"
- [x] **2.3.3** Design prompt template for "Summarize this module"
- [x] **2.3.4** Create `cache/` package for response caching
- [x] **2.3.5** Implement `ContentHash` calculation for cache invalidation
- [x] **2.3.6** Implement JSON-based cache storage in `.codemap/cache/`
- [x] **2.3.7** Add cache hit/miss statistics

### 2.4 CLI Commands

- [x] **2.4.1** Implement `codemap explain <symbol>` command
- [x] **2.4.2** Add `--model` flag to select LLM model
- [x] **2.4.3** Add `--no-cache` flag to bypass cache
- [x] **2.4.4** Implement `codemap summarize <path>` for module summaries

### 2.5 MCP Tools

- [x] **2.5.1** Implement `explain_symbol` MCP tool
- [x] **2.5.2** Implement `summarize_module` MCP tool
- [x] **2.5.3** Add structured output with code context

### 2.6 Testing

- [x] **2.6.1** Create mock LLM client returning fixed strings
- [x] **2.6.2** Test caching with content hash validation
- [x] **2.6.3** Manual verification with Ollama and OpenAI

---

## Phase 3: Hybrid Retrieval & Search

### 3.1 Vector Storage (`graph/vectors.go`)

- [x] **3.1.1** Define `VectorIndex` interface
- [x] **3.1.2** Implement in-memory vector storage using `gob` serialization
- [x] **3.1.3** Implement Cosine Similarity search
- [x] **3.1.4** Optimize for <50k vectors target
- [x] **3.1.5** Benchmark search performance

### 3.2 Embedding Pipeline

- [x] **3.2.1** Implement `NodeToText(node)` strategy (Signature + DocString + Summary)
- [x] **3.2.2** Implement `EmbedNodes(nodes)` with batching
- [x] **3.2.3** Add progress bar for embedding generation
- [x] **3.2.4** Store embeddings alongside graph in `.codemap/vectors.gob`

### 3.3 Hybrid Search Engine (`analyze/retriever.go`)

- [x] **3.3.1** Implement vector search (top-k semantic matches)
- [x] **3.3.2** Implement graph search (name matching/fuzzy)
- [x] **3.3.3** Implement Reciprocal Rank Fusion for result merging
- [x] **3.3.4** Add context expansion (retrieve callers/callees of top results)
- [x] **3.3.5** Tune weights for optimal relevance

### 3.4 CLI Commands

- [x] **3.4.1** Implement `codemap search "natural language query"`
- [x] **3.4.2** Add `--limit` flag for result count
- [x] **3.4.3** Add `--expand` flag for context expansion
- [x] **3.4.4** Add `--json` output format

### 3.5 MCP Tools

- [x] **3.5.1** Implement `semantic_search` MCP tool
- [x] **3.5.2** Add structured response with relevance scores
- [x] **3.5.3** Include code snippets in results

### 3.6 Performance & Polish

- [x] **3.6.1** Profile embedding generation performance
- [x] **3.6.2** Add TUI progress bars for long operations
- [x] **3.6.3** Optimize memory usage for large codebases
- [x] **3.6.4** Documentation and README updates

---

## Validation & Testing

- [x] **V.1** Run `go fmt ./...` and `go vet ./...`
- [x] **V.2** Test all new commands manually
- [x] **V.3** Test MCP tools via Claude Desktop
- [x] **V.4** Verify cross-platform build (`GOOS=darwin`, `GOOS=windows`)
- [x] **V.5** Performance benchmarks meet targets (<100ms startup)

---

## Progress Tracking

| Phase | Total Tasks | Completed | Progress |
|-------|-------------|-----------|----------|
| Phase 1 | 48 | 48 | 100% |
| Phase 2 | 27 | 27 | 100% |
| Phase 3 | 22 | 22 | 100% |
| Validation | 5 | 5 | 100% |
| **Total** | **102** | **102** | **100%** |

---

## Notes & Deviations

### 2024-12-03: Phase 1 Core Implementation

**Completed:**
- Created `graph/` package with types.go, store.go, query.go, builder.go
- Implemented knowledge graph with NodeKind/EdgeKind enums, Node/Edge structs
- Added gzip-compressed gob persistence (graph.gob)
- Created `scanner/calls.go` with inline tree-sitter query patterns for call extraction
- Supported languages for call extraction: Go, Python, JavaScript, TypeScript, Rust, Java
- Implemented `codemap --index` command to build graph
- Implemented `codemap --query` command with --from, --to, --depth flags
- JSON output support for all query modes

**Test Results:**
- Index built in 277ms for codemap codebase (237 nodes, 457 edges)
- Query performance excellent
- Call graph extraction working for Go code

**Deviations:**
- Used inline query patterns in calls.go instead of separate .scm files for simplicity
- Added gzip compression to gob storage for smaller files
- Skipped ArityFilter (1.2.11) - can be added later if needed
- MCP tool updates deferred to later session

### 2024-12-03: MCP Tools Implementation

**Completed:**
- Added `trace_path` MCP tool for finding call paths between symbols
- Added `get_callers` MCP tool for finding who calls a symbol
- Added `get_callees` MCP tool for finding what a symbol calls
- Added `--output` flag to `codemap --index` for custom graph output path
- Updated MCP server to v2.1.0 with 11 total tools

**Implementation Notes:**
- MCP tools use the same graph package as CLI (LoadBinary, FindPath, GetReverseTree, GetDependencyTree)
- Tools require a pre-built index (run `codemap --index` first)
- Added helpful error messages when no index is found
- Status tool updated to list all 11 available tools

### 2024-12-03: Incremental Updates & ArityFilter

**Completed:**
- Task 1.5.1: Updated `get_structure` MCP tool to show graph stats when available
- Task 1.3.7: Implemented incremental update logic for changed files
  - Added `RemoveNodesForPath()` to graph/types.go
  - Added `GetDeletedFiles()` and `IsFileInGraph()` to graph/store.go
  - Added `WithExistingGraph()` builder option
  - Fixed bug: package paths (no extension) were incorrectly treated as deleted files
  - Result: Only modified/new files are re-indexed, shows "✓ Index is up-to-date" when nothing changed
- Task 1.2.11: Implemented ArityFilter to eliminate arity-mismatched calls
  - Added `ParamCount` field to FuncInfo (scanner/types.go) and Node (graph/types.go)
  - Added `ArgCount` field to Edge (graph/types.go)
  - Implemented `countParams()` in scanner/grammar.go (handles variadic functions, nested types)
  - Added arity check in `FilterCallEdges()`
  - Result: Reduced edges from 519 to 498 (21 false positives removed)

**Test Results:**
- Index: 249 nodes, 498 edges (after arity filter)
- Incremental update correctly detects modified files only
- All builds passing, go fmt/vet clean

### 2024-12-03: Test Corpus Implementation

**Completed:**
- Task 1.6.1: Created `scanner/testdata/corpus/` directory structure
- Task 1.6.2: Added Go test files (main.go, types.go) with call patterns
- Task 1.6.3: Added Python test files (main.py, classes.py) with call patterns
- Task 1.6.4: Added TypeScript test files (main.ts, types.ts) with call patterns
- Task 1.6.5: Implemented snapshot tests with JSON baselines

**Test Corpus Structure:**
```
scanner/testdata/corpus/
├── go/
│   ├── main.go           # Entry point with hello/add/process calls
│   ├── types.go          # User/Service types with methods
│   └── expected.json     # Baseline: 15 nodes, 19 edges
├── python/
│   ├── main.py           # Entry point with similar patterns
│   ├── classes.py        # Classes with methods
│   └── expected.json     # Baseline: 19 nodes, 23 edges
├── typescript/
│   ├── main.ts           # Entry point with TS patterns
│   ├── types.ts          # Interfaces and classes
│   └── expected.json     # Baseline: 18 nodes, 22 edges
└── corpus_test.go        # Snapshot test harness
```

**Test Coverage:**
- Node counts (files, functions, types)
- Edge counts
- Expected call edges (from -> to relationships)
- Run with: `cd scanner/testdata/corpus && go test -v ./...`

**Notes:**
- Cross-file method calls not yet captured (documented in expected.json)
- Only task 1.2.1 (walker refactoring) remains incomplete in Phase 1

### 2024-12-03: Phase 2 LLM Integration Started

**Completed:**
- Created `config/` package with YAML-based configuration system
  - `config/config.go`: Config struct with LLM settings (provider, model, API keys, timeouts)
  - Supports loading from `~/.config/codemap/config.yaml` (user-level)
  - Supports loading from `.codemap/config.yaml` (project-level override)
  - Environment variable overrides: CODEMAP_LLM_PROVIDER, OLLAMA_HOST, OPENAI_API_KEY, ANTHROPIC_API_KEY
  - Config validation with helpful error messages
- Created `analyze/` package with LLM client layer
  - `analyze/client.go`: LLMClient interface with Complete() and Embed() methods
  - `analyze/ollama.go`: OllamaClient for local inference (default provider)
  - `analyze/openai.go`: OpenAIClient for OpenAI API
  - `analyze/anthropic.go`: AnthropicClient for Claude API
  - `analyze/factory.go`: NewClient() factory function to create clients from config
  - `analyze/tokens.go`: Token estimation and budget management utilities
  - Retry logic with exponential backoff
  - Timeout handling
  - Rate limit handling (429 responses)

**New Dependencies:**
- `gopkg.in/yaml.v3` for YAML config parsing

**Files Created:**
- `config/config.go`
- `analyze/client.go`
- `analyze/ollama.go`
- `analyze/openai.go`
- `analyze/anthropic.go`
- `analyze/factory.go`
- `analyze/tokens.go`

**Next Steps:**
- 2.4.4: Implement `codemap summarize <path>` command
- 2.5: MCP Tools (explain_symbol, summarize_module)
- 2.6.2-2.6.3: Testing and verification

### 2024-12-03: Phase 2 Continued (Session 2)

**Completed:**
- Task 2.3.1: Implemented `ReadSymbolSource(node)` in `analyze/source.go`
  - Extracts source code from files using Node metadata (Path, Line, EndLine)
  - Computes ContentHash for cache invalidation
  - Language detection from file extensions
  - Context extraction (before/after lines, file header)
- Task 2.3.2-2.3.3: Created prompt templates in `analyze/prompts.go`
  - `ExplainSymbolPrompt`: Generates prompts for explaining code symbols
  - `SummarizeModulePrompt`: Generates prompts for module summaries
  - `QuickExplainPrompt`: Minimal prompt for quick explanations
  - `CallGraphExplainPrompt`: Explains with caller/callee context
- Task 2.3.4-2.3.7: Created `cache/` package in `cache/cache.go`
  - File-based JSON cache with TTL support
  - ContentHash-based cache invalidation
  - Hit/miss statistics tracking
  - Cleanup for expired entries
- Task 2.4.1-2.4.3: Implemented `codemap --explain` CLI command
  - `--symbol <name>` to specify symbol to explain
  - `--model <name>` to override LLM model
  - `--no-cache` to bypass response cache
  - `--json` for structured output
  - Integration with knowledge graph for symbol lookup
- Task 2.6.1: Created mock LLM client in `analyze/mock.go`
  - Configurable responses for testing
  - Request recording for verification
  - Simulated latency support

**Files Created:**
- `analyze/source.go` - Source code extraction
- `analyze/prompts.go` - LLM prompt templates
- `cache/cache.go` - Response caching
- `analyze/mock.go` - Mock LLM client for testing

**Usage:**
```bash
# Build the graph index first
codemap --index .

# Explain a symbol
codemap --explain --symbol main .

# Use a specific model
codemap --explain --symbol main --model llama3 .

# Bypass cache
codemap --explain --symbol main --no-cache .

# JSON output
codemap --explain --symbol main --json .
```

**Current Status:**
- Phase 1: 100% complete (48/48 tasks)
- Phase 2: 100% complete (27/27 tasks)
- Ready for Phase 3: Hybrid Retrieval & Search

### 2024-12-03: Phase 2 Continued (Session 3)

**Completed:**
- Task 2.4.4: Implemented `codemap --summarize` CLI command
  - Takes a path (file or directory) to summarize
  - Uses `ReadModuleSource()` to read source files
  - Uses `SummarizeModulePrompt()` for LLM prompt
  - Caches using combined content hash of all files
  - Supports `--model`, `--no-cache`, `--json` flags
- Task 2.5.1-2.5.3: Implemented MCP tools for LLM-powered analysis
  - `explain_symbol` - Explain code symbols using LLM
  - `summarize_module` - Summarize modules/directories using LLM
  - Both tools support model override and cache bypass
  - Updated MCP server version to v2.2.0 (13 tools total)
  - Updated status handler to list all available tools

**Files Modified:**
- `main.go` - Added `--summarize` flag and `runSummarizeMode()` function
- `mcp/main.go` - Added ExplainSymbolInput, SummarizeModuleInput structs
  - Added `handleExplainSymbol()` and `handleSummarizeModule()` handlers
  - Updated tool count from 11 to 13

**Usage:**
```bash
# CLI summarize command
codemap --summarize graph/ .           # Summarize graph package
codemap --summarize . --model llama3   # Summarize with specific model
codemap --summarize --json .           # JSON output

# MCP tools (via Claude Desktop or other MCP clients)
# explain_symbol: Explain a symbol with LLM
# summarize_module: Summarize a module with LLM
```

**Testing:**
- Task 2.6.2-2.6.3: Verified code paths work correctly
  - Summarize command attempts LLM connection with proper error handling
  - Explain command finds symbols and attempts LLM connection
  - Error messages are clear and actionable
  - Cache initialization and content hash calculation working
  - All builds pass `go fmt` and `go vet`

**Phase 2 Complete!**
- All 27 tasks completed
- LLM client layer with 3 providers (Ollama, OpenAI, Anthropic)
- Configuration system with user/project config and env vars
- Caching with content hash invalidation
- CLI commands: `--explain` and `--summarize`
- MCP tools: `explain_symbol` and `summarize_module`

### 2024-12-03: Phase 1 Completion (Walker Refactoring)

**Completed:**
- Task 1.2.1: Refactored `walker.go` to decouple file scanning from data generation
  - Created `WalkOptions` struct for configuring walk behavior (gitignore, language filter)
  - Created `WalkFunc` callback type `func(absPath, relPath string, info os.FileInfo) error`
  - Created `WalkFiles(root, opts, fn)` as the core walk function with callback pattern
  - Refactored `ScanFiles` to use `WalkFiles` (thin wrapper for collecting FileInfo)
  - Refactored `ScanForDeps` to use `WalkFiles` (thin wrapper for collecting FileAnalysis)

**Benefits of Decoupling:**
- Single point of walking logic (ignores, gitignore handling)
- Callers can process files as they're found (streaming)
- Allows custom filtering and processing
- Maintains full backwards compatibility

**Testing:**
- All builds pass `go fmt` and `go vet`
- Tree view, deps mode, index mode all working
- Query mode working
- Corpus tests passing (Go, Python, TypeScript)

**Phase 1 Complete!**
- All 48 tasks completed
- Knowledge graph with gob persistence
- Call graph extraction for 6 languages
- CLI commands: `--index`, `--query` with `--from/--to/--depth`
- MCP tools: `trace_path`, `get_callers`, `get_callees`
- Incremental updates with arity filtering
- Test corpus with snapshot testing

### 2024-12-03: Phase 3 Hybrid Retrieval Implementation

**Completed:**
- All Phase 3 tasks except documentation (3.6.4)

**Vector Storage (`graph/vectors.go`):**
- `VectorIndex` interface with Add, Remove, Search, Has, Count, Save, Clear methods
- `InMemoryVectorIndex` implementation with gzip-compressed gob persistence
- Cosine similarity search with pre-computed query norm for efficiency
- Thread-safe with RWMutex for concurrent access

**Embedding Pipeline (`analyze/embed.go`):**
- `NodeToText(node)` converts symbols to embeddable text (kind + name + signature + docstring + path)
- `EmbedNodes(nodes)` with configurable batch size, retries, and progress callback
- `EmbedGraph()` orchestrates embedding generation for an entire graph
- Skips existing embeddings by default (incremental embedding)

**Hybrid Search Engine (`analyze/retriever.go`):**
- Three search modes: Hybrid, Vector-only, Graph-only
- Graph search with exact, prefix, contains, and word matching
- Reciprocal Rank Fusion (RRF) with configurable weights (default: 0.6 vector, 0.4 graph)
- Context expansion with callers/callees and code snippets
- Graceful fallback to graph search when embeddings unavailable

**CLI Commands:**
- `codemap --search --q "query"` - Hybrid/graph search
- `codemap --embed` - Generate embeddings for the graph
- `--limit`, `--expand`, `--json` flags for customization
- Automatic fallback with helpful message when embeddings missing

**MCP Tools:**
- `semantic_search` tool with query, limit, expand parameters
- Structured output with relevance scores and match reasons
- Code snippets included when expand=true
- Updated MCP server to v2.3.0 (14 tools total)

**Files Created/Modified:**
- `graph/vectors.go` - Vector index interface and implementation
- `analyze/embed.go` - Embedding pipeline
- `analyze/retriever.go` - Hybrid search engine
- `main.go` - Added `--search` and `--embed` commands
- `mcp/main.go` - Added `semantic_search` tool

**Testing:**
- All builds pass `go fmt` and `go vet`
- Graph search working (tested with "vector search", "cosine" queries)
- Expand mode shows callers, callees, and code snippets
- JSON output working correctly

**Phase 3 Complete!**
- All 22 tasks completed
- Vector index with gzip-compressed gob persistence
- Hybrid search with Reciprocal Rank Fusion
- CLI commands: `--search`, `--embed`
- MCP tool: `semantic_search`
- README updated with comprehensive documentation

### 2024-12-04: Final Validation & Project Completion

**Validation Results:**

1. **V.1 - Code Quality**: `go fmt ./...` and `go vet ./...` passed without issues

2. **V.2 - CLI Commands**: All commands tested successfully:
   - `codemap .` - Tree view works (105 files, 2.2MB, ~667k tokens)
   - `codemap --index .` - Index built: 506 nodes, 1010 edges in 508ms
   - `codemap --query --from main .` - Query mode working
   - `codemap --query --to main .` - Reverse query working
   - `codemap --search --q "vector" .` - Search mode working (fallback to graph search)
   - `codemap --explain` - Works (requires LLM)
   - `codemap --summarize` - Works (requires LLM)
   - `codemap --deps .` - Dependency flow visualization working

3. **V.3 - MCP Tools**: All 14 tools verified via JSON-RPC:
   - MCP server v2.3.0 responding correctly
   - `status` tool lists all available tools
   - `semantic_search` tool working with graph search fallback
   - All tools registered and callable

4. **V.4 - Cross-Platform Build**: EXPECTED FAILURES (CGO requirement)
   - `GOOS=darwin` build fails due to tree-sitter CGO dependency
   - `GOOS=windows` build fails due to tree-sitter CGO dependency
   - This is a known constraint documented in PLAN.md
   - Native builds on each platform work correctly

5. **V.5 - Performance Benchmarks**: ALL PASSED (<100ms target)
   - Tree view: ~7-10ms
   - Query mode: ~9-10ms
   - Search mode: ~8-9ms
   - Index mode: ~270-280ms (acceptable for full rebuild)

**Project Complete!**

The Codemap GraphRAG implementation is 100% complete:
- **Phase 1**: Knowledge Graph Foundation (48/48 tasks)
- **Phase 2**: Semantic Intelligence / LLM Integration (27/27 tasks)
- **Phase 3**: Hybrid Retrieval & Search (22/22 tasks)
- **Validation**: All tests passed (5/5 tasks)

**Final Statistics:**
- Total tasks: 102
- Completed: 102 (100%)
- CLI commands: 8 modes (tree, deps, skyline, diff, index, query, explain, summarize, search, embed)
- MCP tools: 14 tools
- Supported languages for call extraction: Go, Python, JavaScript, TypeScript, Rust, Java
- LLM providers: Ollama (default), OpenAI, Anthropic
</file>
<file path="development-docs/0004-gemini-integration-plan.md">
# PLAN.md: Integração do Google Gemini API

## 1. Executive Summary

### 1.1. Objetivo
Integrar o **Google Gemini API** como um novo provider de LLM no sistema `codemap`, permitindo que usuários utilizem modelos Gemini para análise, explicação e sumarização de código, além de geração de embeddings para busca semântica.

### 1.2. Escopo

**Incluído:**
- Implementar `GeminiClient` aderindo à interface `analyze.LLMClient`
- Adicionar configuração para `ProviderGemini` com campos específicos
- Integrar nas factories `NewClient` e `NewEmbeddingClient`
- Suportar tanto completion quanto embeddings nativamente
- Documentação completa (README, .env.example)

**Não-Escopo:**
- Streaming responses (arquitetura atual não suporta)
- Suporte a Vertex AI (OAuth/GCP) - apenas Google AI Studio (API Key)
- Multimodal inputs (imagens, áudio) - apenas texto

### 1.3. Definição de Esforço

| Tag | Linhas de Código | Tempo Estimado | Complexidade |
|-----|------------------|----------------|--------------|
| **S** (Small) | < 30 linhas | < 30 min | Alterações pontuais, adição de constantes/campos |
| **M** (Medium) | 30-100 linhas | 30-60 min | Funções completas, structs, lógica moderada |
| **L** (Large) | > 100 linhas | > 1h | Implementações complexas com múltiplas dependências |

---

## 2. Current Situation Analysis

### 2.1. Arquitetura Existente

```
analyze/
├── client.go      # LLMClient interface, Request/Response structs
├── factory.go     # NewClient(), NewEmbeddingClient()
├── openai.go      # OpenAI adapter (~350 linhas)
├── anthropic.go   # Anthropic adapter (~280 linhas, sem embeddings)
├── ollama.go      # Ollama adapter (~300 linhas)
└── tokens.go      # Token estimation utilities

config/
└── config.go      # Provider enum, LLMConfig struct, Validate()
```

### 2.2. Interface LLMClient (analyze/client.go)

```go
type LLMClient interface {
    Complete(ctx context.Context, req *CompletionRequest) (*CompletionResponse, error)
    Embed(ctx context.Context, req *EmbeddingRequest) (*EmbeddingResponse, error)
    Models(ctx context.Context) ([]string, error)
    Ping(ctx context.Context) error
    Name() string
}
```

### 2.3. Structs de Request/Response Existentes

```go
type Message struct {
    Role    string // "system", "user", "assistant"
    Content string
}

type CompletionRequest struct {
    Messages    []Message
    Model       string
    Temperature float64
    MaxTokens   int
    Stop        []string
}

type CompletionResponse struct {
    Content      string
    Model        string
    FinishReason string  // "stop", "length", "content_filter", "other"
    Duration     time.Duration
    Usage        TokenUsage
}

type EmbeddingRequest struct {
    Text  string
    Model string
}

type EmbeddingResponse struct {
    Embedding []float64
    Model     string
    Duration  time.Duration
    Usage     TokenUsage
}

type TokenUsage struct {
    PromptTokens     int
    CompletionTokens int
    TotalTokens      int
}
```

### 2.4. Providers Atuais - Comparativo

| Provider | Completion | Embeddings | Auth Header | Notas |
|----------|------------|------------|-------------|-------|
| OpenAI | ✅ | ✅ | `Authorization: Bearer` | Referência de implementação |
| Anthropic | ✅ | ❌ | `x-api-key` | Fallback para Ollama em embeddings |
| Ollama | ✅ | ✅ | Nenhum | Local, sem API key |
| **Gemini** | 🎯 | 🎯 | `x-goog-api-key` | **A implementar** |

### 2.5. Gaps Identificados

1. **Mapeamento de Mensagens:** Gemini usa formato diferente para system messages
2. **Roles Diferentes:** `assistant` → `model` no Gemini
3. **Estrutura de Content:** `content: string` → `parts: [{text: string}]`
4. **Finish Reasons:** Nomenclatura diferente (STOP vs stop)
5. **Token Usage em Embeddings:** Gemini não retorna contagem de tokens

---

## 3. Gemini API Technical Reference

### 3.1. Endpoints

| Operação | Endpoint | Método |
|----------|----------|--------|
| Generate | `https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent` | POST |
| Embed | `https://generativelanguage.googleapis.com/v1beta/models/{model}:embedContent` | POST |
| List Models | `https://generativelanguage.googleapis.com/v1beta/models` | GET |

### 3.2. Autenticação

**Header obrigatório:**
```
x-goog-api-key: $GEMINI_API_KEY
Content-Type: application/json
```

> ⚠️ **IMPORTANTE:** NÃO usar query parameter `?key=` para evitar vazamento de chaves em logs de servidor e histórico de navegador.

### 3.3. Modelos Disponíveis (Dezembro 2024)

**Completion:**
| Modelo | Context Window | Notas |
|--------|----------------|-------|
| `gemini-2.0-flash-exp` | 1M tokens | Mais recente, experimental |
| `gemini-1.5-flash` | 1M tokens | Estável, rápido |
| `gemini-1.5-flash-8b` | 1M tokens | Menor, mais econômico |
| `gemini-1.5-pro` | 2M tokens | Maior capacidade |

**Embeddings:**
| Modelo | Dimensões | Notas |
|--------|-----------|-------|
| `text-embedding-004` | 768 | Recomendado |
| `embedding-001` | 768 | Legacy |

### 3.4. Request Format - GenerateContent

```json
{
  "systemInstruction": {
    "parts": [{"text": "You are a code analysis assistant."}]
  },
  "contents": [
    {
      "role": "user",
      "parts": [{"text": "Explain this function..."}]
    },
    {
      "role": "model",
      "parts": [{"text": "This function does..."}]
    }
  ],
  "generationConfig": {
    "temperature": 0.1,
    "maxOutputTokens": 2048,
    "stopSequences": ["```"]
  }
}
```

**Diferenças Críticas vs OpenAI:**

| Aspecto | OpenAI | Gemini |
|---------|--------|--------|
| System message | `role: "system"` no array | Campo separado `systemInstruction` |
| Assistant role | `role: "assistant"` | `role: "model"` |
| Content | `content: string` | `parts: [{text: string}]` |
| Max tokens | `max_tokens` | `maxOutputTokens` |

### 3.5. Response Format - GenerateContent

```json
{
  "candidates": [{
    "content": {
      "parts": [{"text": "Response text here..."}],
      "role": "model"
    },
    "finishReason": "STOP",
    "index": 0
  }],
  "usageMetadata": {
    "promptTokenCount": 150,
    "candidatesTokenCount": 500,
    "totalTokenCount": 650
  },
  "modelVersion": "gemini-2.0-flash-exp"
}
```

### 3.6. Request Format - EmbedContent

```json
{
  "content": {
    "parts": [{"text": "Text to embed"}]
  }
}
```

### 3.7. Response Format - EmbedContent

```json
{
  "embedding": {
    "values": [0.123, -0.456, 0.789, ...]
  }
}
```

> ⚠️ **NOTA:** O endpoint de embedding NÃO retorna contagem de tokens. `EmbeddingResponse.Usage` será retornado zerado.

### 3.8. Error Response Format

```json
{
  "error": {
    "code": 400,
    "message": "Invalid value at 'contents[0].parts'",
    "status": "INVALID_ARGUMENT"
  }
}
```

### 3.9. Códigos de Erro HTTP

| Código | Status | Significado | Ação |
|--------|--------|-------------|------|
| `400` | INVALID_ARGUMENT | Request malformado | Verificar formato JSON |
| `401` | UNAUTHENTICATED | API key ausente | Verificar header |
| `403` | PERMISSION_DENIED | API key inválida | Retornar `ErrNoAPIKey` |
| `404` | NOT_FOUND | Modelo não existe | Retornar `ErrModelNotFound` |
| `429` | RESOURCE_EXHAUSTED | Rate limit | Retry com backoff |
| `500` | INTERNAL | Erro servidor | Retry com backoff |
| `503` | UNAVAILABLE | Serviço indisponível | Retry com backoff |

### 3.10. Finish Reason Mapping

| Gemini | Padrão codemap | Descrição |
|--------|----------------|-----------|
| `STOP` | `"stop"` | Geração completa normalmente |
| `MAX_TOKENS` | `"length"` | Limite de tokens atingido |
| `SAFETY` | `"content_filter"` | Bloqueado por filtro de segurança |
| `RECITATION` | `"content_filter"` | Bloqueado por detecção de cópia |
| `OTHER` | `"other"` | Motivo não especificado |
| _(vazio/null)_ | `"other"` | Fallback para casos inesperados |

---

## 4. Implementation Plan

### Phase 1: Configuration (`config/config.go`)

**Objetivo:** Preparar o sistema para reconhecer e configurar o provider Gemini.

#### Task 1.1: Adicionar Provider Enum [S]

**Arquivo:** `config/config.go`
**Localização:** Após linha 26 (após `ProviderAnthropic`)

```go
const (
    ProviderOllama    Provider = "ollama"
    ProviderOpenAI    Provider = "openai"
    ProviderAnthropic Provider = "anthropic"
    ProviderGemini    Provider = "gemini"  // NEW
)
```

**Critério de Conclusão:** Constante `ProviderGemini` declarada e compilável.

---

#### Task 1.2: Adicionar Campos no LLMConfig [S]

**Arquivo:** `config/config.go`
**Localização:** Dentro de `LLMConfig struct`, após campos Anthropic

```go
// Gemini-specific settings
GeminiAPIKey  string `yaml:"gemini_api_key"`  // GEMINI_API_KEY env var
GeminiBaseURL string `yaml:"gemini_base_url"` // Default: generativelanguage.googleapis.com
```

**Critério de Conclusão:** Campos adicionados com tags YAML corretas.

---

#### Task 1.3: Atualizar applyEnvOverrides() [S]

**Arquivo:** `config/config.go`
**Localização:** Função `applyEnvOverrides()`, após bloco Anthropic

```go
// Gemini
if v := os.Getenv("GEMINI_API_KEY"); v != "" {
    cfg.LLM.GeminiAPIKey = v
}
if v := os.Getenv("GEMINI_BASE_URL"); v != "" {
    cfg.LLM.GeminiBaseURL = v
}
```

**Critério de Conclusão:** Variáveis de ambiente `GEMINI_API_KEY` e `GEMINI_BASE_URL` reconhecidas.

---

#### Task 1.4: Atualizar Validate() [S]

**Arquivo:** `config/config.go`
**Localização:** Função `Validate()`, dentro do switch de providers

```go
case ProviderGemini:
    if c.LLM.GeminiAPIKey == "" {
        errs = append(errs, "gemini_api_key required for gemini provider (or set GEMINI_API_KEY env var)")
    }
```

**Critério de Conclusão:** Validação falha com mensagem clara quando API key está ausente.

---

### Phase 2: GeminiClient Implementation (`analyze/gemini.go`)

**Objetivo:** Criar o adaptador funcional para a API Gemini.

#### Task 2.1: Criar Arquivo e Struct Base [M]

**Arquivo:** `analyze/gemini.go` (NOVO)

```go
package analyze

import (
    "bytes"
    "context"
    "encoding/json"
    "fmt"
    "io"
    "net/http"
    "time"
)

// GeminiClient implements LLMClient for Google's Gemini API.
type GeminiClient struct {
    apiKey     string
    baseURL    string
    httpClient *http.Client
    config     ClientConfig
}

// NewGeminiClient creates a new Gemini client.
func NewGeminiClient(apiKey, baseURL string, cfg ClientConfig) (*GeminiClient, error) {
    if apiKey == "" {
        return nil, ErrNoAPIKey
    }
    if baseURL == "" {
        baseURL = "https://generativelanguage.googleapis.com/v1beta"
    }
    if cfg.Model == "" {
        cfg.Model = "gemini-2.0-flash-exp"
    }
    if cfg.EmbeddingModel == "" {
        cfg.EmbeddingModel = "text-embedding-004"
    }
    if cfg.Timeout == 0 {
        cfg.Timeout = 60 * time.Second
    }

    return &GeminiClient{
        apiKey:  apiKey,
        baseURL: baseURL,
        httpClient: &http.Client{
            Timeout: cfg.Timeout,
        },
        config: cfg,
    }, nil
}
```

**Critério de Conclusão:** Struct criada, constructor com validação e defaults.

---

#### Task 2.2: Definir Request/Response Types [M]

**Arquivo:** `analyze/gemini.go`

```go
// geminiPart represents a content part in Gemini API.
type geminiPart struct {
    Text string `json:"text"`
}

// geminiContent represents a message in Gemini API.
type geminiContent struct {
    Role  string       `json:"role,omitempty"`
    Parts []geminiPart `json:"parts"`
}

// geminiGenerationConfig holds generation parameters.
type geminiGenerationConfig struct {
    Temperature     float64  `json:"temperature,omitempty"`
    MaxOutputTokens int      `json:"maxOutputTokens,omitempty"`
    StopSequences   []string `json:"stopSequences,omitempty"`
}

// geminiRequest is the request format for Gemini's generateContent API.
type geminiRequest struct {
    SystemInstruction *geminiContent          `json:"systemInstruction,omitempty"`
    Contents          []geminiContent         `json:"contents"`
    GenerationConfig  *geminiGenerationConfig `json:"generationConfig,omitempty"`
}

// geminiResponse is the response format from Gemini's generateContent API.
type geminiResponse struct {
    Candidates []struct {
        Content      geminiContent `json:"content"`
        FinishReason string        `json:"finishReason"`
        Index        int           `json:"index"`
    } `json:"candidates"`
    UsageMetadata struct {
        PromptTokenCount     int `json:"promptTokenCount"`
        CandidatesTokenCount int `json:"candidatesTokenCount"`
        TotalTokenCount      int `json:"totalTokenCount"`
    } `json:"usageMetadata"`
    ModelVersion string `json:"modelVersion"`
}

// geminiErrorResponse is the error format from Gemini API.
type geminiErrorResponse struct {
    Error struct {
        Code    int    `json:"code"`
        Message string `json:"message"`
        Status  string `json:"status"`
    } `json:"error"`
}

// geminiEmbedRequest is the request format for Gemini's embedContent API.
type geminiEmbedRequest struct {
    Content geminiContent `json:"content"`
}

// geminiEmbedResponse is the response format from Gemini's embedContent API.
type geminiEmbedResponse struct {
    Embedding struct {
        Values []float64 `json:"values"`
    } `json:"embedding"`
}
```

**Critério de Conclusão:** Todas as 8 structs definidas e mapeando corretamente o JSON da API.

---

#### Task 2.3: Implementar Name() e Ping() [S]

**Arquivo:** `analyze/gemini.go`

```go
// Name returns the provider name.
func (c *GeminiClient) Name() string {
    return "gemini"
}

// Ping checks if the Gemini API is reachable by calling the models endpoint.
// This validates both connectivity and API key validity.
func (c *GeminiClient) Ping(ctx context.Context) error {
    req, err := http.NewRequestWithContext(ctx, "GET", c.baseURL+"/models", nil)
    if err != nil {
        return err
    }
    req.Header.Set("x-goog-api-key", c.apiKey)

    resp, err := c.httpClient.Do(req)
    if err != nil {
        return fmt.Errorf("gemini not reachable: %w", err)
    }
    defer resp.Body.Close()

    if resp.StatusCode == http.StatusUnauthorized || resp.StatusCode == http.StatusForbidden {
        return ErrNoAPIKey
    }
    if resp.StatusCode != http.StatusOK {
        return fmt.Errorf("gemini returned status %d", resp.StatusCode)
    }

    return nil
}
```

**Critério de Conclusão:** `Ping()` valida conectividade E credenciais via `/models`.

---

#### Task 2.4: Implementar Models() [S]

**Arquivo:** `analyze/gemini.go`

```go
// Models returns a list of available Gemini models.
// Returns a static list of known models for simplicity and speed.
func (c *GeminiClient) Models(ctx context.Context) ([]string, error) {
    return []string{
        // Completion models
        "gemini-2.0-flash-exp",
        "gemini-1.5-flash",
        "gemini-1.5-flash-8b",
        "gemini-1.5-pro",
        // Embedding models
        "text-embedding-004",
        "embedding-001",
    }, nil
}
```

**Critério de Conclusão:** Lista estática retornada sem chamada de API (otimização).

---

#### Task 2.5: Implementar Complete() [L]

**Arquivo:** `analyze/gemini.go`

```go
// Complete generates a completion using Gemini's generateContent API.
func (c *GeminiClient) Complete(ctx context.Context, req *CompletionRequest) (*CompletionResponse, error) {
    start := time.Now()

    // Use request values or fall back to config defaults
    model := req.Model
    if model == "" {
        model = c.config.Model
    }

    temp := req.Temperature
    if temp == 0 {
        temp = c.config.Temperature
    }

    maxTokens := req.MaxTokens
    if maxTokens == 0 {
        maxTokens = c.config.MaxTokens
    }

    // Convert messages to Gemini format
    // CRITICAL: Extract system messages into separate systemInstruction field
    var systemInstruction *geminiContent
    contents := make([]geminiContent, 0, len(req.Messages))

    for _, m := range req.Messages {
        switch m.Role {
        case "system":
            // Gemini uses a separate systemInstruction field (not in contents array)
            if systemInstruction == nil {
                systemInstruction = &geminiContent{
                    Parts: []geminiPart{{Text: m.Content}},
                }
            } else {
                // Multiple system messages: append to existing
                systemInstruction.Parts = append(systemInstruction.Parts, geminiPart{Text: m.Content})
            }
        case "assistant":
            // CRITICAL: Gemini uses "model" instead of "assistant"
            contents = append(contents, geminiContent{
                Role:  "model",
                Parts: []geminiPart{{Text: m.Content}},
            })
        default:
            // "user" role stays the same
            contents = append(contents, geminiContent{
                Role:  m.Role,
                Parts: []geminiPart{{Text: m.Content}},
            })
        }
    }

    geminiReq := geminiRequest{
        SystemInstruction: systemInstruction,
        Contents:          contents,
        GenerationConfig: &geminiGenerationConfig{
            Temperature:     temp,
            MaxOutputTokens: maxTokens,
            StopSequences:   req.Stop,
        },
    }

    body, err := json.Marshal(geminiReq)
    if err != nil {
        return nil, fmt.Errorf("failed to marshal request: %w", err)
    }

    endpoint := fmt.Sprintf("%s/models/%s:generateContent", c.baseURL, model)
    httpReq, err := http.NewRequestWithContext(ctx, "POST", endpoint, bytes.NewReader(body))
    if err != nil {
        return nil, err
    }
    httpReq.Header.Set("Content-Type", "application/json")
    httpReq.Header.Set("x-goog-api-key", c.apiKey)

    // Retry logic with exponential backoff (same pattern as OpenAI/Anthropic)
    var resp *http.Response
    var lastErr error
    for attempt := 0; attempt <= c.config.MaxRetries; attempt++ {
        if attempt > 0 {
            // Exponential backoff: 1s, 4s, 9s, ...
            time.Sleep(time.Duration(attempt*attempt) * time.Second)
        }

        resp, lastErr = c.httpClient.Do(httpReq)
        if lastErr == nil {
            // Check for rate limiting (429)
            if resp.StatusCode == http.StatusTooManyRequests {
                resp.Body.Close()
                lastErr = ErrRateLimited
                continue
            }
            // Check for server errors (5xx) - retry
            if resp.StatusCode >= 500 {
                resp.Body.Close()
                lastErr = fmt.Errorf("server error: %d", resp.StatusCode)
                continue
            }
            if resp.StatusCode == http.StatusOK {
                break
            }
        }

        if resp != nil {
            resp.Body.Close()
        }

        if ctx.Err() != nil {
            return nil, ErrTimeout
        }
    }

    if lastErr != nil {
        return nil, fmt.Errorf("request failed after %d retries: %w", c.config.MaxRetries, lastErr)
    }
    defer resp.Body.Close()

    // Handle non-OK responses
    if resp.StatusCode != http.StatusOK {
        bodyBytes, _ := io.ReadAll(resp.Body)
        var errResp geminiErrorResponse
        if json.Unmarshal(bodyBytes, &errResp) == nil && errResp.Error.Message != "" {
            return nil, fmt.Errorf("gemini error: %s", errResp.Error.Message)
        }
        return nil, fmt.Errorf("gemini error (status %d): %s", resp.StatusCode, string(bodyBytes))
    }

    var geminiResp geminiResponse
    if err := json.NewDecoder(resp.Body).Decode(&geminiResp); err != nil {
        return nil, fmt.Errorf("failed to parse response: %w", err)
    }

    if len(geminiResp.Candidates) == 0 {
        return nil, fmt.Errorf("no candidates returned")
    }

    // Extract text from all parts
    var content string
    for _, part := range geminiResp.Candidates[0].Content.Parts {
        content += part.Text
    }

    // Map finish reason to standard format
    finishReason := mapGeminiFinishReason(geminiResp.Candidates[0].FinishReason)

    return &CompletionResponse{
        Content:      content,
        Model:        geminiResp.ModelVersion,
        FinishReason: finishReason,
        Duration:     time.Since(start),
        Usage: TokenUsage{
            PromptTokens:     geminiResp.UsageMetadata.PromptTokenCount,
            CompletionTokens: geminiResp.UsageMetadata.CandidatesTokenCount,
            TotalTokens:      geminiResp.UsageMetadata.TotalTokenCount,
        },
    }, nil
}

// mapGeminiFinishReason converts Gemini finish reasons to standard format.
func mapGeminiFinishReason(reason string) string {
    switch reason {
    case "STOP":
        return "stop"
    case "MAX_TOKENS":
        return "length"
    case "SAFETY", "RECITATION":
        return "content_filter"
    default:
        return "other"
    }
}
```

**Critério de Conclusão:**
- Conversão de mensagens funciona (system → systemInstruction, assistant → model)
- Retry logic implementada
- Finish reasons mapeados corretamente
- Token usage extraído

---

#### Task 2.6: Implementar Embed() [M]

**Arquivo:** `analyze/gemini.go`

```go
// Embed generates an embedding vector using Gemini's embedContent API.
// NOTE: Gemini's embedding endpoint does NOT return token usage information.
func (c *GeminiClient) Embed(ctx context.Context, req *EmbeddingRequest) (*EmbeddingResponse, error) {
    start := time.Now()

    model := req.Model
    if model == "" {
        model = c.config.EmbeddingModel
    }

    geminiReq := geminiEmbedRequest{
        Content: geminiContent{
            Parts: []geminiPart{{Text: req.Text}},
        },
    }

    body, err := json.Marshal(geminiReq)
    if err != nil {
        return nil, fmt.Errorf("failed to marshal request: %w", err)
    }

    endpoint := fmt.Sprintf("%s/models/%s:embedContent", c.baseURL, model)
    httpReq, err := http.NewRequestWithContext(ctx, "POST", endpoint, bytes.NewReader(body))
    if err != nil {
        return nil, err
    }
    httpReq.Header.Set("Content-Type", "application/json")
    httpReq.Header.Set("x-goog-api-key", c.apiKey)

    resp, err := c.httpClient.Do(httpReq)
    if err != nil {
        return nil, fmt.Errorf("embed request failed: %w", err)
    }
    defer resp.Body.Close()

    if resp.StatusCode != http.StatusOK {
        bodyBytes, _ := io.ReadAll(resp.Body)
        return nil, fmt.Errorf("gemini embed error (status %d): %s", resp.StatusCode, string(bodyBytes))
    }

    var geminiResp geminiEmbedResponse
    if err := json.NewDecoder(resp.Body).Decode(&geminiResp); err != nil {
        return nil, fmt.Errorf("failed to parse response: %w", err)
    }

    if len(geminiResp.Embedding.Values) == 0 {
        return nil, fmt.Errorf("no embedding returned")
    }

    return &EmbeddingResponse{
        Embedding: geminiResp.Embedding.Values,
        Model:     model,
        Duration:  time.Since(start),
        // Usage is empty - Gemini embed endpoint doesn't return token counts
        Usage: TokenUsage{},
    }, nil
}
```

**Critério de Conclusão:** Embeddings gerados, `Usage` retornado zerado (documentado).

---

### Phase 3: Factory Integration (`analyze/factory.go`)

**Objetivo:** Conectar o novo cliente ao sistema de seleção de providers.

#### Task 3.1: Atualizar NewClient() [S]

**Arquivo:** `analyze/factory.go`
**Localização:** Dentro do switch de `cfg.LLM.Provider`

```go
case config.ProviderGemini:
    return NewGeminiClient(cfg.LLM.GeminiAPIKey, cfg.LLM.GeminiBaseURL, clientCfg)
```

**Critério de Conclusão:** `NewClient()` retorna `GeminiClient` quando provider é "gemini".

---

#### Task 3.2: Verificar NewEmbeddingClient() [S]

**Análise:** O Gemini suporta embeddings nativamente, então a lógica existente funciona:

```go
func NewEmbeddingClient(cfg *config.Config) (LLMClient, error) {
    // Se provider explícito para embeddings, usar ele
    if cfg.LLM.EmbeddingProvider != "" {
        embCfg := *cfg
        embCfg.LLM.Provider = config.Provider(cfg.LLM.EmbeddingProvider)
        return NewClient(&embCfg)
    }

    // Anthropic não tem embeddings - fallback para Ollama
    if cfg.LLM.Provider == config.ProviderAnthropic {
        clientCfg := ClientConfig{...}
        return NewOllamaClient(cfg.LLM.OllamaURL, clientCfg), nil
    }

    // OpenAI, Ollama, e GEMINI suportam embeddings nativamente
    return NewClient(cfg)  // ✅ Gemini passa por aqui
}
```

**Critério de Conclusão:** Nenhuma modificação necessária - verificar que funciona.

---

### Phase 4: Documentation & Testing

#### Task 4.1: Atualizar .env.example [S]

**Arquivo:** `.env.example`

```bash
# =============================================================================
# GEMINI CONFIGURATION
# =============================================================================

# Gemini API key (required for gemini provider)
# GEMINI_API_KEY=your-api-key-here

# Gemini base URL (optional, default: https://generativelanguage.googleapis.com/v1beta)
# GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta
```

---

#### Task 4.2: Atualizar README.md [S]

**Localizações a atualizar:**
1. Lista de providers no overview
2. Tabela de serviços externos
3. Seção de configuração

---

#### Task 4.3: Build e Verificação [S]

```bash
go build -o codemap .    # Deve compilar sem erros
go vet ./...             # Deve passar sem warnings
go fmt ./...             # Formatar código
```

---

## 5. File Changes Summary

| Arquivo | Tipo | Linhas | Descrição |
|---------|------|--------|-----------|
| `config/config.go` | Modificação | ~20 | Adicionar `ProviderGemini`, campos, env overrides, validation |
| `analyze/gemini.go` | **Novo** | ~320 | Implementação completa do `GeminiClient` |
| `analyze/factory.go` | Modificação | ~3 | Adicionar case `ProviderGemini` |
| `.env.example` | Modificação | ~10 | Seção de configuração Gemini |
| `README.md` | Modificação | ~10 | Documentar novo provider |

**Total estimado:** ~360 linhas de código

---

## 6. Risk Mitigation

| Risco | Probabilidade | Impacto | Mitigação |
|-------|--------------|---------|-----------|
| **R1:** Safety filters bloqueando código válido | Média | Alto | Capturar `finishReason: "SAFETY"` e retornar erro descritivo |
| **R2:** Rate limiting agressivo | Média | Médio | Exponential backoff já implementado |
| **R3:** Mudanças na API v1beta | Baixa | Alto | Usar versionamento explícito no `baseURL` |
| **R4:** Embedding dimensions diferentes | Baixa | Médio | `text-embedding-004` retorna 768 dims (compatível com Ollama) |
| **R5:** Token usage ausente em embeddings | Certa | Baixo | Documentado, retornar `TokenUsage{}` zerado |

---

## 7. Testing Strategy

### 7.1. Testes de Compilação

```bash
# Build
go build -o codemap .

# Static analysis
go vet ./...

# Format check
go fmt ./...
```

### 7.2. Testes de Validação (sem API key)

```bash
# Deve falhar com mensagem clara
CODEMAP_LLM_PROVIDER=gemini ./codemap --explain --symbol main . 2>&1
# Esperado: "gemini_api_key required for gemini provider"
```

### 7.3. Testes de Integração (com API key)

```bash
# Configurar
export GEMINI_API_KEY="your-key-here"
export CODEMAP_LLM_PROVIDER="gemini"

# Test Ping (implícito no explain)
./codemap --explain --symbol "main" . 2>&1 | head -5
# Esperado: Não deve mostrar erro de conexão

# Test Complete
./codemap --explain --symbol "NewGraphBuilder" .
# Esperado: Explicação do símbolo

# Test Embed + Search
./codemap --embed .
./codemap --search "function that builds graph" .
# Esperado: Resultados de busca semântica
```

### 7.4. Checklist de Validação

- [ ] `go build` compila sem erros
- [ ] `go vet ./...` sem warnings
- [ ] `go fmt ./...` sem alterações
- [ ] Sem API key → erro claro na validação
- [ ] `--explain` funciona com Gemini
- [ ] `--embed` gera embeddings com Gemini
- [ ] `--search` retorna resultados após embedding
- [ ] Cache funciona corretamente com Gemini

---

## 8. Success Metrics

| Métrica | Critério | Como Medir |
|---------|----------|------------|
| **Funcionalidade** | Usuário pode usar `--explain`, `--summarize`, `--embed`, `--search` com Gemini | Testes manuais |
| **Confiabilidade** | Taxa de erro < 1% em chamadas válidas | Logs de erro |
| **Performance** | Latência similar a OpenAI (~2-5s para completion) | Timing nos logs |
| **Manutenibilidade** | Código segue padrões dos clientes existentes | Code review |

---

## 9. Implementation Checklist

### Phase 1: Configuration
- [ ] 1.1 Adicionar `ProviderGemini` ao enum
- [ ] 1.2 Adicionar `GeminiAPIKey`, `GeminiBaseURL` no `LLMConfig`
- [ ] 1.3 Atualizar `applyEnvOverrides()` para `GEMINI_API_KEY`, `GEMINI_BASE_URL`
- [ ] 1.4 Atualizar `Validate()` para validar `GeminiAPIKey`

### Phase 2: GeminiClient
- [ ] 2.1 Criar struct `GeminiClient` e `NewGeminiClient()`
- [ ] 2.2 Definir tipos de request/response (8 structs)
- [ ] 2.3 Implementar `Name()` e `Ping()`
- [ ] 2.4 Implementar `Models()`
- [ ] 2.5 Implementar `Complete()` com conversão de mensagens e retry
- [ ] 2.6 Implementar `Embed()`

### Phase 3: Factory Integration
- [ ] 3.1 Adicionar case `ProviderGemini` em `NewClient()`
- [ ] 3.2 Verificar `NewEmbeddingClient()` funciona com Gemini

### Phase 4: Documentation & Testing
- [ ] 4.1 Atualizar `.env.example`
- [ ] 4.2 Atualizar `README.md`
- [ ] 4.3 Executar build e verificação
- [ ] 4.4 Executar testes de integração (se API key disponível)

---

## 10. Assumptions

1. O Gemini API está acessível com uma API Key válida do Google AI Studio
2. O formato `v1beta` do Gemini API permanecerá estável durante a implementação
3. A lógica de retry existente é suficiente para lidar com rate limiting
4. O modelo `text-embedding-004` (768 dims) é compatível com o sistema de vetores existente

---

## 11. References

- [Google Gemini API Documentation](https://ai.google.dev/docs)
- [Gemini API Reference - generateContent](https://ai.google.dev/api/rest/v1beta/models/generateContent)
- [Gemini API Reference - embedContent](https://ai.google.dev/api/rest/v1beta/models/embedContent)
- Implementação de referência: `analyze/openai.go`
</file>
<file path="development-docs/0004-gemini-integration.md">
# TASKS.md: Google Gemini API Integration

## Project Briefing

**Objective:** Integrate Google Gemini API as a new LLM provider in codemap, enabling users to leverage Gemini models for code analysis, explanation, summarization, and semantic search via embeddings.

**Scope:**
- Implement `GeminiClient` adhering to `analyze.LLMClient` interface
- Add configuration support for `ProviderGemini` with API key and base URL
- Integrate in factory functions (`NewClient`, `NewEmbeddingClient`)
- Update documentation (.env.example, README.md)

**Non-Scope:**
- Streaming responses
- Vertex AI (OAuth/GCP) - only Google AI Studio (API Key)
- Multimodal inputs (images, audio)

---

## Implementation Checklist

### Phase 1: Configuration (`config/config.go`)

- [x] 1.1 Add `ProviderGemini` to provider enum
- [x] 1.2 Add `GeminiAPIKey`, `GeminiBaseURL` fields to `LLMConfig`
- [x] 1.3 Update `applyEnvOverrides()` for `GEMINI_API_KEY`, `GEMINI_BASE_URL`
- [x] 1.4 Update `Validate()` to validate `GeminiAPIKey`

### Phase 2: GeminiClient Implementation (`analyze/gemini.go`)

- [x] 2.1 Create `GeminiClient` struct and `NewGeminiClient()` constructor
- [x] 2.2 Define request/response types (8 structs)
  - [x] `geminiPart`
  - [x] `geminiContent`
  - [x] `geminiGenerationConfig`
  - [x] `geminiRequest`
  - [x] `geminiResponse`
  - [x] `geminiErrorResponse`
  - [x] `geminiEmbedRequest`
  - [x] `geminiEmbedResponse`
- [x] 2.3 Implement `Name()` and `Ping()` methods
- [x] 2.4 Implement `Models()` method (static list)
- [x] 2.5 Implement `Complete()` with:
  - [x] Message conversion (system → systemInstruction, assistant → model)
  - [x] Retry logic with exponential backoff
  - [x] Finish reason mapping
  - [x] Token usage extraction
- [x] 2.6 Implement `Embed()` method

### Phase 3: Factory Integration (`analyze/factory.go`)

- [x] 3.1 Add `ProviderGemini` case in `NewClient()`
- [x] 3.2 Verify `NewEmbeddingClient()` works with Gemini (no changes needed)

### Phase 4: Documentation & Testing

- [x] 4.1 Update `.env.example` with Gemini configuration section
- [x] 4.2 Update `README.md` to document Gemini provider
- [x] 4.3 Build and verification
  - [x] `go build -o codemap .` compiles without errors
  - [x] `go vet ./...` passes without warnings
  - [x] `go fmt ./...` produces no changes
- [x] 4.4 Validation test (without API key)
  - [x] Verify error message: "gemini_api_key required for gemini provider"
- [x] 4.5 Integration tests (with API key)
  - [x] Ping API successful
  - [x] Complete generates text correctly
  - [x] `--embed` generates embeddings with Gemini (768 dimensions)

---

## Validation & Testing

### Build Verification
```bash
go build -o codemap .    # ✅ Compiles without errors
go vet ./...             # ✅ Passes without warnings
go fmt ./...             # ✅ Produces no changes
```

### Validation Test (No API Key)
```bash
# Verified via direct test - returns:
# "config validation failed: gemini_api_key required for gemini provider (or set GEMINI_API_KEY env var)"
```

### Integration Test (With API Key)
```bash
export GEMINI_API_KEY="your-key-here"
export CODEMAP_LLM_PROVIDER="gemini"
export CODEMAP_EMBEDDING_MODEL="text-embedding-004"

# Test embeddings - ✅ Verified working (768 dimensions)
./codemap --embed .
```

---

## Files Changed

| File | Type | Status |
|------|------|--------|
| `config/config.go` | Modified | ✅ Complete |
| `analyze/gemini.go` | New | ✅ Complete |
| `analyze/factory.go` | Modified | ✅ Complete |
| `.env.example` | Modified | ✅ Complete |
| `README.md` | Modified | ✅ Complete |

---

## Additional Changes Made During Implementation

### Bug Fixes
1. **Retry Logic Bug (analyze/gemini.go):** Fixed issue where HTTP response body was closed prematurely for 4xx errors, preventing proper error message extraction

### Model Updates
1. **Default Model Updated:** Changed from `gemini-2.0-flash-exp` to `gemini-2.0-flash` (stable release)
2. **Models() Updated:** Updated available models list to reflect current API (gemini-2.5-flash, gemini-2.5-pro, gemini-2.0-flash, gemini-2.0-flash-exp)

### Configuration Enhancements
1. **Embedding Environment Variables:** Added `CODEMAP_EMBEDDING_MODEL` and `CODEMAP_EMBEDDING_PROVIDER` env var overrides in `config/config.go`
2. **Updated .env.example:** Added documentation for embedding model configuration per provider

---

## Notes

- Implementation follows the existing adapter pattern from OpenAI/Anthropic/Ollama clients
- Gemini embedding endpoint does NOT return token usage (documented, returns zero)
- System messages are converted to Gemini's `systemInstruction` field
- Role mapping: `assistant` → `model`
- Finish reason mapping: `STOP` → `stop`, `MAX_TOKENS` → `length`, `SAFETY`/`RECITATION` → `content_filter`
- When using Gemini for embeddings, set `CODEMAP_EMBEDDING_MODEL=text-embedding-004`

---

## Summary

The Google Gemini API integration is **complete and verified**. All tests pass:
- ✅ Build compiles without errors
- ✅ Static analysis (go vet) passes
- ✅ Code formatted correctly
- ✅ API validation works correctly
- ✅ Complete API generates text successfully
- ✅ Embed API generates 768-dimension vectors
</file>
<file path="graph/builder.go">
package graph

import (
	"fmt"
	"path/filepath"
	"strings"
	"time"

	ignore "github.com/sabhiram/go-gitignore"
)

// Builder constructs a CodeGraph from a codebase.
type Builder struct {
	graph      *CodeGraph
	rootPath   string
	gitignore  *ignore.GitIgnore
	progress   func(msg string)
	fileCount  int
	errorCount int
}

// BuilderOption configures the graph builder.
type BuilderOption func(*Builder)

// WithProgress sets a progress callback.
func WithProgress(fn func(msg string)) BuilderOption {
	return func(b *Builder) {
		b.progress = fn
	}
}

// WithGitignore sets the gitignore matcher.
func WithGitignore(gi *ignore.GitIgnore) BuilderOption {
	return func(b *Builder) {
		b.gitignore = gi
	}
}

// WithExistingGraph sets an existing graph for incremental updates.
func WithExistingGraph(g *CodeGraph) BuilderOption {
	return func(b *Builder) {
		b.graph = g
	}
}

// NewBuilder creates a new graph builder.
func NewBuilder(rootPath string, opts ...BuilderOption) *Builder {
	b := &Builder{
		graph:    NewCodeGraph(rootPath),
		rootPath: rootPath,
		progress: func(msg string) {}, // Default: no-op
	}

	for _, opt := range opts {
		opt(b)
	}

	return b
}

// FileAnalysis represents the analysis result from scanner.
type FileAnalysis struct {
	Path      string
	Language  string
	Functions []FuncInfo
	Types     []TypeInfo
	Imports   []string
	Calls     []CallInfo
}

// FuncInfo represents a function/method from scanner.
type FuncInfo struct {
	Name       string
	Signature  string
	Receiver   string
	IsExported bool
	Line       int
	EndLine    int
	ParamCount int // Number of parameters (-1 if unknown/variadic)
}

// TypeInfo represents a type definition from scanner.
type TypeInfo struct {
	Name       string
	Kind       string
	IsExported bool
	Line       int
}

// CallInfo represents a function call from scanner.
type CallInfo struct {
	CallerFunc string
	CallerLine int
	CalleeName string
	CallLine   int
	Args       int
	Receiver   string
}

// AddFile adds a file's analysis to the graph.
func (b *Builder) AddFile(analysis *FileAnalysis) error {
	if analysis == nil {
		return nil
	}

	b.fileCount++
	b.progress(fmt.Sprintf("Processing %s", analysis.Path))

	// Create file node
	fileID := GenerateNodeID(analysis.Path, "")
	fileNode := &Node{
		ID:      fileID,
		Kind:    KindFile,
		Name:    filepath.Base(analysis.Path),
		Path:    analysis.Path,
		Package: getPackageFromPath(analysis.Path),
	}
	b.graph.AddNode(fileNode)

	// Process functions
	funcNodes := make(map[string]NodeID) // name -> nodeID for call resolution
	for _, fn := range analysis.Functions {
		funcID := GenerateNodeID(analysis.Path, fn.Name)
		funcNode := &Node{
			ID:         funcID,
			Kind:       kindFromFunc(fn),
			Name:       fn.Name,
			Path:       analysis.Path,
			Line:       fn.Line,
			EndLine:    fn.EndLine,
			Signature:  fn.Signature,
			Exported:   fn.IsExported,
			ParamCount: fn.ParamCount,
		}
		b.graph.AddNode(funcNode)
		funcNodes[fn.Name] = funcID

		// File contains function
		b.graph.AddEdge(&Edge{
			From: fileID,
			To:   funcID,
			Kind: EdgeContains,
		})
	}

	// Process types
	for _, t := range analysis.Types {
		typeID := GenerateNodeID(analysis.Path, t.Name)
		typeNode := &Node{
			ID:       typeID,
			Kind:     KindType,
			Name:     t.Name,
			Path:     analysis.Path,
			Line:     t.Line,
			Exported: t.IsExported,
		}
		b.graph.AddNode(typeNode)

		// File contains type
		b.graph.AddEdge(&Edge{
			From: fileID,
			To:   typeID,
			Kind: EdgeContains,
		})
	}

	// Process imports
	for _, imp := range analysis.Imports {
		impID := GenerateNodeID(imp, "")
		impNode := &Node{
			ID:   impID,
			Kind: KindPackage,
			Name: filepath.Base(imp),
			Path: imp,
		}
		b.graph.AddNode(impNode)

		// File imports package
		b.graph.AddEdge(&Edge{
			From: fileID,
			To:   impID,
			Kind: EdgeImports,
		})
	}

	// Process calls (create edges between functions)
	for _, call := range analysis.Calls {
		// Find caller node
		callerID, ok := funcNodes[call.CallerFunc]
		if !ok {
			continue // Caller not found (might be global scope)
		}

		// Try to find callee node
		// First, look in the same file
		calleeID, ok := funcNodes[call.CalleeName]
		if !ok {
			// Create a placeholder node for the callee (will be resolved later)
			calleeID = GenerateNodeID("", call.CalleeName) // Global lookup
		}

		b.graph.AddEdge(&Edge{
			From:     callerID,
			To:       calleeID,
			Kind:     EdgeCalls,
			Line:     call.CallLine,
			CallSite: call.CalleeName,
			ArgCount: call.Args,
		})
	}

	return nil
}

// Build finalizes the graph and returns it.
func (b *Builder) Build() *CodeGraph {
	b.graph.LastIndexed = time.Now().Unix()
	b.graph.NodeCount = len(b.graph.Nodes)
	b.graph.EdgeCount = len(b.graph.Edges)

	b.progress(fmt.Sprintf("Built graph: %d nodes, %d edges from %d files",
		b.graph.NodeCount, b.graph.EdgeCount, b.fileCount))

	return b.graph
}

// ResolveCallEdges attempts to resolve placeholder callee nodes to actual nodes.
// Call this after all files have been added.
func (b *Builder) ResolveCallEdges() {
	// Build name lookup index
	nameToNodes := make(map[string][]*Node)
	for _, node := range b.graph.Nodes {
		if node.Kind == KindFunction || node.Kind == KindMethod {
			nameToNodes[node.Name] = append(nameToNodes[node.Name], node)
		}
	}

	// Update edges
	for _, edge := range b.graph.Edges {
		if edge.Kind != EdgeCalls {
			continue
		}

		// Check if callee exists
		if b.graph.GetNode(edge.To) != nil {
			continue
		}

		// Try to find by name
		candidates := nameToNodes[edge.CallSite]
		if len(candidates) == 1 {
			edge.To = candidates[0].ID
		} else if len(candidates) > 1 {
			// Ambiguous: use heuristics (same package preferred)
			callerNode := b.graph.GetNode(edge.From)
			if callerNode != nil {
				for _, c := range candidates {
					if c.Path == callerNode.Path || getPackageFromPath(c.Path) == getPackageFromPath(callerNode.Path) {
						edge.To = c.ID
						break
					}
				}
			}
		}
	}
}

// kindFromFunc determines the NodeKind based on function info.
func kindFromFunc(fn FuncInfo) NodeKind {
	if fn.Receiver != "" {
		return KindMethod
	}
	return KindFunction
}

// getPackageFromPath extracts a package name from a file path.
func getPackageFromPath(path string) string {
	dir := filepath.Dir(path)
	if dir == "." || dir == "" {
		return ""
	}
	return filepath.Base(dir)
}

// FilterCallEdges removes call edges that violate import constraints.
// This implements the ImportGraphFilter from the plan.
func (b *Builder) FilterCallEdges() {
	// Build file -> imported packages map
	fileImports := make(map[NodeID]map[string]bool)

	for _, edge := range b.graph.Edges {
		if edge.Kind != EdgeImports {
			continue
		}
		fromNode := b.graph.GetNode(edge.From)
		toNode := b.graph.GetNode(edge.To)
		if fromNode == nil || toNode == nil {
			continue
		}

		if _, ok := fileImports[edge.From]; !ok {
			fileImports[edge.From] = make(map[string]bool)
		}
		fileImports[edge.From][toNode.Name] = true
	}

	// Filter call edges
	validEdges := make([]*Edge, 0, len(b.graph.Edges))
	for _, edge := range b.graph.Edges {
		if edge.Kind != EdgeCalls {
			validEdges = append(validEdges, edge)
			continue
		}

		// Get caller's file
		callerNode := b.graph.GetNode(edge.From)
		if callerNode == nil {
			continue
		}

		callerFileID := GenerateNodeID(callerNode.Path, "")

		// Get callee
		calleeNode := b.graph.GetNode(edge.To)
		if calleeNode == nil {
			continue // Unresolved callee
		}

		// Arity check: skip if argument count doesn't match parameter count
		// ParamCount -1 means variadic (always matches)
		// ParamCount 0 with ArgCount 0 is valid (no params)
		if calleeNode.ParamCount >= 0 && edge.ArgCount != calleeNode.ParamCount {
			continue // Arity mismatch
		}

		// Same file? Always valid
		if callerNode.Path == calleeNode.Path {
			validEdges = append(validEdges, edge)
			continue
		}

		// Check if callee's package is imported
		calleePackage := getPackageFromPath(calleeNode.Path)
		imports := fileImports[callerFileID]
		if imports != nil && imports[calleePackage] {
			validEdges = append(validEdges, edge)
			continue
		}

		// Also check for standard library and external calls
		// For now, keep edges where callee package looks like an import
		if strings.Contains(calleePackage, "/") {
			validEdges = append(validEdges, edge)
		}
	}

	b.graph.Edges = validEdges
	b.graph.RebuildIndexes()
}
</file>
<file path="graph/query.go">
package graph

import (
	"strings"
)

// QueryResult represents a search result with context.
type QueryResult struct {
	Node  *Node   `json:"node"`
	Score float64 `json:"score,omitempty"`
	Path  []*Node `json:"path,omitempty"` // For path queries
}

// PathResult represents a path between two nodes.
type PathResult struct {
	From   *Node   `json:"from"`
	To     *Node   `json:"to"`
	Path   []*Node `json:"path"`
	Edges  []*Edge `json:"edges"`
	Length int     `json:"length"`
}

// FindNodesByPattern searches for nodes matching a pattern in name or path.
func (g *CodeGraph) FindNodesByPattern(pattern string, kinds []NodeKind) []*Node {
	pattern = strings.ToLower(pattern)
	var results []*Node

	kindSet := make(map[NodeKind]bool)
	for _, k := range kinds {
		kindSet[k] = true
	}

	for _, node := range g.Nodes {
		// Filter by kind if specified
		if len(kinds) > 0 && !kindSet[node.Kind] {
			continue
		}

		// Match name or path
		nameLower := strings.ToLower(node.Name)
		pathLower := strings.ToLower(node.Path)

		if strings.Contains(nameLower, pattern) || strings.Contains(pathLower, pattern) {
			results = append(results, node)
		}
	}

	return results
}

// FindPath finds the shortest path between two nodes using BFS.
// Returns nil if no path exists.
func (g *CodeGraph) FindPath(fromID, toID NodeID, maxDepth int) *PathResult {
	if maxDepth <= 0 {
		maxDepth = 10 // Default max depth
	}

	from := g.GetNode(fromID)
	to := g.GetNode(toID)
	if from == nil || to == nil {
		return nil
	}

	// BFS with path tracking
	type queueItem struct {
		nodeID NodeID
		path   []NodeID
		edges  []*Edge
	}

	visited := make(map[NodeID]bool)
	queue := []queueItem{{nodeID: fromID, path: []NodeID{fromID}, edges: nil}}

	for len(queue) > 0 {
		current := queue[0]
		queue = queue[1:]

		if len(current.path) > maxDepth {
			continue
		}

		if current.nodeID == toID {
			// Build result
			path := make([]*Node, len(current.path))
			for i, id := range current.path {
				path[i] = g.GetNode(id)
			}
			return &PathResult{
				From:   from,
				To:     to,
				Path:   path,
				Edges:  current.edges,
				Length: len(path) - 1,
			}
		}

		if visited[current.nodeID] {
			continue
		}
		visited[current.nodeID] = true

		// Explore outgoing edges
		for _, edge := range g.GetOutgoingEdges(current.nodeID) {
			if !visited[edge.To] {
				newPath := make([]NodeID, len(current.path)+1)
				copy(newPath, current.path)
				newPath[len(current.path)] = edge.To

				newEdges := make([]*Edge, len(current.edges)+1)
				copy(newEdges, current.edges)
				newEdges[len(current.edges)] = edge

				queue = append(queue, queueItem{
					nodeID: edge.To,
					path:   newPath,
					edges:  newEdges,
				})
			}
		}
	}

	return nil // No path found
}

// FindAllPaths finds all paths between two nodes up to maxDepth.
func (g *CodeGraph) FindAllPaths(fromID, toID NodeID, maxDepth int) []*PathResult {
	if maxDepth <= 0 {
		maxDepth = 5
	}

	from := g.GetNode(fromID)
	to := g.GetNode(toID)
	if from == nil || to == nil {
		return nil
	}

	var results []*PathResult
	visited := make(map[NodeID]bool)

	var dfs func(current NodeID, path []NodeID, edges []*Edge)
	dfs = func(current NodeID, path []NodeID, edges []*Edge) {
		if len(path) > maxDepth {
			return
		}

		if current == toID {
			pathNodes := make([]*Node, len(path))
			for i, id := range path {
				pathNodes[i] = g.GetNode(id)
			}
			edgesCopy := make([]*Edge, len(edges))
			copy(edgesCopy, edges)

			results = append(results, &PathResult{
				From:   from,
				To:     to,
				Path:   pathNodes,
				Edges:  edgesCopy,
				Length: len(path) - 1,
			})
			return
		}

		visited[current] = true
		defer func() { visited[current] = false }()

		for _, edge := range g.GetOutgoingEdges(current) {
			if !visited[edge.To] {
				dfs(edge.To, append(path, edge.To), append(edges, edge))
			}
		}
	}

	dfs(fromID, []NodeID{fromID}, nil)
	return results
}

// GetDependencyTree returns all nodes reachable from a starting node.
func (g *CodeGraph) GetDependencyTree(startID NodeID, maxDepth int) map[int][]*Node {
	if maxDepth <= 0 {
		maxDepth = 5
	}

	levels := make(map[int][]*Node)
	visited := make(map[NodeID]bool)

	var bfs func(ids []NodeID, depth int)
	bfs = func(ids []NodeID, depth int) {
		if depth > maxDepth || len(ids) == 0 {
			return
		}

		var nextLevel []NodeID
		for _, id := range ids {
			if visited[id] {
				continue
			}
			visited[id] = true

			if node := g.GetNode(id); node != nil {
				levels[depth] = append(levels[depth], node)
			}

			for _, edge := range g.GetOutgoingEdges(id) {
				if !visited[edge.To] {
					nextLevel = append(nextLevel, edge.To)
				}
			}
		}

		if len(nextLevel) > 0 {
			bfs(nextLevel, depth+1)
		}
	}

	bfs([]NodeID{startID}, 0)
	return levels
}

// GetReverseTree returns all nodes that transitively depend on the starting node.
func (g *CodeGraph) GetReverseTree(startID NodeID, maxDepth int) map[int][]*Node {
	if maxDepth <= 0 {
		maxDepth = 5
	}

	levels := make(map[int][]*Node)
	visited := make(map[NodeID]bool)

	var bfs func(ids []NodeID, depth int)
	bfs = func(ids []NodeID, depth int) {
		if depth > maxDepth || len(ids) == 0 {
			return
		}

		var nextLevel []NodeID
		for _, id := range ids {
			if visited[id] {
				continue
			}
			visited[id] = true

			if node := g.GetNode(id); node != nil {
				levels[depth] = append(levels[depth], node)
			}

			for _, edge := range g.GetIncomingEdges(id) {
				if !visited[edge.From] {
					nextLevel = append(nextLevel, edge.From)
				}
			}
		}

		if len(nextLevel) > 0 {
			bfs(nextLevel, depth+1)
		}
	}

	bfs([]NodeID{startID}, 0)
	return levels
}

// Stats returns statistics about the graph.
type Stats struct {
	TotalNodes      int            `json:"total_nodes"`
	TotalEdges      int            `json:"total_edges"`
	NodesByKind     map[string]int `json:"nodes_by_kind"`
	EdgesByKind     map[string]int `json:"edges_by_kind"`
	FileCount       int            `json:"file_count"`
	FunctionCount   int            `json:"function_count"`
	AvgEdgesPerNode float64        `json:"avg_edges_per_node"`
}

// GetStats computes statistics about the graph.
func (g *CodeGraph) GetStats() *Stats {
	stats := &Stats{
		TotalNodes:  len(g.Nodes),
		TotalEdges:  len(g.Edges),
		NodesByKind: make(map[string]int),
		EdgesByKind: make(map[string]int),
	}

	for _, node := range g.Nodes {
		stats.NodesByKind[node.Kind.String()]++
		if node.Kind == KindFile {
			stats.FileCount++
		} else if node.Kind == KindFunction || node.Kind == KindMethod {
			stats.FunctionCount++
		}
	}

	for _, edge := range g.Edges {
		stats.EdgesByKind[edge.Kind.String()]++
	}

	if stats.TotalNodes > 0 {
		stats.AvgEdgesPerNode = float64(stats.TotalEdges) / float64(stats.TotalNodes)
	}

	return stats
}
</file>
<file path="graph/store.go">
package graph

import (
	"compress/gzip"
	"encoding/gob"
	"fmt"
	"os"
	"path/filepath"
	"time"
)

const (
	// DefaultGraphDir is the directory name for codemap data
	DefaultGraphDir = ".codemap"
	// DefaultGraphFile is the default graph file name
	DefaultGraphFile = "graph.gob"
)

// GraphPath returns the default graph file path for a project root.
func GraphPath(rootPath string) string {
	return filepath.Join(rootPath, DefaultGraphDir, DefaultGraphFile)
}

// EnsureDir creates the .codemap directory if it doesn't exist.
func EnsureDir(rootPath string) error {
	dir := filepath.Join(rootPath, DefaultGraphDir)
	return os.MkdirAll(dir, 0755)
}

// SaveBinary writes the graph to disk using gob encoding with gzip compression.
func (g *CodeGraph) SaveBinary(path string) error {
	// Ensure parent directory exists
	if err := os.MkdirAll(filepath.Dir(path), 0755); err != nil {
		return fmt.Errorf("create directory: %w", err)
	}

	// Update metadata
	g.LastIndexed = time.Now().Unix()
	g.NodeCount = len(g.Nodes)
	g.EdgeCount = len(g.Edges)

	// Create file
	f, err := os.Create(path)
	if err != nil {
		return fmt.Errorf("create file: %w", err)
	}
	defer f.Close()

	// Wrap with gzip for compression
	gz := gzip.NewWriter(f)
	defer gz.Close()

	// Encode with gob
	enc := gob.NewEncoder(gz)
	if err := enc.Encode(g); err != nil {
		return fmt.Errorf("encode graph: %w", err)
	}

	return nil
}

// LoadBinary reads a graph from disk and rebuilds indexes.
func LoadBinary(path string) (*CodeGraph, error) {
	f, err := os.Open(path)
	if err != nil {
		return nil, fmt.Errorf("open file: %w", err)
	}
	defer f.Close()

	// Unwrap gzip
	gz, err := gzip.NewReader(f)
	if err != nil {
		return nil, fmt.Errorf("gzip reader: %w", err)
	}
	defer gz.Close()

	// Decode with gob
	var g CodeGraph
	dec := gob.NewDecoder(gz)
	if err := dec.Decode(&g); err != nil {
		return nil, fmt.Errorf("decode graph: %w", err)
	}

	// Rebuild in-memory indexes
	g.RebuildIndexes()

	return &g, nil
}

// Exists checks if a graph file exists at the given path.
func Exists(path string) bool {
	_, err := os.Stat(path)
	return err == nil
}

// IsStale checks if the graph needs to be rebuilt.
// Returns true if any source file has been modified after the graph was indexed.
func IsStale(g *CodeGraph, rootPath string) (bool, error) {
	if g == nil || g.LastIndexed == 0 {
		return true, nil
	}

	indexTime := time.Unix(g.LastIndexed, 0)

	// Check each file in the graph
	for path := range g.nodesByPath {
		fullPath := filepath.Join(rootPath, path)
		info, err := os.Stat(fullPath)
		if err != nil {
			if os.IsNotExist(err) {
				// File was deleted - graph is stale
				return true, nil
			}
			continue // Skip files we can't stat
		}
		if info.ModTime().After(indexTime) {
			return true, nil
		}
	}

	return false, nil
}

// GetModifiedFiles returns a list of files that have been modified since the graph was indexed.
// Only considers paths that look like actual source files (contain a file extension).
func GetModifiedFiles(g *CodeGraph, rootPath string) ([]string, error) {
	if g == nil || g.LastIndexed == 0 {
		return nil, nil
	}

	indexTime := time.Unix(g.LastIndexed, 0)
	var modified []string

	for path := range g.nodesByPath {
		// Skip package paths (no extension) - only check actual file paths
		ext := filepath.Ext(path)
		if ext == "" {
			continue
		}
		fullPath := filepath.Join(rootPath, path)
		info, err := os.Stat(fullPath)
		if err != nil {
			if os.IsNotExist(err) {
				modified = append(modified, path)
			}
			continue
		}
		if info.ModTime().After(indexTime) {
			modified = append(modified, path)
		}
	}

	return modified, nil
}

// GetDeletedFiles returns files that were in the graph but no longer exist.
// Only considers paths that look like actual source files (contain a file extension).
func GetDeletedFiles(g *CodeGraph, rootPath string) []string {
	if g == nil {
		return nil
	}

	var deleted []string
	for path := range g.nodesByPath {
		// Skip package paths (no extension) - only check actual file paths
		ext := filepath.Ext(path)
		if ext == "" {
			continue
		}
		fullPath := filepath.Join(rootPath, path)
		if _, err := os.Stat(fullPath); os.IsNotExist(err) {
			deleted = append(deleted, path)
		}
	}

	return deleted
}

// IsFileInGraph checks if a file path is already in the graph.
func (g *CodeGraph) IsFileInGraph(path string) bool {
	_, exists := g.nodesByPath[path]
	return exists
}

func init() {
	// Register types for gob encoding
	gob.Register(&CodeGraph{})
	gob.Register(&Node{})
	gob.Register(&Edge{})
}
</file>
<file path="graph/types.go">
// Package graph provides the knowledge graph data structures for codemap.
// It represents code entities (functions, types, files) and their relationships
// (calls, imports, contains) in a queryable graph structure.
package graph

import (
	"crypto/sha256"
	"encoding/hex"
	"fmt"
)

// NodeKind represents the type of a code entity in the graph.
type NodeKind int

const (
	KindFile NodeKind = iota
	KindPackage
	KindFunction
	KindMethod
	KindType
	KindVariable
	KindConstant
)

func (k NodeKind) String() string {
	switch k {
	case KindFile:
		return "file"
	case KindPackage:
		return "package"
	case KindFunction:
		return "function"
	case KindMethod:
		return "method"
	case KindType:
		return "type"
	case KindVariable:
		return "variable"
	case KindConstant:
		return "constant"
	default:
		return "unknown"
	}
}

// EdgeKind represents the type of relationship between nodes.
type EdgeKind int

const (
	EdgeImports EdgeKind = iota
	EdgeCalls
	EdgeDefines
	EdgeContains
	EdgeReferences
	EdgeImplements
	EdgeExtends
)

func (e EdgeKind) String() string {
	switch e {
	case EdgeImports:
		return "imports"
	case EdgeCalls:
		return "calls"
	case EdgeDefines:
		return "defines"
	case EdgeContains:
		return "contains"
	case EdgeReferences:
		return "references"
	case EdgeImplements:
		return "implements"
	case EdgeExtends:
		return "extends"
	default:
		return "unknown"
	}
}

// NodeID is a unique identifier for a node in the graph.
// Generated deterministically from path + symbol name.
type NodeID string

// GenerateNodeID creates a deterministic NodeID from path and symbol.
func GenerateNodeID(path, symbol string) NodeID {
	data := fmt.Sprintf("%s:%s", path, symbol)
	hash := sha256.Sum256([]byte(data))
	return NodeID(hex.EncodeToString(hash[:16])) // Use first 16 bytes (32 hex chars)
}

// Node represents a code entity in the knowledge graph.
type Node struct {
	ID         NodeID   `json:"id"`
	Kind       NodeKind `json:"kind"`
	Name       string   `json:"name"`
	Path       string   `json:"path"`                  // File path relative to project root
	Line       int      `json:"line,omitempty"`        // Line number (1-indexed)
	EndLine    int      `json:"end_line,omitempty"`    // End line number
	Signature  string   `json:"signature,omitempty"`   // Function/method signature
	DocString  string   `json:"doc,omitempty"`         // Documentation comment
	Exported   bool     `json:"exported,omitempty"`    // Is publicly visible
	Package    string   `json:"package,omitempty"`     // Package/module name
	ParamCount int      `json:"param_count,omitempty"` // For functions: parameter count (-1 = variadic)
}

// Edge represents a relationship between two nodes.
type Edge struct {
	From     NodeID   `json:"from"`
	To       NodeID   `json:"to"`
	Kind     EdgeKind `json:"kind"`
	Line     int      `json:"line,omitempty"`      // Line where the reference occurs
	Weight   float64  `json:"weight,omitempty"`    // Relationship strength (0-1)
	CallSite string   `json:"callsite,omitempty"`  // For calls: the call expression text
	ArgCount int      `json:"arg_count,omitempty"` // For calls: number of arguments
}

// CodeGraph is the main knowledge graph structure with indexed lookups.
type CodeGraph struct {
	// Core storage
	Nodes map[NodeID]*Node `json:"nodes"`
	Edges []*Edge          `json:"edges"`

	// Indexes for fast lookup (rebuilt on load)
	nodesByPath map[string][]*Node // path -> nodes in that file
	nodesByName map[string][]*Node // name -> nodes with that name
	edgesByFrom map[NodeID][]*Edge // from -> outgoing edges
	edgesByTo   map[NodeID][]*Edge // to -> incoming edges

	// Metadata
	RootPath    string `json:"root"`
	Version     int    `json:"version"`
	NodeCount   int    `json:"node_count"`
	EdgeCount   int    `json:"edge_count"`
	LastIndexed int64  `json:"last_indexed"` // Unix timestamp
}

// NewCodeGraph creates an empty CodeGraph with initialized maps.
func NewCodeGraph(rootPath string) *CodeGraph {
	return &CodeGraph{
		Nodes:       make(map[NodeID]*Node),
		Edges:       make([]*Edge, 0),
		nodesByPath: make(map[string][]*Node),
		nodesByName: make(map[string][]*Node),
		edgesByFrom: make(map[NodeID][]*Edge),
		edgesByTo:   make(map[NodeID][]*Edge),
		RootPath:    rootPath,
		Version:     1,
	}
}

// AddNode adds a node to the graph and updates indexes.
func (g *CodeGraph) AddNode(n *Node) {
	if _, exists := g.Nodes[n.ID]; exists {
		return // Already exists
	}
	g.Nodes[n.ID] = n
	g.NodeCount++

	// Update indexes
	g.nodesByPath[n.Path] = append(g.nodesByPath[n.Path], n)
	g.nodesByName[n.Name] = append(g.nodesByName[n.Name], n)
}

// AddEdge adds an edge to the graph and updates indexes.
func (g *CodeGraph) AddEdge(e *Edge) {
	g.Edges = append(g.Edges, e)
	g.EdgeCount++

	// Update indexes
	g.edgesByFrom[e.From] = append(g.edgesByFrom[e.From], e)
	g.edgesByTo[e.To] = append(g.edgesByTo[e.To], e)
}

// GetNode retrieves a node by ID.
func (g *CodeGraph) GetNode(id NodeID) *Node {
	return g.Nodes[id]
}

// GetNodesByPath returns all nodes in a given file path.
func (g *CodeGraph) GetNodesByPath(path string) []*Node {
	return g.nodesByPath[path]
}

// GetNodesByName returns all nodes with a given name.
func (g *CodeGraph) GetNodesByName(name string) []*Node {
	return g.nodesByName[name]
}

// GetOutgoingEdges returns all edges originating from a node.
func (g *CodeGraph) GetOutgoingEdges(id NodeID) []*Edge {
	return g.edgesByFrom[id]
}

// GetIncomingEdges returns all edges pointing to a node.
func (g *CodeGraph) GetIncomingEdges(id NodeID) []*Edge {
	return g.edgesByTo[id]
}

// GetCallers returns all nodes that call the given node.
func (g *CodeGraph) GetCallers(id NodeID) []*Node {
	var callers []*Node
	for _, edge := range g.edgesByTo[id] {
		if edge.Kind == EdgeCalls {
			if caller := g.Nodes[edge.From]; caller != nil {
				callers = append(callers, caller)
			}
		}
	}
	return callers
}

// GetCallees returns all nodes that the given node calls.
func (g *CodeGraph) GetCallees(id NodeID) []*Node {
	var callees []*Node
	for _, edge := range g.edgesByFrom[id] {
		if edge.Kind == EdgeCalls {
			if callee := g.Nodes[edge.To]; callee != nil {
				callees = append(callees, callee)
			}
		}
	}
	return callees
}

// RebuildIndexes rebuilds the in-memory indexes from Nodes and Edges.
// Call this after loading from disk.
func (g *CodeGraph) RebuildIndexes() {
	g.nodesByPath = make(map[string][]*Node)
	g.nodesByName = make(map[string][]*Node)
	g.edgesByFrom = make(map[NodeID][]*Edge)
	g.edgesByTo = make(map[NodeID][]*Edge)

	for _, n := range g.Nodes {
		g.nodesByPath[n.Path] = append(g.nodesByPath[n.Path], n)
		g.nodesByName[n.Name] = append(g.nodesByName[n.Name], n)
	}

	for _, e := range g.Edges {
		g.edgesByFrom[e.From] = append(g.edgesByFrom[e.From], e)
		g.edgesByTo[e.To] = append(g.edgesByTo[e.To], e)
	}
}

// RemoveNodesForPath removes all nodes and edges associated with a file path.
// Used for incremental updates when a file changes.
func (g *CodeGraph) RemoveNodesForPath(path string) {
	// Collect node IDs to remove
	nodesToRemove := make(map[NodeID]bool)
	for _, node := range g.nodesByPath[path] {
		nodesToRemove[node.ID] = true
		delete(g.Nodes, node.ID)
		g.NodeCount--
	}

	// Remove from nodesByName index
	for name, nodes := range g.nodesByName {
		var filtered []*Node
		for _, n := range nodes {
			if !nodesToRemove[n.ID] {
				filtered = append(filtered, n)
			}
		}
		if len(filtered) > 0 {
			g.nodesByName[name] = filtered
		} else {
			delete(g.nodesByName, name)
		}
	}

	// Remove from nodesByPath
	delete(g.nodesByPath, path)

	// Remove edges involving these nodes
	var newEdges []*Edge
	for _, edge := range g.Edges {
		if !nodesToRemove[edge.From] && !nodesToRemove[edge.To] {
			newEdges = append(newEdges, edge)
		} else {
			g.EdgeCount--
		}
	}
	g.Edges = newEdges

	// Rebuild edge indexes
	g.edgesByFrom = make(map[NodeID][]*Edge)
	g.edgesByTo = make(map[NodeID][]*Edge)
	for _, e := range g.Edges {
		g.edgesByFrom[e.From] = append(g.edgesByFrom[e.From], e)
		g.edgesByTo[e.To] = append(g.edgesByTo[e.To], e)
	}
}
</file>
<file path="graph/vectors.go">
package graph

import (
	"compress/gzip"
	"encoding/gob"
	"fmt"
	"math"
	"os"
	"path/filepath"
	"sort"
	"sync"
)

// DefaultVectorFile is the default name for vector storage
const DefaultVectorFile = "vectors.gob"

// VectorEntry holds an embedding vector for a node
type VectorEntry struct {
	NodeID    NodeID
	Vector    []float64
	Text      string // The text that was embedded (for debugging/inspection)
	Dimension int    // Vector dimension for validation
}

// SearchResult represents a search result with similarity score
type SearchResult struct {
	NodeID     NodeID
	Score      float64 // Cosine similarity score (0-1, higher is better)
	Node       *Node   // Optional: populated if graph is available
	SourceText string  // The text that was embedded
}

// VectorIndex defines the interface for vector storage and search
type VectorIndex interface {
	// Add stores a vector for a node
	Add(nodeID NodeID, vector []float64, text string) error

	// Remove deletes a vector for a node
	Remove(nodeID NodeID) error

	// Search finds the top-k most similar vectors
	Search(query []float64, k int) ([]SearchResult, error)

	// Has checks if a node has a stored vector
	Has(nodeID NodeID) bool

	// Count returns the number of stored vectors
	Count() int

	// Dimension returns the expected vector dimension
	Dimension() int

	// Save persists the index to disk
	Save(path string) error

	// Clear removes all vectors
	Clear()
}

// InMemoryVectorIndex implements VectorIndex with in-memory storage
type InMemoryVectorIndex struct {
	mu        sync.RWMutex
	vectors   map[NodeID]*VectorEntry
	dimension int
}

// NewVectorIndex creates a new in-memory vector index
// dimension specifies the expected embedding dimension (e.g., 768 for many models)
func NewVectorIndex(dimension int) *InMemoryVectorIndex {
	return &InMemoryVectorIndex{
		vectors:   make(map[NodeID]*VectorEntry),
		dimension: dimension,
	}
}

// Add stores a vector for a node
func (idx *InMemoryVectorIndex) Add(nodeID NodeID, vector []float64, text string) error {
	if len(vector) == 0 {
		return fmt.Errorf("empty vector for node %s", nodeID)
	}

	// Set dimension from first vector if not set
	idx.mu.Lock()
	defer idx.mu.Unlock()

	if idx.dimension == 0 {
		idx.dimension = len(vector)
	} else if len(vector) != idx.dimension {
		return fmt.Errorf("vector dimension mismatch: expected %d, got %d", idx.dimension, len(vector))
	}

	idx.vectors[nodeID] = &VectorEntry{
		NodeID:    nodeID,
		Vector:    vector,
		Text:      text,
		Dimension: len(vector),
	}

	return nil
}

// Remove deletes a vector for a node
func (idx *InMemoryVectorIndex) Remove(nodeID NodeID) error {
	idx.mu.Lock()
	defer idx.mu.Unlock()

	if _, exists := idx.vectors[nodeID]; !exists {
		return fmt.Errorf("vector not found for node %s", nodeID)
	}

	delete(idx.vectors, nodeID)
	return nil
}

// Search finds the top-k most similar vectors using cosine similarity
func (idx *InMemoryVectorIndex) Search(query []float64, k int) ([]SearchResult, error) {
	idx.mu.RLock()
	defer idx.mu.RUnlock()

	if len(query) == 0 {
		return nil, fmt.Errorf("empty query vector")
	}

	if idx.dimension > 0 && len(query) != idx.dimension {
		return nil, fmt.Errorf("query dimension mismatch: expected %d, got %d", idx.dimension, len(query))
	}

	if len(idx.vectors) == 0 {
		return nil, nil
	}

	// Calculate cosine similarity for all vectors
	type scored struct {
		nodeID NodeID
		score  float64
		text   string
	}

	results := make([]scored, 0, len(idx.vectors))
	queryNorm := vectorNorm(query)

	for nodeID, entry := range idx.vectors {
		similarity := cosineSimilarity(query, entry.Vector, queryNorm)
		results = append(results, scored{
			nodeID: nodeID,
			score:  similarity,
			text:   entry.Text,
		})
	}

	// Sort by score descending
	sort.Slice(results, func(i, j int) bool {
		return results[i].score > results[j].score
	})

	// Take top-k
	if k > len(results) {
		k = len(results)
	}

	searchResults := make([]SearchResult, k)
	for i := 0; i < k; i++ {
		searchResults[i] = SearchResult{
			NodeID:     results[i].nodeID,
			Score:      results[i].score,
			SourceText: results[i].text,
		}
	}

	return searchResults, nil
}

// Has checks if a node has a stored vector
func (idx *InMemoryVectorIndex) Has(nodeID NodeID) bool {
	idx.mu.RLock()
	defer idx.mu.RUnlock()
	_, exists := idx.vectors[nodeID]
	return exists
}

// Count returns the number of stored vectors
func (idx *InMemoryVectorIndex) Count() int {
	idx.mu.RLock()
	defer idx.mu.RUnlock()
	return len(idx.vectors)
}

// Dimension returns the expected vector dimension
func (idx *InMemoryVectorIndex) Dimension() int {
	idx.mu.RLock()
	defer idx.mu.RUnlock()
	return idx.dimension
}

// Clear removes all vectors
func (idx *InMemoryVectorIndex) Clear() {
	idx.mu.Lock()
	defer idx.mu.Unlock()
	idx.vectors = make(map[NodeID]*VectorEntry)
}

// Save persists the index to disk using gob with gzip compression
func (idx *InMemoryVectorIndex) Save(path string) error {
	idx.mu.RLock()
	defer idx.mu.RUnlock()

	// Ensure directory exists
	dir := filepath.Dir(path)
	if err := os.MkdirAll(dir, 0755); err != nil {
		return fmt.Errorf("creating directory: %w", err)
	}

	file, err := os.Create(path)
	if err != nil {
		return fmt.Errorf("creating file: %w", err)
	}
	defer file.Close()

	// Use gzip compression
	gzWriter := gzip.NewWriter(file)
	defer gzWriter.Close()

	encoder := gob.NewEncoder(gzWriter)

	// Encode dimension first
	if err := encoder.Encode(idx.dimension); err != nil {
		return fmt.Errorf("encoding dimension: %w", err)
	}

	// Encode vector count
	count := len(idx.vectors)
	if err := encoder.Encode(count); err != nil {
		return fmt.Errorf("encoding count: %w", err)
	}

	// Encode each vector entry
	for _, entry := range idx.vectors {
		if err := encoder.Encode(entry); err != nil {
			return fmt.Errorf("encoding vector entry: %w", err)
		}
	}

	return nil
}

// LoadVectorIndex loads a vector index from disk
func LoadVectorIndex(path string) (*InMemoryVectorIndex, error) {
	file, err := os.Open(path)
	if err != nil {
		return nil, fmt.Errorf("opening file: %w", err)
	}
	defer file.Close()

	// Use gzip decompression
	gzReader, err := gzip.NewReader(file)
	if err != nil {
		return nil, fmt.Errorf("creating gzip reader: %w", err)
	}
	defer gzReader.Close()

	decoder := gob.NewDecoder(gzReader)

	// Decode dimension
	var dimension int
	if err := decoder.Decode(&dimension); err != nil {
		return nil, fmt.Errorf("decoding dimension: %w", err)
	}

	// Decode vector count
	var count int
	if err := decoder.Decode(&count); err != nil {
		return nil, fmt.Errorf("decoding count: %w", err)
	}

	// Create index
	idx := &InMemoryVectorIndex{
		vectors:   make(map[NodeID]*VectorEntry, count),
		dimension: dimension,
	}

	// Decode each vector entry
	for i := 0; i < count; i++ {
		var entry VectorEntry
		if err := decoder.Decode(&entry); err != nil {
			return nil, fmt.Errorf("decoding vector entry %d: %w", i, err)
		}
		idx.vectors[entry.NodeID] = &entry
	}

	return idx, nil
}

// VectorIndexExists checks if a vector index file exists
func VectorIndexExists(root string) bool {
	path := filepath.Join(root, DefaultGraphDir, DefaultVectorFile)
	_, err := os.Stat(path)
	return err == nil
}

// VectorIndexPath returns the default path for the vector index
func VectorIndexPath(root string) string {
	return filepath.Join(root, DefaultGraphDir, DefaultVectorFile)
}

// cosineSimilarity calculates cosine similarity between two vectors
// queryNorm is pre-computed for efficiency
func cosineSimilarity(a, b []float64, queryNorm float64) float64 {
	if len(a) != len(b) {
		return 0
	}

	var dotProduct float64
	var bNormSq float64

	for i := range a {
		dotProduct += a[i] * b[i]
		bNormSq += b[i] * b[i]
	}

	bNorm := math.Sqrt(bNormSq)

	if queryNorm == 0 || bNorm == 0 {
		return 0
	}

	return dotProduct / (queryNorm * bNorm)
}

// vectorNorm calculates the L2 norm of a vector
func vectorNorm(v []float64) float64 {
	var sum float64
	for _, x := range v {
		sum += x * x
	}
	return math.Sqrt(sum)
}
</file>
<file path="mcp/main.go">
// MCP Server for codemap - provides codebase analysis tools to LLMs
package main

import (
	"bytes"
	"context"
	"fmt"
	"log"
	"os"
	"path/filepath"
	"regexp"
	"strings"
	"time"

	"codemap/analyze"
	"codemap/cache"
	"codemap/config"
	"codemap/graph"
	"codemap/render"
	"codemap/scanner"

	"github.com/modelcontextprotocol/go-sdk/mcp"
)

// Input types for tools
type PathInput struct {
	Path string `json:"path" jsonschema:"Path to the project directory to analyze"`
}

type DepsInput struct {
	Path   string `json:"path" jsonschema:"Path to the project directory to analyze"`
	Detail int    `json:"detail,omitempty" jsonschema:"Detail level: 0=names only (default), 1=signatures, 2=full (with type fields)"`
	Mode   string `json:"mode,omitempty" jsonschema:"Output mode: deps (default) shows dependency flow, api shows API surface (exported functions/types)"`
}

type DiffInput struct {
	Path string `json:"path" jsonschema:"Path to the project directory to analyze"`
	Ref  string `json:"ref,omitempty" jsonschema:"Git branch/ref to compare against (default: main)"`
}

type FindInput struct {
	Path    string `json:"path" jsonschema:"Path to the project directory to search"`
	Pattern string `json:"pattern" jsonschema:"Filename pattern to search for (case-insensitive substring match)"`
}

type ImportersInput struct {
	Path string `json:"path" jsonschema:"Path to the project directory"`
	File string `json:"file" jsonschema:"Relative path to the file to check (e.g. src/utils.ts)"`
}

type ListProjectsInput struct {
	Path    string `json:"path" jsonschema:"Parent directory containing projects (e.g. /Users/name/Code or ~/Code)"`
	Pattern string `json:"pattern,omitempty" jsonschema:"Optional filter to match project names (case-insensitive substring)"`
}

type SymbolInput struct {
	Path string `json:"path" jsonschema:"Path to the project directory"`
	Name string `json:"name" jsonschema:"Symbol name to search (substring match, case-insensitive)"`
	Kind string `json:"kind,omitempty" jsonschema:"Filter by symbol type: function, type, or all (default: all)"`
	File string `json:"file,omitempty" jsonschema:"Filter to specific file path (substring match)"`
}

type TracePathInput struct {
	Path  string `json:"path" jsonschema:"Path to the project directory"`
	From  string `json:"from" jsonschema:"Source symbol name to trace from"`
	To    string `json:"to" jsonschema:"Target symbol name to trace to"`
	Depth int    `json:"depth,omitempty" jsonschema:"Maximum traversal depth (default: 5)"`
}

type CallersInput struct {
	Path   string `json:"path" jsonschema:"Path to the project directory"`
	Symbol string `json:"symbol" jsonschema:"Symbol name to find callers for"`
	Depth  int    `json:"depth,omitempty" jsonschema:"Depth of caller chain (default: 1, max: 5)"`
}

type CalleesInput struct {
	Path   string `json:"path" jsonschema:"Path to the project directory"`
	Symbol string `json:"symbol" jsonschema:"Symbol name to find callees for"`
	Depth  int    `json:"depth,omitempty" jsonschema:"Depth of callee chain (default: 1, max: 5)"`
}

type ExplainSymbolInput struct {
	Path    string `json:"path" jsonschema:"Path to the project directory"`
	Symbol  string `json:"symbol" jsonschema:"Symbol name to explain (function, type, method)"`
	Model   string `json:"model,omitempty" jsonschema:"LLM model to use (overrides config)"`
	NoCache bool   `json:"no_cache,omitempty" jsonschema:"Bypass cache for this request"`
}

type SummarizeModuleInput struct {
	Path    string `json:"path" jsonschema:"Path to the project directory"`
	Module  string `json:"module" jsonschema:"Module/directory path to summarize (relative to project)"`
	Model   string `json:"model,omitempty" jsonschema:"LLM model to use (overrides config)"`
	NoCache bool   `json:"no_cache,omitempty" jsonschema:"Bypass cache for this request"`
}

type SemanticSearchInput struct {
	Path   string `json:"path" jsonschema:"Path to the project directory"`
	Query  string `json:"query" jsonschema:"Natural language search query"`
	Limit  int    `json:"limit,omitempty" jsonschema:"Maximum number of results (default: 10)"`
	Expand bool   `json:"expand,omitempty" jsonschema:"Include callers/callees in results"`
}

func main() {
	server := mcp.NewServer(&mcp.Implementation{
		Name:    "codemap",
		Version: "2.3.0",
	}, nil)

	// Tool: get_structure - Get project tree view
	mcp.AddTool(server, &mcp.Tool{
		Name:        "get_structure",
		Description: "Get the project structure as a tree view. Shows files organized by directory with language detection, file sizes, and highlights the top 5 largest source files. Use this to understand how a codebase is organized.",
	}, handleGetStructure)

	// Tool: get_dependencies - Get dependency graph
	mcp.AddTool(server, &mcp.Tool{
		Name:        "get_dependencies",
		Description: "Get the dependency flow of a project. Shows external dependencies by language, internal import chains between files, hub files (most-imported), and function counts. Use detail=1 for function signatures, detail=2 for full type information.",
	}, handleGetDependencies)

	// Tool: get_diff - Get changed files with impact analysis
	mcp.AddTool(server, &mcp.Tool{
		Name:        "get_diff",
		Description: "Get files changed compared to a git branch, with line counts and impact analysis showing which changed files are imported by others. Use this to understand what work has been done and what might break.",
	}, handleGetDiff)

	// Tool: find_file - Find files by pattern
	mcp.AddTool(server, &mcp.Tool{
		Name:        "find_file",
		Description: "Find files in a project matching a name pattern. Returns file paths with their sizes and languages.",
	}, handleFindFile)

	// Tool: get_importers - Find what imports a file
	mcp.AddTool(server, &mcp.Tool{
		Name:        "get_importers",
		Description: "Find all files that import/depend on a specific file. Use this to understand the impact of changing a file.",
	}, handleGetImporters)

	// Tool: status - Verify MCP connection
	mcp.AddTool(server, &mcp.Tool{
		Name:        "status",
		Description: "Check codemap MCP server status. Returns version and confirms local filesystem access is available.",
	}, handleStatus)

	// Tool: list_projects - Discover projects in a directory
	mcp.AddTool(server, &mcp.Tool{
		Name:        "list_projects",
		Description: "List project directories under a parent path. Use this to discover projects when you only know the general location (e.g., ~/Code) but not the exact folder name. Optionally filter by pattern to find specific projects. Returns directory names with file counts and primary language.",
	}, handleListProjects)

	// Tool: get_symbol - Search for symbols by name
	mcp.AddTool(server, &mcp.Tool{
		Name:        "get_symbol",
		Description: "Search for functions and types by name. Returns matching symbols with file location (path:line). Use this to find specific code elements without browsing files. Supports filtering by kind (function/type) and file path.",
	}, handleGetSymbol)

	// Tool: trace_path - Find path between two symbols
	mcp.AddTool(server, &mcp.Tool{
		Name:        "trace_path",
		Description: "Find the connection path between two symbols in the call graph. Requires a pre-built index (run 'codemap --index' first). Returns the chain of calls connecting the source to the target.",
	}, handleTracePath)

	// Tool: get_callers - Find what calls a symbol
	mcp.AddTool(server, &mcp.Tool{
		Name:        "get_callers",
		Description: "Find all functions that call a specific symbol. Requires a pre-built index (run 'codemap --index' first). Returns the caller chain showing who calls the target symbol.",
	}, handleGetCallers)

	// Tool: get_callees - Find what a symbol calls
	mcp.AddTool(server, &mcp.Tool{
		Name:        "get_callees",
		Description: "Find all functions called by a specific symbol. Requires a pre-built index (run 'codemap --index' first). Returns the callee chain showing what the source symbol calls.",
	}, handleGetCallees)

	// Tool: explain_symbol - LLM-powered code explanation
	mcp.AddTool(server, &mcp.Tool{
		Name:        "explain_symbol",
		Description: "Explain a code symbol (function, type, method) using LLM. Requires a pre-built index (run 'codemap --index' first) and configured LLM provider. Returns a natural language explanation of what the code does.",
	}, handleExplainSymbol)

	// Tool: summarize_module - LLM-powered module summary
	mcp.AddTool(server, &mcp.Tool{
		Name:        "summarize_module",
		Description: "Summarize a module/directory using LLM. Reads source files and generates a high-level overview of the module's purpose, components, and dependencies. Requires a configured LLM provider.",
	}, handleSummarizeModule)

	// Tool: semantic_search - Hybrid semantic/graph search
	mcp.AddTool(server, &mcp.Tool{
		Name:        "semantic_search",
		Description: "Search the codebase using natural language. Combines semantic vector search with graph-based name matching using Reciprocal Rank Fusion. Requires index (run 'codemap --index') and optionally embeddings (run 'codemap --embed' for semantic search). Returns matching symbols with relevance scores.",
	}, handleSemanticSearch)

	// Run server on stdio
	if err := server.Run(context.Background(), &mcp.StdioTransport{}); err != nil {
		log.Printf("Server error: %v", err)
	}
}

// validatePath validates and returns the absolute path
func validatePath(path string) (string, error) {
	if path == "" {
		return "", fmt.Errorf("path is required")
	}

	// Expand ~ to home directory
	if strings.HasPrefix(path, "~/") {
		home := os.Getenv("HOME")
		path = filepath.Join(home, path[2:])
	}

	absPath, err := filepath.Abs(path)
	if err != nil {
		return "", fmt.Errorf("invalid path: %w", err)
	}

	if _, err := os.Stat(absPath); os.IsNotExist(err) {
		return "", fmt.Errorf("path does not exist: %s", absPath)
	}

	return absPath, nil
}

func textResult(text string) *mcp.CallToolResult {
	return &mcp.CallToolResult{
		Content: []mcp.Content{
			&mcp.TextContent{Text: text},
		},
	}
}

func errorResult(text string) *mcp.CallToolResult {
	return &mcp.CallToolResult{
		Content: []mcp.Content{
			&mcp.TextContent{Text: text},
		},
		IsError: true,
	}
}

func handleGetStructure(ctx context.Context, req *mcp.CallToolRequest, input PathInput) (*mcp.CallToolResult, any, error) {
	absRoot, err := validatePath(input.Path)
	if err != nil {
		return errorResult(err.Error()), nil, nil
	}

	gitignore := scanner.LoadGitignore(absRoot)
	files, err := scanner.ScanFiles(absRoot, gitignore)
	if err != nil {
		return errorResult("Scan error: " + err.Error()), nil, nil
	}

	project := scanner.Project{
		Root:  absRoot,
		Mode:  "tree",
		Files: files,
	}

	output := captureOutput(func() {
		render.Tree(project)
	})

	// If graph exists, append summary statistics
	if g, err := loadGraph(absRoot); err == nil {
		stats := g.GetStats()
		var sb strings.Builder
		sb.WriteString(output)
		sb.WriteString("\n───────────────────────────────────\n")
		sb.WriteString("Knowledge Graph (pre-indexed):\n")
		sb.WriteString(fmt.Sprintf("  Nodes: %d (functions: %d, types: %d)\n",
			stats.TotalNodes, stats.FunctionCount, stats.NodesByKind["type"]))
		sb.WriteString(fmt.Sprintf("  Edges: %d (calls: %d, imports: %d)\n",
			stats.TotalEdges, stats.EdgesByKind["calls"], stats.EdgesByKind["imports"]))
		sb.WriteString("\n  Use trace_path, get_callers, get_callees for call graph queries.\n")
		output = sb.String()
	}

	return textResult(output), nil, nil
}

func handleGetDependencies(ctx context.Context, req *mcp.CallToolRequest, input DepsInput) (*mcp.CallToolResult, any, error) {
	absRoot, err := validatePath(input.Path)
	if err != nil {
		return errorResult(err.Error()), nil, nil
	}

	gitignore := scanner.LoadGitignore(absRoot)
	loader := scanner.NewGrammarLoader()

	// Use the detail level from input (default 0 = names only)
	detailLevel := scanner.DetailLevel(input.Detail)
	analyses, err := scanner.ScanForDeps(absRoot, gitignore, loader, detailLevel)
	if err != nil {
		return errorResult("Scan error: " + err.Error()), nil, nil
	}

	depsProject := scanner.DepsProject{
		Root:         absRoot,
		Mode:         "deps",
		Files:        analyses,
		ExternalDeps: scanner.ReadExternalDeps(absRoot),
		DetailLevel:  input.Detail,
	}

	// Use API mode if requested
	var output string
	if input.Mode == "api" {
		output = captureOutput(func() {
			render.APIView(depsProject)
		})
	} else {
		output = captureOutput(func() {
			render.Depgraph(depsProject)
		})
	}

	return textResult(output), nil, nil
}

func handleGetDiff(ctx context.Context, req *mcp.CallToolRequest, input DiffInput) (*mcp.CallToolResult, any, error) {
	ref := input.Ref
	if ref == "" {
		ref = "main"
	}

	absRoot, err := validatePath(input.Path)
	if err != nil {
		return errorResult(err.Error()), nil, nil
	}

	diffInfo, err := scanner.GitDiffInfo(absRoot, ref)
	if err != nil {
		return errorResult("Git diff error: " + err.Error() + "\nMake sure '" + ref + "' is a valid branch/ref"), nil, nil
	}

	if len(diffInfo.Changed) == 0 {
		return textResult("No files changed vs " + ref), nil, nil
	}

	gitignore := scanner.LoadGitignore(absRoot)
	files, err := scanner.ScanFiles(absRoot, gitignore)
	if err != nil {
		return errorResult("Scan error: " + err.Error()), nil, nil
	}

	files = scanner.FilterToChangedWithInfo(files, diffInfo)
	impact := scanner.AnalyzeImpact(absRoot, files)

	project := scanner.Project{
		Root:    absRoot,
		Mode:    "tree",
		Files:   files,
		DiffRef: ref,
		Impact:  impact,
	}

	output := captureOutput(func() {
		render.Tree(project)
	})

	return textResult(output), nil, nil
}

func handleFindFile(ctx context.Context, req *mcp.CallToolRequest, input FindInput) (*mcp.CallToolResult, any, error) {
	absRoot, err := validatePath(input.Path)
	if err != nil {
		return errorResult(err.Error()), nil, nil
	}

	gitignore := scanner.LoadGitignore(absRoot)
	files, err := scanner.ScanFiles(absRoot, gitignore)
	if err != nil {
		return errorResult("Scan error: " + err.Error()), nil, nil
	}

	// Filter files matching pattern (case-insensitive)
	var matches []string
	pattern := strings.ToLower(input.Pattern)
	for _, f := range files {
		if strings.Contains(strings.ToLower(f.Path), pattern) {
			matches = append(matches, f.Path)
		}
	}

	if len(matches) == 0 {
		return textResult("No files found matching '" + input.Pattern + "'"), nil, nil
	}

	return textResult(fmt.Sprintf("Found %d files:\n%s", len(matches), strings.Join(matches, "\n"))), nil, nil
}

// EmptyInput for tools that don't need parameters
type EmptyInput struct{}

func handleStatus(ctx context.Context, req *mcp.CallToolRequest, input EmptyInput) (*mcp.CallToolResult, any, error) {
	cwd, _ := os.Getwd()
	home := os.Getenv("HOME")

	return textResult(fmt.Sprintf(`codemap MCP server v2.3.0
Status: connected
Local filesystem access: enabled
Working directory: %s
Home directory: %s

Available tools:
  list_projects      - Discover projects in a directory
  get_structure      - Project tree view
  get_dependencies   - Import/function analysis
  get_diff           - Changed files vs branch
  find_file          - Search by filename
  get_importers      - Find what imports a file
  get_symbol         - Search for functions/types by name
  trace_path         - Find call path between symbols (requires index)
  get_callers        - Find what calls a symbol (requires index)
  get_callees        - Find what a symbol calls (requires index)
  explain_symbol     - LLM-powered code explanation (requires index + LLM)
  summarize_module   - LLM-powered module summary (requires LLM)
  semantic_search    - Hybrid semantic/graph search (requires index)`, cwd, home)), nil, nil
}

func handleListProjects(ctx context.Context, req *mcp.CallToolRequest, input ListProjectsInput) (*mcp.CallToolResult, any, error) {
	absPath, err := validatePath(input.Path)
	if err != nil {
		return errorResult(err.Error()), nil, nil
	}

	entries, err := os.ReadDir(absPath)
	if err != nil {
		return errorResult("Cannot read directory: " + err.Error()), nil, nil
	}

	pattern := strings.ToLower(input.Pattern)
	var projects []string

	for _, entry := range entries {
		if !entry.IsDir() {
			continue
		}
		name := entry.Name()

		// Skip hidden directories and common non-project dirs
		if strings.HasPrefix(name, ".") {
			continue
		}

		// Filter by pattern if provided
		if pattern != "" && !strings.Contains(strings.ToLower(name), pattern) {
			continue
		}

		// Get project stats
		projectPath := filepath.Join(absPath, name)
		stats := getProjectStats(projectPath)

		projects = append(projects, fmt.Sprintf("%-30s %s", name+"/", stats))
	}

	if len(projects) == 0 {
		if pattern != "" {
			return textResult(fmt.Sprintf("No projects matching '%s' in %s", input.Pattern, absPath)), nil, nil
		}
		return textResult("No project directories found in " + absPath), nil, nil
	}

	header := fmt.Sprintf("Projects in %s", absPath)
	if pattern != "" {
		header = fmt.Sprintf("Projects matching '%s' in %s", input.Pattern, absPath)
	}

	return textResult(fmt.Sprintf("%s:\n\n%s", header, strings.Join(projects, "\n"))), nil, nil
}

// getProjectStats returns a brief summary of a project directory
// Uses the same scanner logic as the main codemap command (respects .gitignore)
func getProjectStats(path string) string {
	gitignore := scanner.LoadGitignore(path)
	files, err := scanner.ScanFiles(path, gitignore)
	if err != nil {
		return "(error scanning)"
	}

	// Count files by language
	langCounts := make(map[string]int)
	for _, f := range files {
		lang := scanner.DetectLanguage(f.Path)
		if lang != "" {
			langCounts[lang]++
		}
	}

	// Find primary language
	var primaryLang string
	var maxCount int
	for lang, count := range langCounts {
		if count > maxCount {
			maxCount = count
			primaryLang = lang
		}
	}

	// Check if it's a git repo
	isGit := ""
	if _, err := os.Stat(filepath.Join(path, ".git")); err == nil {
		isGit = " [git]"
	}

	if info, ok := scanner.LangDisplay[primaryLang]; ok {
		return fmt.Sprintf("(%d files, %s%s)", len(files), info.Full, isGit)
	}
	return fmt.Sprintf("(%d files%s)", len(files), isGit)
}

func handleGetImporters(ctx context.Context, req *mcp.CallToolRequest, input ImportersInput) (*mcp.CallToolResult, any, error) {
	absRoot, err := validatePath(input.Path)
	if err != nil {
		return errorResult(err.Error()), nil, nil
	}

	gitignore := scanner.LoadGitignore(absRoot)
	loader := scanner.NewGrammarLoader()

	// For importers, we only need basic info (imports)
	analyses, err := scanner.ScanForDeps(absRoot, gitignore, loader, scanner.DetailNone)
	if err != nil {
		return errorResult("Scan error: " + err.Error()), nil, nil
	}

	targetBase := filepath.Base(input.File)
	targetNoExt := strings.TrimSuffix(targetBase, filepath.Ext(targetBase))
	
	// Resolve target directory absolutely to compare with file paths
	targetDir := filepath.Join(absRoot, filepath.Dir(input.File))

	var importers []string
	for _, a := range analyses {
		// Skip files in the same directory only for Go (implicit package imports)
		// For other languages (TS, JS, Python), importing sibling files is common and significant
		if a.Language == "go" && filepath.Dir(a.Path) == targetDir {
			continue
		}
		for _, imp := range a.Imports {
			impBase := filepath.Base(imp)
			impNoExt := strings.TrimSuffix(impBase, filepath.Ext(impBase))
			// Match by filename, name without ext, full path, or package/directory
			if impBase == targetBase || impNoExt == targetNoExt ||
				strings.HasSuffix(imp, input.File) ||
				strings.HasSuffix(imp, targetDir) || imp == targetDir {
				importers = append(importers, a.Path)
				break
			}
		}
	}

	if len(importers) == 0 {
		return textResult("No files import '" + input.File + "'"), nil, nil
	}

	return textResult(fmt.Sprintf("%d files import '%s':\n%s", len(importers), input.File, strings.Join(importers, "\n"))), nil, nil
}

func handleGetSymbol(ctx context.Context, req *mcp.CallToolRequest, input SymbolInput) (*mcp.CallToolResult, any, error) {
	absRoot, err := validatePath(input.Path)
	if err != nil {
		return errorResult(err.Error()), nil, nil
	}

	gitignore := scanner.LoadGitignore(absRoot)
	loader := scanner.NewGrammarLoader()

	// Use signature detail level to get function signatures
	analyses, err := scanner.ScanForDeps(absRoot, gitignore, loader, scanner.DetailSignature)
	if err != nil {
		return errorResult("Scan error: " + err.Error()), nil, nil
	}

	// Build query
	query := scanner.SymbolQuery{
		Name: input.Name,
		Kind: input.Kind,
		File: input.File,
	}

	matches := scanner.SearchSymbols(analyses, query)

	if len(matches) == 0 {
		msg := fmt.Sprintf("No symbols found matching '%s'", input.Name)
		if input.Kind != "" && input.Kind != "all" {
			msg += fmt.Sprintf(" (kind: %s)", input.Kind)
		}
		if input.File != "" {
			msg += fmt.Sprintf(" in file '%s'", input.File)
		}
		return textResult(msg), nil, nil
	}

	// Format output
	var sb strings.Builder
	sb.WriteString(fmt.Sprintf("=== Symbol Search: \"%s\" ===\n", input.Name))
	sb.WriteString(fmt.Sprintf("Path: %s\n\n", absRoot))
	sb.WriteString(fmt.Sprintf("Found %d matches:\n\n", len(matches)))

	funcCount := 0
	typeCount := 0

	for _, m := range matches {
		if m.Kind == "function" {
			funcCount++
		} else {
			typeCount++
		}

		sb.WriteString(fmt.Sprintf("  %s:%d\n", m.File, m.Line))

		if m.Kind == "function" {
			if m.Signature != "" {
				sb.WriteString(fmt.Sprintf("  ├─ %s\n", m.Signature))
			} else {
				sb.WriteString(fmt.Sprintf("  ├─ func %s\n", m.Name))
			}
		} else {
			sb.WriteString(fmt.Sprintf("  ├─ %s %s\n", m.TypeKind, m.Name))
		}

		if m.Exported {
			sb.WriteString("  └─ exported\n")
		} else {
			sb.WriteString("  └─ private\n")
		}
		sb.WriteString("\n")
	}

	sb.WriteString("───────────────────────────────────\n")
	sb.WriteString(fmt.Sprintf("Matches: %d", len(matches)))
	if funcCount > 0 && typeCount > 0 {
		sb.WriteString(fmt.Sprintf(" (%d functions, %d types)", funcCount, typeCount))
	} else if funcCount > 0 {
		funcWord := "functions"
		if funcCount == 1 {
			funcWord = "function"
		}
		sb.WriteString(fmt.Sprintf(" (%d %s)", funcCount, funcWord))
	} else if typeCount > 0 {
		typeWord := "types"
		if typeCount == 1 {
			typeWord = "type"
		}
		sb.WriteString(fmt.Sprintf(" (%d %s)", typeCount, typeWord))
	}
	sb.WriteString("\n")

	return textResult(sb.String()), nil, nil
}

// ANSI escape code pattern
var ansiRegex = regexp.MustCompile(`\x1b\[[0-9;]*m`)

// stripANSI removes ANSI color codes from a string
func stripANSI(s string) string {
	return ansiRegex.ReplaceAllString(s, "")
}

// captureOutput captures stdout from a function and strips ANSI codes
func captureOutput(f func()) string {
	old := os.Stdout
	r, w, _ := os.Pipe()
	os.Stdout = w

	f()

	w.Close()
	os.Stdout = old

	var buf bytes.Buffer
	buf.ReadFrom(r)
	return stripANSI(buf.String())
}

// loadGraph loads the knowledge graph for a project, or returns an error if not indexed
func loadGraph(projectPath string) (*graph.CodeGraph, error) {
	graphPath := filepath.Join(projectPath, ".codemap", "graph.gob")
	if _, err := os.Stat(graphPath); os.IsNotExist(err) {
		return nil, fmt.Errorf("no index found. Run 'codemap --index %s' first", projectPath)
	}
	return graph.LoadBinary(graphPath)
}

func handleTracePath(ctx context.Context, req *mcp.CallToolRequest, input TracePathInput) (*mcp.CallToolResult, any, error) {
	absRoot, err := validatePath(input.Path)
	if err != nil {
		return errorResult(err.Error()), nil, nil
	}

	g, err := loadGraph(absRoot)
	if err != nil {
		return errorResult(err.Error()), nil, nil
	}

	// Find source nodes matching "from"
	fromNodes := g.FindNodesByPattern(input.From, []graph.NodeKind{graph.KindFunction, graph.KindMethod})
	if len(fromNodes) == 0 {
		return errorResult(fmt.Sprintf("No function/method found matching '%s'", input.From)), nil, nil
	}

	// Find target nodes matching "to"
	toNodes := g.FindNodesByPattern(input.To, []graph.NodeKind{graph.KindFunction, graph.KindMethod})
	if len(toNodes) == 0 {
		return errorResult(fmt.Sprintf("No function/method found matching '%s'", input.To)), nil, nil
	}

	depth := input.Depth
	if depth <= 0 {
		depth = 5
	}

	// Try to find path between any from/to combination
	var result *graph.PathResult
	for _, from := range fromNodes {
		for _, to := range toNodes {
			if from.ID == to.ID {
				continue
			}
			if path := g.FindPath(from.ID, to.ID, depth); path != nil {
				result = path
				break
			}
		}
		if result != nil {
			break
		}
	}

	if result == nil {
		return textResult(fmt.Sprintf("No path found between '%s' and '%s' within depth %d", input.From, input.To, depth)), nil, nil
	}

	// Format output
	var sb strings.Builder
	sb.WriteString(fmt.Sprintf("=== Path: %s → %s ===\n", input.From, input.To))
	sb.WriteString(fmt.Sprintf("Length: %d hops\n\n", result.Length))

	for i, node := range result.Path {
		prefix := "├─"
		if i == len(result.Path)-1 {
			prefix = "└─"
		}
		sb.WriteString(fmt.Sprintf("%s %s (%s:%d)\n", prefix, node.Name, node.Path, node.Line))
		if i < len(result.Edges) {
			edge := result.Edges[i]
			sb.WriteString(fmt.Sprintf("   │ %s\n", edge.Kind.String()))
		}
	}

	return textResult(sb.String()), nil, nil
}

func handleGetCallers(ctx context.Context, req *mcp.CallToolRequest, input CallersInput) (*mcp.CallToolResult, any, error) {
	absRoot, err := validatePath(input.Path)
	if err != nil {
		return errorResult(err.Error()), nil, nil
	}

	g, err := loadGraph(absRoot)
	if err != nil {
		return errorResult(err.Error()), nil, nil
	}

	// Find nodes matching the symbol
	nodes := g.FindNodesByPattern(input.Symbol, []graph.NodeKind{graph.KindFunction, graph.KindMethod})
	if len(nodes) == 0 {
		return errorResult(fmt.Sprintf("No function/method found matching '%s'", input.Symbol)), nil, nil
	}

	depth := input.Depth
	if depth <= 0 {
		depth = 1
	}
	if depth > 5 {
		depth = 5
	}

	var sb strings.Builder
	sb.WriteString(fmt.Sprintf("=== Callers of '%s' ===\n\n", input.Symbol))

	totalCallers := 0
	for _, node := range nodes {
		callerTree := g.GetReverseTree(node.ID, depth)
		if len(callerTree) <= 1 { // Only has the node itself
			continue
		}

		sb.WriteString(fmt.Sprintf("Target: %s (%s:%d)\n", node.Name, node.Path, node.Line))

		for level := 1; level <= depth; level++ {
			callers := callerTree[level]
			if len(callers) == 0 {
				continue
			}
			for _, caller := range callers {
				indent := strings.Repeat("  ", level-1)
				sb.WriteString(fmt.Sprintf("%s├─ %s (%s:%d)\n", indent, caller.Name, caller.Path, caller.Line))
				totalCallers++
			}
		}
		sb.WriteString("\n")
	}

	if totalCallers == 0 {
		return textResult(fmt.Sprintf("No callers found for '%s'", input.Symbol)), nil, nil
	}

	sb.WriteString(fmt.Sprintf("───────────────────────────────────\nTotal callers: %d\n", totalCallers))
	return textResult(sb.String()), nil, nil
}

func handleGetCallees(ctx context.Context, req *mcp.CallToolRequest, input CalleesInput) (*mcp.CallToolResult, any, error) {
	absRoot, err := validatePath(input.Path)
	if err != nil {
		return errorResult(err.Error()), nil, nil
	}

	g, err := loadGraph(absRoot)
	if err != nil {
		return errorResult(err.Error()), nil, nil
	}

	// Find nodes matching the symbol
	nodes := g.FindNodesByPattern(input.Symbol, []graph.NodeKind{graph.KindFunction, graph.KindMethod})
	if len(nodes) == 0 {
		return errorResult(fmt.Sprintf("No function/method found matching '%s'", input.Symbol)), nil, nil
	}

	depth := input.Depth
	if depth <= 0 {
		depth = 1
	}
	if depth > 5 {
		depth = 5
	}

	var sb strings.Builder
	sb.WriteString(fmt.Sprintf("=== Callees of '%s' ===\n\n", input.Symbol))

	totalCallees := 0
	for _, node := range nodes {
		calleeTree := g.GetDependencyTree(node.ID, depth)
		if len(calleeTree) <= 1 { // Only has the node itself
			continue
		}

		sb.WriteString(fmt.Sprintf("Source: %s (%s:%d)\n", node.Name, node.Path, node.Line))

		for level := 1; level <= depth; level++ {
			callees := calleeTree[level]
			if len(callees) == 0 {
				continue
			}
			for _, callee := range callees {
				indent := strings.Repeat("  ", level-1)
				sb.WriteString(fmt.Sprintf("%s├─ %s (%s:%d)\n", indent, callee.Name, callee.Path, callee.Line))
				totalCallees++
			}
		}
		sb.WriteString("\n")
	}

	if totalCallees == 0 {
		return textResult(fmt.Sprintf("No callees found for '%s'", input.Symbol)), nil, nil
	}

	sb.WriteString(fmt.Sprintf("───────────────────────────────────\nTotal callees: %d\n", totalCallees))
	return textResult(sb.String()), nil, nil
}

func handleExplainSymbol(ctx context.Context, req *mcp.CallToolRequest, input ExplainSymbolInput) (*mcp.CallToolResult, any, error) {
	absRoot, err := validatePath(input.Path)
	if err != nil {
		return errorResult(err.Error()), nil, nil
	}

	if input.Symbol == "" {
		return errorResult("symbol is required"), nil, nil
	}

	// Load graph to find symbol
	g, err := loadGraph(absRoot)
	if err != nil {
		return errorResult(err.Error()), nil, nil
	}

	// Find matching nodes
	nodes := g.FindNodesByPattern(input.Symbol, nil)
	if len(nodes) == 0 {
		return errorResult(fmt.Sprintf("No symbols found matching '%s'", input.Symbol)), nil, nil
	}

	// Use first match
	node := nodes[0]

	// Load configuration
	cfg, err := config.Load()
	if err != nil {
		cfg = config.DefaultConfig()
	}

	if input.Model != "" {
		cfg.LLM.Model = input.Model
	}

	// Create LLM client
	client, err := analyze.NewClient(cfg)
	if err != nil {
		return errorResult(fmt.Sprintf("Error creating LLM client: %v. Check your configuration.", err)), nil, nil
	}

	// Check connectivity
	pingCtx, cancel := context.WithTimeout(ctx, 10*time.Second)
	defer cancel()

	if err := client.Ping(pingCtx); err != nil {
		return errorResult(fmt.Sprintf("Error connecting to LLM provider (%s): %v", client.Name(), err)), nil, nil
	}

	// Read source code
	source, err := analyze.ReadSymbolSource(absRoot, node)
	if err != nil {
		return errorResult(fmt.Sprintf("Error reading source: %v", err)), nil, nil
	}

	// Initialize cache
	cacheOpts := cache.Options{
		Dir:     filepath.Join(absRoot, ".codemap", "cache"),
		Enabled: !input.NoCache && cfg.Cache.Enabled,
	}
	responseCache, _ := cache.New(cacheOpts)

	// Check cache
	operation := "explain"
	if responseCache != nil && responseCache.Enabled() {
		if entry, ok := responseCache.GetByContentHash(source.ContentHash, operation, cfg.LLM.Model); ok {
			var sb strings.Builder
			sb.WriteString(fmt.Sprintf("=== %s ===\n", node.Name))
			sb.WriteString(fmt.Sprintf("Path: %s:%d\n", node.Path, node.Line))
			sb.WriteString(fmt.Sprintf("Kind: %s\n", node.Kind.String()))
			sb.WriteString("(cached)\n\n")
			sb.WriteString(entry.Response)
			return textResult(sb.String()), nil, nil
		}
	}

	// Generate prompt
	messages := analyze.ExplainSymbolPrompt(source)

	// Make request
	reqCtx, reqCancel := context.WithTimeout(ctx, time.Duration(cfg.LLM.Timeout)*time.Second)
	defer reqCancel()

	resp, err := client.Complete(reqCtx, &analyze.CompletionRequest{
		Messages:    messages,
		Temperature: cfg.LLM.Temperature,
		MaxTokens:   cfg.LLM.MaxTokens,
	})
	if err != nil {
		return errorResult(fmt.Sprintf("Error from LLM: %v", err)), nil, nil
	}

	// Cache response
	if responseCache != nil && responseCache.Enabled() {
		responseCache.SetResponse(source.ContentHash, operation, resp.Model, resp.Content, &cache.TokenUsage{
			PromptTokens:     resp.Usage.PromptTokens,
			CompletionTokens: resp.Usage.CompletionTokens,
			TotalTokens:      resp.Usage.TotalTokens,
		})
	}

	// Format output
	var sb strings.Builder
	sb.WriteString(fmt.Sprintf("=== %s ===\n", node.Name))
	sb.WriteString(fmt.Sprintf("Path: %s:%d\n", node.Path, node.Line))
	sb.WriteString(fmt.Sprintf("Kind: %s\n", node.Kind.String()))
	sb.WriteString(fmt.Sprintf("Model: %s | Tokens: %d | Time: %v\n\n", resp.Model, resp.Usage.TotalTokens, resp.Duration.Round(time.Millisecond)))
	sb.WriteString(resp.Content)

	return textResult(sb.String()), nil, nil
}

func handleSummarizeModule(ctx context.Context, req *mcp.CallToolRequest, input SummarizeModuleInput) (*mcp.CallToolResult, any, error) {
	absRoot, err := validatePath(input.Path)
	if err != nil {
		return errorResult(err.Error()), nil, nil
	}

	// Default to project root if no module specified
	modulePath := input.Module
	if modulePath == "" {
		modulePath = "."
	}

	// Resolve full path
	fullPath := filepath.Join(absRoot, modulePath)
	info, err := os.Stat(fullPath)
	if err != nil {
		return errorResult(fmt.Sprintf("Path not found: %s", modulePath)), nil, nil
	}

	// Load configuration
	cfg, err := config.Load()
	if err != nil {
		cfg = config.DefaultConfig()
	}

	if input.Model != "" {
		cfg.LLM.Model = input.Model
	}

	// Create LLM client
	client, err := analyze.NewClient(cfg)
	if err != nil {
		return errorResult(fmt.Sprintf("Error creating LLM client: %v. Check your configuration.", err)), nil, nil
	}

	// Check connectivity
	pingCtx, cancel := context.WithTimeout(ctx, 10*time.Second)
	defer cancel()

	if err := client.Ping(pingCtx); err != nil {
		return errorResult(fmt.Sprintf("Error connecting to LLM provider (%s): %v", client.Name(), err)), nil, nil
	}

	// Read source files
	var sources []*analyze.SymbolSource

	if info.IsDir() {
		sources, err = analyze.ReadModuleSource(absRoot, modulePath)
		if err != nil {
			return errorResult(fmt.Sprintf("Error reading module: %v", err)), nil, nil
		}
	} else {
		// Single file
		source, err := analyze.ReadSymbolSource(absRoot, &graph.Node{
			Kind: graph.KindFile,
			Name: filepath.Base(modulePath),
			Path: modulePath,
		})
		if err != nil {
			return errorResult(fmt.Sprintf("Error reading file: %v", err)), nil, nil
		}
		sources = []*analyze.SymbolSource{source}
	}

	if len(sources) == 0 {
		return errorResult("No source files found in the specified path"), nil, nil
	}

	// Compute combined content hash for caching
	var hashes []string
	for _, s := range sources {
		hashes = append(hashes, s.ContentHash)
	}
	combinedHash := analyze.ContentHash(fmt.Sprintf("%v", hashes))

	// Initialize cache
	cacheOpts := cache.Options{
		Dir:     filepath.Join(absRoot, ".codemap", "cache"),
		Enabled: !input.NoCache && cfg.Cache.Enabled,
	}
	responseCache, _ := cache.New(cacheOpts)

	// Check cache
	operation := "summarize"
	if responseCache != nil && responseCache.Enabled() {
		if entry, ok := responseCache.GetByContentHash(combinedHash, operation, cfg.LLM.Model); ok {
			var sb strings.Builder
			sb.WriteString(fmt.Sprintf("=== %s ===\n", modulePath))
			sb.WriteString(fmt.Sprintf("Files: %d\n", len(sources)))
			sb.WriteString("(cached)\n\n")
			sb.WriteString(entry.Response)
			return textResult(sb.String()), nil, nil
		}
	}

	// Generate prompt
	messages := analyze.SummarizeModulePrompt(modulePath, sources)

	// Make request
	reqCtx, reqCancel := context.WithTimeout(ctx, time.Duration(cfg.LLM.Timeout)*time.Second)
	defer reqCancel()

	resp, err := client.Complete(reqCtx, &analyze.CompletionRequest{
		Messages:    messages,
		Temperature: cfg.LLM.Temperature,
		MaxTokens:   cfg.LLM.MaxTokens,
	})
	if err != nil {
		return errorResult(fmt.Sprintf("Error from LLM: %v", err)), nil, nil
	}

	// Cache response
	if responseCache != nil && responseCache.Enabled() {
		responseCache.SetResponse(combinedHash, operation, resp.Model, resp.Content, &cache.TokenUsage{
			PromptTokens:     resp.Usage.PromptTokens,
			CompletionTokens: resp.Usage.CompletionTokens,
			TotalTokens:      resp.Usage.TotalTokens,
		})
	}

	// Collect file names
	var fileNames []string
	for _, s := range sources {
		fileNames = append(fileNames, s.Node.Name)
	}

	// Format output
	var sb strings.Builder
	sb.WriteString(fmt.Sprintf("=== %s ===\n", modulePath))
	sb.WriteString(fmt.Sprintf("Files: %s\n", strings.Join(fileNames, ", ")))
	sb.WriteString(fmt.Sprintf("Model: %s | Tokens: %d | Time: %v\n\n", resp.Model, resp.Usage.TotalTokens, resp.Duration.Round(time.Millisecond)))
	sb.WriteString(resp.Content)

	return textResult(sb.String()), nil, nil
}

func handleSemanticSearch(ctx context.Context, req *mcp.CallToolRequest, input SemanticSearchInput) (*mcp.CallToolResult, any, error) {
	absPath, err := validatePath(input.Path)
	if err != nil {
		return errorResult(err.Error()), nil, nil
	}

	if input.Query == "" {
		return errorResult("query is required"), nil, nil
	}

	// Load graph
	codeGraph, err := loadGraph(absPath)
	if err != nil {
		return errorResult(err.Error()), nil, nil
	}

	// Load vector index if available
	var vectorIndex *graph.InMemoryVectorIndex
	vectorPath := graph.VectorIndexPath(absPath)
	if graph.VectorIndexExists(absPath) {
		vectorIndex, err = graph.LoadVectorIndex(vectorPath)
		if err != nil {
			// Log but continue - will fall back to graph search
			vectorIndex = nil
		}
	}

	// Load configuration
	cfg, err := config.Load()
	if err != nil {
		cfg = config.DefaultConfig()
	}

	// Create LLM client (needed for query embedding)
	var client analyze.LLMClient
	if vectorIndex != nil && vectorIndex.Count() > 0 {
		client, _ = analyze.NewClient(cfg)
	}

	// Create retriever
	retriever := analyze.NewRetriever(codeGraph, vectorIndex, client, absPath)

	// Configure search
	searchConfig := analyze.DefaultSearchConfig()
	if input.Limit > 0 {
		searchConfig.Limit = input.Limit
	}
	searchConfig.ExpandContext = input.Expand

	// Determine search mode
	if client == nil || vectorIndex == nil || vectorIndex.Count() == 0 {
		searchConfig.Mode = analyze.SearchModeGraph
	}

	// Perform search
	results, err := retriever.Search(ctx, input.Query, searchConfig)
	if err != nil {
		return errorResult(fmt.Sprintf("Search error: %v", err)), nil, nil
	}

	if len(results) == 0 {
		return textResult(fmt.Sprintf("No results found for: %s", input.Query)), nil, nil
	}

	// Format output
	var sb strings.Builder
	sb.WriteString(fmt.Sprintf("Search: \"%s\"\n", input.Query))
	sb.WriteString(fmt.Sprintf("Results: %d\n\n", len(results)))

	for i, r := range results {
		sb.WriteString(fmt.Sprintf("%d. %s [%s] (score: %.3f)\n", i+1, r.Node.Name, r.Node.Kind, r.FinalScore))
		sb.WriteString(fmt.Sprintf("   %s:%d\n", r.Node.Path, r.Node.Line))
		if r.MatchReason != "" {
			sb.WriteString(fmt.Sprintf("   Match: %s\n", r.MatchReason))
		}
		if len(r.Callers) > 0 {
			sb.WriteString("   Callers: ")
			for j, c := range r.Callers {
				if j > 0 {
					sb.WriteString(", ")
				}
				sb.WriteString(c.Name)
			}
			sb.WriteString("\n")
		}
		if len(r.Callees) > 0 {
			sb.WriteString("   Callees: ")
			for j, c := range r.Callees {
				if j > 0 {
					sb.WriteString(", ")
				}
				sb.WriteString(c.Name)
			}
			sb.WriteString("\n")
		}
		if r.Snippet != "" {
			sb.WriteString(fmt.Sprintf("   ```\n%s\n   ```\n", r.Snippet))
		}
		sb.WriteString("\n")
	}

	return textResult(sb.String()), nil, nil
}
</file>
<file path="mcp/tools_test.go">
package main

import (
	"context"
	"os"
	"os/exec"
	"path/filepath"
	"strings"
	"testing"

	"github.com/modelcontextprotocol/go-sdk/mcp"
)

func TestMCPTools(t *testing.T) {
	// Setup path to test data
	cwd, err := os.Getwd()
	if err != nil {
		t.Fatalf("Failed to get cwd: %v", err)
	}
	
	// Assuming test is running in codemap/mcp
	// Test data is in codemap/scanner/testdata/corpus/go
	testDataPath := filepath.Join(filepath.Dir(cwd), "scanner", "testdata", "corpus", "go")
	if _, err := os.Stat(testDataPath); os.IsNotExist(err) {
		t.Fatalf("Test data path does not exist: %s", testDataPath)
	}

	ctx := context.Background()

	t.Run("get_structure", func(t *testing.T) {
		input := PathInput{Path: testDataPath}
		result, _, err := handleGetStructure(ctx, nil, input)
		if err != nil {
			t.Fatalf("handleGetStructure failed: %v", err)
		}
		if result.IsError {
			t.Fatalf("handleGetStructure returned error result: %v", result.Content)
		}
		text := result.Content[0].(*mcp.TextContent).Text
		if !strings.Contains(text, "main.go") {
			t.Errorf("Expected output to contain 'main.go', got:\n%s", text)
		}
	})

	t.Run("find_file", func(t *testing.T) {
		input := FindInput{Path: testDataPath, Pattern: "main"}
		result, _, err := handleFindFile(ctx, nil, input)
		if err != nil {
			t.Fatalf("handleFindFile failed: %v", err)
		}
		text := result.Content[0].(*mcp.TextContent).Text
		if !strings.Contains(text, "main.go") {
			t.Errorf("Expected output to contain 'main.go', got:\n%s", text)
		}
	})

	t.Run("get_dependencies", func(t *testing.T) {
		// Note: This requires grammars to be built. If not, it might return empty deps but shouldn't error.
		input := DepsInput{Path: testDataPath, Detail: 0}
		result, _, err := handleGetDependencies(ctx, nil, input)
		if err != nil {
			t.Fatalf("handleGetDependencies failed: %v", err)
		}
		text := result.Content[0].(*mcp.TextContent).Text
		// Check for known output. main.go imports fmt.
		// Output format depends on render.Depgraph.
		// It typically lists files or packages.
		if !strings.Contains(text, "fmt") && !strings.Contains(text, "main.go") {
			t.Logf("Warning: 'fmt' or 'main.go' not found in dependencies. Output:\n%s", text)
			// It might be empty if grammars are not loaded, but tool shouldn't crash.
		}
	})

	t.Run("get_symbol", func(t *testing.T) {
		input := SymbolInput{Path: testDataPath, Name: "main"}
		result, _, err := handleGetSymbol(ctx, nil, input)
		if err != nil {
			t.Fatalf("handleGetSymbol failed: %v", err)
		}
		text := result.Content[0].(*mcp.TextContent).Text
		if !strings.Contains(text, "main.go") {
			t.Errorf("Expected output to contain 'main.go', got:\n%s", text)
		}
		// 'main' function should be found
		if !strings.Contains(text, "func main") {
			t.Errorf("Expected output to contain 'func main', got:\n%s", text)
		}
	})

    // Graph dependent tests
    // Check if graph.gob exists
    graphPath := filepath.Join(testDataPath, ".codemap", "graph.gob")
    if _, err := os.Stat(graphPath); err == nil {
        t.Run("get_callers", func(t *testing.T) {
            input := CallersInput{Path: testDataPath, Symbol: "helper"}
            result, _, err := handleGetCallers(ctx, nil, input)
            if err != nil {
                t.Fatalf("handleGetCallers failed: %v", err)
            }
            if result.IsError {
                 // Might fail if graph is stale
                 t.Logf("handleGetCallers returned error (possibly stale graph): %v", result.Content)
                 return
            }
            text := result.Content[0].(*mcp.TextContent).Text
            if !strings.Contains(text, "process") {
                 t.Errorf("Expected caller 'process' for 'helper', got:\n%s", text)
            }
        })

        t.Run("trace_path", func(t *testing.T) {
            input := TracePathInput{Path: testDataPath, From: "process", To: "nested"}
            result, _, err := handleTracePath(ctx, nil, input)
            if err != nil {
                t.Fatalf("handleTracePath failed: %v", err)
            }
            if result.IsError {
                 t.Logf("handleTracePath returned error: %v", result.Content)
                 return
            }
            text := result.Content[0].(*mcp.TextContent).Text
            // Path: process -> helper -> nested
            if !strings.Contains(text, "helper") || !strings.Contains(text, "nested") {
                t.Errorf("Expected path process -> ... -> nested, got:\n%s", text)
            }
        })
    } else {
        t.Log("Skipping graph-dependent tests (graph.gob not found)")
    }

        t.Run("get_callees", func(t *testing.T) {
            input := CalleesInput{Path: testDataPath, Symbol: "process"}
            result, _, err := handleGetCallees(ctx, nil, input)
            if err != nil {
                t.Fatalf("handleGetCallees failed: %v", err)
            }
            if result.IsError {
                 t.Logf("handleGetCallees returned error: %v", result.Content)
                 return
            }
            text := result.Content[0].(*mcp.TextContent).Text
            if !strings.Contains(text, "helper") {
                 t.Errorf("Expected callee 'helper' for 'process', got:\n%s", text)
            }
        })

	t.Run("list_projects", func(t *testing.T) {
		// Parent of testDataPath is 'corpus'
		corpusPath := filepath.Dir(testDataPath)
		input := ListProjectsInput{Path: corpusPath}
		result, _, err := handleListProjects(ctx, nil, input)
		if err != nil {
			t.Fatalf("handleListProjects failed: %v", err)
		}
		text := result.Content[0].(*mcp.TextContent).Text
		if !strings.Contains(text, "go/") || !strings.Contains(text, "python/") {
			t.Errorf("Expected output to contain 'go/' and 'python/', got:\n%s", text)
		}
	})

	t.Run("get_dependencies_ts", func(t *testing.T) {
		tsPath := filepath.Join(filepath.Dir(testDataPath), "typescript")
		input := DepsInput{Path: tsPath, Detail: 0}
		result, _, err := handleGetDependencies(ctx, nil, input)
		if err != nil {
			t.Fatalf("handleGetDependencies TS failed: %v", err)
		}
		text := result.Content[0].(*mcp.TextContent).Text
		// Should show connection from types.ts to main
		// Or at least list types.ts
		if !strings.Contains(text, "types") {
			t.Errorf("Expected output to contain 'types', got:\n%s", text)
		}
	})

	t.Run("get_importers", func(t *testing.T) {
		// Use TypeScript corpus where types.ts imports main.ts
		tsPath := filepath.Join(filepath.Dir(testDataPath), "typescript")
		if _, err := os.Stat(tsPath); os.IsNotExist(err) {
			t.Skip("TypeScript corpus not found")
		}
		
		input := ImportersInput{Path: tsPath, File: "main.ts"}
		result, _, err := handleGetImporters(ctx, nil, input)
		if err != nil {
			t.Fatalf("handleGetImporters failed: %v", err)
		}
		text := result.Content[0].(*mcp.TextContent).Text
		if !strings.Contains(text, "types.ts") {
			// Debug: run deps scan manually and print imports
			t.Logf("Debug: running deps scan on %s", tsPath)
			// (Cannot call ScanForDeps here easily without importing scanner, which is not allowed if I'm in package main test)
			t.Errorf("Expected 'types.ts' to import 'main.ts', got:\n%s", text)
		}
	})

    // LLM dependent tests - use Mock provider
    os.Setenv("CODEMAP_LLM_PROVIDER", "mock")
    os.Setenv("CODEMAP_LLM_MODEL", "mock-model")

    // Ensure graph exists for these tests
    if _, err := os.Stat(graphPath); err == nil {
        t.Run("explain_symbol", func(t *testing.T) {
            input := ExplainSymbolInput{Path: testDataPath, Symbol: "main"}
            result, _, err := handleExplainSymbol(ctx, nil, input)
            if err != nil {
                t.Fatalf("handleExplainSymbol failed: %v", err)
            }
            if result.IsError {
                 text := result.Content[0].(*mcp.TextContent).Text
                 t.Logf("handleExplainSymbol returned error: %s", text)
                 // Don't fail yet, just log to debug. The test expects success.
                 t.Fail()
                 return
            }
            text := result.Content[0].(*mcp.TextContent).Text
            if !strings.Contains(text, "mock response") {
                t.Errorf("Expected mock response, got:\n%s", text)
            }
        })

        t.Run("summarize_module", func(t *testing.T) {
            input := SummarizeModuleInput{Path: testDataPath} // Summarize root of testDataPath
            result, _, err := handleSummarizeModule(ctx, nil, input)
            if err != nil {
                t.Fatalf("handleSummarizeModule failed: %v", err)
            }
            if result.IsError {
                 text := result.Content[0].(*mcp.TextContent).Text
                 t.Logf("handleSummarizeModule returned error: %s", text)
                 t.Fail()
                 return
            }
            text := result.Content[0].(*mcp.TextContent).Text
            if !strings.Contains(text, "mock response") {
                t.Errorf("Expected mock response, got:\n%s", text)
            }
        })

        t.Run("semantic_search_graph_only", func(t *testing.T) {
            // Without vector index, should fall back to graph search
            input := SemanticSearchInput{Path: testDataPath, Query: "main"}
            result, _, err := handleSemanticSearch(ctx, nil, input)
            if err != nil {
                t.Fatalf("handleSemanticSearch failed: %v", err)
            }
             if result.IsError {
                 text := result.Content[0].(*mcp.TextContent).Text
                 t.Logf("handleSemanticSearch returned error: %s", text)
                 t.Fail()
                 return
            }
            text := result.Content[0].(*mcp.TextContent).Text
            // Should find 'main' symbol
            if !strings.Contains(text, "main") {
                t.Errorf("Expected search to find 'main', got:\n%s", text)
            }
        })
    }

    // Git Diff Test
    t.Run("get_diff", func(t *testing.T) {
        // Create temp git repo
        tempDir, err := os.MkdirTemp("", "codemap-git-test")
        if err != nil {
            t.Fatalf("Failed to create temp dir: %v", err)
        }
        defer os.RemoveAll(tempDir)

        // Helper to run git commands
        git := func(args ...string) {
            cmd := exec.Command("git", args...)
            cmd.Dir = tempDir
            if out, err := cmd.CombinedOutput(); err != nil {
                t.Fatalf("git %v failed: %v\nOutput: %s", args, err, out)
            }
        }

        // Initialize repo
        git("init")
        git("config", "user.email", "test@example.com")
        git("config", "user.name", "Test User")
        
        // Ensure main branch
        git("checkout", "-b", "main")

        // Create a file
        filePath := filepath.Join(tempDir, "file.go")
        err = os.WriteFile(filePath, []byte("package main\nfunc main() {}\n"), 0644)
        if err != nil {
            t.Fatalf("WriteFile failed: %v", err)
        }

        git("add", ".")
        git("commit", "-m", "Initial commit")

        // Modify the file
        err = os.WriteFile(filePath, []byte("package main\nfunc main() { fmt.Println() }\n"), 0644)
        if err != nil {
            t.Fatalf("WriteFile failed: %v", err)
        }

        // Test get_diff
        input := DiffInput{Path: tempDir}
        result, _, err := handleGetDiff(ctx, nil, input)
        if err != nil {
            t.Fatalf("handleGetDiff failed: %v", err)
        }
        text := result.Content[0].(*mcp.TextContent).Text
        
        if !strings.Contains(text, "file.go") {
            t.Errorf("Expected diff to contain 'file.go', got:\n%s", text)
        }
    })
}
</file>
<file path="render/api.go">
package render

import (
	"fmt"
	"path/filepath"
	"sort"
	"strings"

	"codemap/scanner"
)

// APIView renders a compact public API surface view
func APIView(project scanner.DepsProject) {
	files := project.Files
	projectName := filepath.Base(project.Root)

	if len(files) == 0 {
		fmt.Println("  No source files found.")
		return
	}

	fmt.Println()
	fmt.Printf("=== API Surface: %s ===\n", projectName)
	fmt.Println()

	// Group by directory
	packages := make(map[string][]scanner.FileAnalysis)
	for _, f := range files {
		dir := filepath.Dir(f.Path)
		if dir == "." {
			dir = projectName
		}
		packages[dir] = append(packages[dir], f)
	}

	// Sort package names
	var pkgNames []string
	for name := range packages {
		pkgNames = append(pkgNames, name)
	}
	sort.Strings(pkgNames)

	for _, pkg := range pkgNames {
		pkgFiles := packages[pkg]

		// Collect all exported types and functions
		var exportedTypes []scanner.TypeInfo
		var exportedFuncs []scanner.FuncInfo

		for _, f := range pkgFiles {
			for _, t := range f.Types {
				if t.IsExported {
					exportedTypes = append(exportedTypes, t)
				}
			}
			for _, fn := range f.Functions {
				if fn.IsExported {
					exportedFuncs = append(exportedFuncs, fn)
				}
			}
		}

		// Skip packages with no exports
		if len(exportedTypes) == 0 && len(exportedFuncs) == 0 {
			continue
		}

		fmt.Printf("%s/\n", pkg)

		// Group methods by receiver type
		methodsByType := make(map[string][]scanner.FuncInfo)
		var standaloneFuncs []scanner.FuncInfo

		for _, fn := range exportedFuncs {
			if fn.Receiver != "" {
				// Extract type name from receiver
				typeName := extractTypeName(fn.Receiver)
				methodsByType[typeName] = append(methodsByType[typeName], fn)
			} else {
				standaloneFuncs = append(standaloneFuncs, fn)
			}
		}

		// Print types with their methods
		for _, t := range exportedTypes {
			icon := typeIcon(t.Kind)

			if len(t.Fields) > 0 {
				fmt.Printf("  %s %s {%s}\n", icon, t.Name, strings.Join(t.Fields, ", "))
			} else {
				fmt.Printf("  %s %s\n", icon, t.Name)
			}

			// Print methods for this type
			if methods, ok := methodsByType[t.Name]; ok {
				for _, m := range methods {
					if m.Signature != "" {
						fmt.Printf("    + %s\n", m.Signature)
					} else {
						fmt.Printf("    + (%s) %s()\n", m.Receiver, m.Name)
					}
				}
			}
		}

		// Print standalone functions
		if len(standaloneFuncs) > 0 {
			for _, fn := range standaloneFuncs {
				if fn.Signature != "" {
					fmt.Printf("  + %s\n", fn.Signature)
				} else {
					fmt.Printf("  + %s()\n", fn.Name)
				}
			}
		}

		fmt.Println()
	}

	// Summary
	totalTypes := 0
	totalFuncs := 0
	for _, f := range files {
		for _, t := range f.Types {
			if t.IsExported {
				totalTypes++
			}
		}
		for _, fn := range f.Functions {
			if fn.IsExported {
				totalFuncs++
			}
		}
	}

	fmt.Printf("─────────────────────────────────────────────────────────────\n")
	fmt.Printf("Exported: %d types · %d functions\n", totalTypes, totalFuncs)
	fmt.Println()
}

// typeIcon returns an ASCII icon for the type kind
func typeIcon(kind scanner.TypeKind) string {
	switch kind {
	case scanner.KindStruct:
		return "[S]"
	case scanner.KindClass:
		return "[C]"
	case scanner.KindInterface:
		return "[I]"
	case scanner.KindTrait:
		return "[T]"
	case scanner.KindEnum:
		return "[E]"
	case scanner.KindTypeAlias:
		return "[A]"
	case scanner.KindProtocol:
		return "[P]"
	default:
		return "[?]"
	}
}

// extractTypeName extracts type name from receiver like "(l *GrammarLoader)"
func extractTypeName(receiver string) string {
	// Remove parentheses
	r := strings.Trim(receiver, "()")

	// Split by space, take last part (the type)
	parts := strings.Fields(r)
	if len(parts) == 0 {
		return ""
	}

	typePart := parts[len(parts)-1]
	// Remove pointer asterisk
	return strings.TrimPrefix(typePart, "*")
}
</file>
<file path="render/colors.go">
package render

import (
	"os"
	"strings"

	"golang.org/x/term"
)

// ANSI color codes
const (
	Reset     = "\033[0m"
	Bold      = "\033[1m"
	Dim       = "\033[2m"
	White     = "\033[37m"
	Cyan      = "\033[36m"
	Yellow    = "\033[33m"
	Magenta   = "\033[35m"
	Green     = "\033[32m"
	Red       = "\033[31m"
	Blue      = "\033[34m"
	BoldWhite = "\033[1;37m"
	BoldRed   = "\033[1;31m"
	BoldBlue  = "\033[1;34m"
	DimWhite  = "\033[2;37m"
	BoldGreen = "\033[1;32m"
)

// Asset extensions to exclude from "top large files"
var assetExtensions = map[string]bool{
	".png": true, ".jpg": true, ".jpeg": true, ".gif": true, ".svg": true, ".ico": true, ".webp": true,
	".ttf": true, ".otf": true, ".woff": true, ".woff2": true, ".eot": true,
	".mp3": true, ".wav": true, ".mp4": true, ".mov": true,
	".zip": true, ".tar": true, ".gz": true, ".7z": true, ".rar": true,
	".pdf": true, ".doc": true, ".docx": true, ".xls": true, ".xlsx": true,
	".exe": true, ".dll": true, ".so": true, ".dylib": true, ".bin": true,
	".lock": true, ".resolved": true, ".sum": true,
	".map": true, ".nib": true, ".xib": true, ".storyboard": true,
}

// GetFileColor returns ANSI color code based on file extension
func GetFileColor(ext string) string {
	ext = strings.ToLower(ext)
	switch {
	case ext == ".go" || ext == ".mod" || ext == ".sum" || ext == ".dart":
		return Cyan
	case ext == ".py" || ext == ".js" || ext == ".ts" || ext == ".jsx" || ext == ".tsx" ||
		ext == ".mjs" || ext == ".cjs" || ext == ".vue" || ext == ".svelte" ||
		ext == ".pl" || ext == ".pm" || ext == ".sql" || ext == ".db" || ext == ".sqlite":
		return Yellow
	case ext == ".html" || ext == ".css" || ext == ".scss" || ext == ".sass" ||
		ext == ".less" || ext == ".php" || ext == ".hs" || ext == ".tf" || ext == ".hcl":
		return Magenta
	case ext == ".md" || ext == ".txt" || ext == ".rst" || ext == ".adoc":
		return Green
	case ext == ".json" || ext == ".yaml" || ext == ".yml" || ext == ".toml" ||
		ext == ".xml" || ext == ".csv" || ext == ".ini" || ext == ".conf" ||
		ext == ".env" || ext == ".rb" || ext == ".erb" || ext == ".gemspec":
		return Red
	case ext == ".sh" || ext == ".bat" || ext == ".ps1" ||
		strings.ToLower(ext) == "makefile" || strings.ToLower(ext) == "dockerfile":
		return BoldWhite
	case ext == ".swift" || ext == ".kt" || ext == ".java" || ext == ".scala" ||
		ext == ".groovy" || ext == ".rs" || ext == ".rlib":
		return BoldRed
	case ext == ".c" || ext == ".cpp" || ext == ".h" || ext == ".hpp" ||
		ext == ".cc" || ext == ".m" || ext == ".mm" || ext == ".cs" || ext == ".fs":
		return BoldBlue
	case ext == ".lua" || ext == ".r" || ext == ".rmd":
		return Blue
	case ext == ".gitignore" || ext == ".dockerignore" || ext == ".gitattributes":
		return DimWhite
	default:
		return White
	}
}

// GetTerminalWidth returns terminal width or default
func GetTerminalWidth() int {
	width, _, err := term.GetSize(int(os.Stdout.Fd()))
	if err != nil || width <= 0 {
		return 80
	}
	return width
}

// CenterString centers a string in the given width
func CenterString(s string, width int) string {
	if len(s) >= width {
		return s
	}
	leftPad := (width - len(s)) / 2
	rightPad := width - len(s) - leftPad
	return strings.Repeat(" ", leftPad) + s + strings.Repeat(" ", rightPad)
}

// IsAssetExtension returns true if the extension is an asset
func IsAssetExtension(ext string) bool {
	return assetExtensions[strings.ToLower(ext)]
}
</file>
<file path="render/depgraph.go">
package render

import (
	"fmt"
	"path/filepath"
	"regexp"
	"sort"
	"strings"

	"codemap/scanner"
)

// Language compatibility groups
var langGroups = map[string]string{
	"python":     "python",
	"go":         "go",
	"javascript": "js",
	"typescript": "js",
	"rust":       "rust",
	"ruby":       "ruby",
	"c":          "c",
	"cpp":        "c",
	"java":       "java",
	"swift":      "swift",
	"bash":       "bash",
	"kotlin":     "kotlin",
	"c_sharp":    "c_sharp",
	"php":        "php",
	"dart":       "dart",
	"r":          "r",
}

// Standard library names to filter out
var stdlibNames = map[string]bool{
	// Go stdlib
	"errors": true, "fmt": true, "io": true, "os": true, "path": true, "sync": true, "time": true, "context": true, "http": true,
	"net": true, "bytes": true, "strings": true, "strconv": true, "sort": true, "flag": true, "log": true, "bufio": true,
	"encoding": true, "testing": true, "runtime": true, "unsafe": true, "reflect": true, "regexp": true,
	// Python stdlib
	"logging": true, "typing": true, "collections": true, "datetime": true, "json": true, "sys": true, "re": true,
	"pathlib": true, "hashlib": true, "base64": true, "asyncio": true, "enum": true, "functools": true, "random": true,
	"math": true, "copy": true, "itertools": true, "contextlib": true,
	// JS/TS common
	"fs": true, "util": true, "events": true, "stream": true, "crypto": true, "https": true,
	"react": true, "filepath": true, "embed": true,
}

// normalizeImport normalizes an import string
func normalizeImport(imp, lang string) string {
	imp = strings.Trim(imp, "\"'")
	if strings.Contains(imp, "/") {
		parts := strings.Split(imp, "/")
		imp = parts[len(parts)-1]
	}
	if strings.Contains(imp, ".") && !strings.HasPrefix(imp, ".") {
		parts := strings.Split(imp, ".")
		imp = parts[len(parts)-1]
	}
	// Remove file extensions
	extPattern := regexp.MustCompile(`\.(py|go|js|ts|jsx|tsx|rb|rs|c|h|cpp|hpp|java|swift)$`)
	imp = extPattern.ReplaceAllString(imp, "")
	return strings.ToLower(imp)
}

// findInternalDeps finds which files import which other files
func findInternalDeps(files []scanner.FileAnalysis) map[string][]string {
	// Build lookup: name -> list of (path, language_group)
	type fileInfo struct {
		path      string
		langGroup string
	}
	nameToInfos := make(map[string][]fileInfo)

	for _, f := range files {
		langGroup := langGroups[f.Language]
		if langGroup == "" {
			langGroup = f.Language
		}
		basename := filepath.Base(f.Path)
		extPattern := regexp.MustCompile(`\.[^.]+$`)
		name := strings.ToLower(extPattern.ReplaceAllString(basename, ""))
		nameToInfos[name] = append(nameToInfos[name], fileInfo{f.Path, langGroup})
	}

	deps := make(map[string][]string)

	for _, f := range files {
		srcLang := f.Language
		srcGroup := langGroups[srcLang]
		if srcGroup == "" {
			srcGroup = srcLang
		}

		for _, imp := range f.Imports {
			// Skip stdlib-looking imports
			if !strings.Contains(imp, "/") && !strings.Contains(imp, ".") {
				if stdlibNames[strings.ToLower(imp)] {
					continue
				}
			}

			norm := normalizeImport(imp, srcLang)
			if stdlibNames[norm] {
				continue
			}

			if infos, ok := nameToInfos[norm]; ok {
				srcBasename := filepath.Base(f.Path)
				for _, info := range infos {
					if info.path == f.Path {
						continue // Skip self
					}
					if srcGroup != info.langGroup {
						continue // Skip cross-language
					}
					targetName := filepath.Base(info.path)
					if targetName == srcBasename {
						continue // Skip same-basename
					}
					// Check if already added
					found := false
					for _, d := range deps[f.Path] {
						if d == targetName {
							found = true
							break
						}
					}
					if !found {
						deps[f.Path] = append(deps[f.Path], targetName)
					}
				}
			}
		}
	}

	return deps
}

// getSystemName infers a system/component name from directory path
func getSystemName(dirPath string) string {
	parts := strings.Split(strings.ReplaceAll(dirPath, "\\", "/"), "/")
	skip := map[string]bool{"src": true, "lib": true, "app": true, "internal": true, "pkg": true, ".": true, "": true}

	var meaningful []string
	for _, p := range parts {
		if !skip[strings.ToLower(p)] {
			meaningful = append(meaningful, p)
		}
	}

	if len(meaningful) > 0 {
		name := meaningful[0]
		name = strings.ReplaceAll(name, "_", " ")
		name = strings.ReplaceAll(name, "-", " ")
		return strings.Title(name)
	}

	if len(parts) > 0 {
		return strings.Title(parts[len(parts)-1])
	}
	return "Root"
}

// Depgraph renders the dependency flow visualization
func Depgraph(project scanner.DepsProject) {
	files := project.Files
	externalDeps := project.ExternalDeps
	projectName := filepath.Base(project.Root)

	if len(files) == 0 {
		fmt.Println("  No source files found.")
		return
	}

	// Build internal names lookup
	internalNames := make(map[string]bool)
	extPattern := regexp.MustCompile(`\.[^.]+$`)
	for _, f := range files {
		basename := filepath.Base(f.Path)
		name := strings.ToLower(extPattern.ReplaceAllString(basename, ""))
		internalNames[name] = true
	}

	internalDeps := findInternalDeps(files)

	// Count dependencies on each file
	depCounts := make(map[string]int)
	for _, targets := range internalDeps {
		for _, target := range targets {
			depCounts[target]++
		}
	}

	// Group by top-level system
	systems := make(map[string][]scanner.FileAnalysis)
	for _, f := range files {
		parts := strings.Split(strings.ReplaceAll(f.Path, "\\", "/"), "/")
		system := "."
		if len(parts) > 1 {
			system = parts[0]
		}
		systems[system] = append(systems[system], f)
	}

	fmt.Println()

	// Build external deps by language
	extByLang := make(map[string][]string)
	versionPattern := regexp.MustCompile(`^v\d+$`)

	for lang, deps := range externalDeps {
		if len(deps) == 0 {
			continue
		}
		seen := make(map[string]bool)
		var names []string
		for _, d := range deps {
			parts := strings.Split(d, "/")
			name := parts[len(parts)-1]
			if versionPattern.MatchString(name) && len(parts) > 1 {
				name = parts[len(parts)-2]
			}
			if !versionPattern.MatchString(name) && len(name) > 1 && !seen[name] {
				seen[name] = true
				names = append(names, name)
			}
		}
		if len(names) > 0 {
			extByLang[lang] = names
		}
	}

	// Calculate box width
	title := fmt.Sprintf("%s - Dependency Flow", projectName)
	maxWidth := len(title) + 6

	// Format dep lines
	var depLines []string
	langOrder := []string{"go", "javascript", "python", "swift", "rust", "ruby", "bash", "kotlin", "c_sharp", "php", "dart", "r"}

	for _, lang := range langOrder {
		if names, ok := extByLang[lang]; ok {
			label := scanner.LangDisplay[lang].Short
			if label == "" {
				label = strings.Title(lang)
			}
			line := fmt.Sprintf("%s: %s", label, strings.Join(names, ", "))
			depLines = append(depLines, line)
			if len(line)+4 > maxWidth {
				maxWidth = len(line) + 4
			}
		}
	}

	// Cap at 80
	if maxWidth > 80 {
		maxWidth = 80
	}
	innerWidth := maxWidth - 2

	// Print header box
	fmt.Printf("╭%s╮\n", strings.Repeat("─", innerWidth))
	titlePadded := CenterString(title, innerWidth)
	fmt.Printf("│%s│\n", titlePadded)

	if len(depLines) > 0 {
		fmt.Printf("├%s┤\n", strings.Repeat("─", innerWidth))
		contentWidth := innerWidth - 2

		for _, line := range depLines {
			for len(line) > contentWidth {
				breakAt := strings.LastIndex(line[:contentWidth], ", ")
				if breakAt == -1 {
					breakAt = contentWidth - 1
				} else {
					breakAt++
				}
				fmt.Printf("│ %-*s │\n", contentWidth, line[:breakAt])
				line = "    " + strings.TrimLeft(line[breakAt:], " ")
			}
			fmt.Printf("│ %-*s │\n", contentWidth, line)
		}
	}

	fmt.Printf("╰%s╯\n", strings.Repeat("─", innerWidth))
	fmt.Println()

	// Sort systems
	var systemNames []string
	for name := range systems {
		systemNames = append(systemNames, name)
	}
	sort.Strings(systemNames)

	// Render each system
	for _, system := range systemNames {
		sysFiles := systems[system]
		systemName := getSystemName(system)

		// Check if system has content
		hasContent := false
		for _, f := range sysFiles {
			if len(internalDeps[f.Path]) > 0 || len(f.Functions) > 0 {
				hasContent = true
				break
			}
		}
		if !hasContent {
			continue
		}

		// Section header
		headerLen := 60 - len(systemName) - 1
		if headerLen < 1 {
			headerLen = 1
		}
		fmt.Printf("%s %s\n", systemName, strings.Repeat("═", headerLen))

		rendered := make(map[string]bool)

		for _, f := range sysFiles {
			basename := filepath.Base(f.Path)
			nameNoExt := extPattern.ReplaceAllString(basename, "")

			if rendered[basename] {
				continue
			}

			targets := internalDeps[f.Path]
			if len(targets) == 0 {
				continue
			}

			if len(targets) == 1 {
				t := targets[0]
				tName := extPattern.ReplaceAllString(t, "")

				// Check for sub-deps
				var tPath string
				for _, ff := range files {
					if filepath.Base(ff.Path) == t {
						tPath = ff.Path
						break
					}
				}

				subTargets := internalDeps[tPath]
				if len(subTargets) > 0 {
					var subNames []string
					for i, s := range subTargets {
						if i >= 3 {
							break
						}
						subNames = append(subNames, extPattern.ReplaceAllString(s, ""))
					}
					chain := fmt.Sprintf("%s ───▶ %s ───▶ %s", nameNoExt, tName, strings.Join(subNames, ", "))
					if len(subTargets) > 3 {
						chain += fmt.Sprintf(" +%d", len(subTargets)-3)
					}
					fmt.Printf("  %s\n", chain)
				} else {
					fmt.Printf("  %s ───▶ %s\n", nameNoExt, tName)
				}
			} else {
				var targetStrs []string
				for _, t := range targets {
					targetStrs = append(targetStrs, extPattern.ReplaceAllString(t, ""))
				}

				if len(targets) <= 4 {
					fmt.Printf("  %s ───▶ %s\n", nameNoExt, strings.Join(targetStrs, ", "))
				} else {
					fmt.Printf("  %s ──┬──▶ %s\n", nameNoExt, targetStrs[0])
					for _, t := range targetStrs[1 : len(targetStrs)-1] {
						fmt.Printf("  %s   ├──▶ %s\n", strings.Repeat(" ", len(nameNoExt)), t)
					}
					fmt.Printf("  %s   └──▶ %s\n", strings.Repeat(" ", len(nameNoExt)), targetStrs[len(targetStrs)-1])
				}
			}

			rendered[basename] = true
		}

		// Count standalone files
		standaloneCount := 0
		for _, f := range sysFiles {
			basename := filepath.Base(f.Path)
			if !rendered[basename] && len(f.Functions) > 0 {
				standaloneCount++
			}
		}

		if standaloneCount > 0 {
			fmt.Printf("  +%d standalone files\n", standaloneCount)
		}

		fmt.Println()
	}

	// HUBS section
	if len(depCounts) > 0 {
		type hub struct {
			name  string
			count int
		}
		var hubs []hub
		for name, count := range depCounts {
			if count >= 2 {
				hubs = append(hubs, hub{name, count})
			}
		}
		sort.Slice(hubs, func(i, j int) bool {
			return hubs[i].count > hubs[j].count
		})
		if len(hubs) > 6 {
			hubs = hubs[:6]
		}

		if len(hubs) > 0 {
			fmt.Println(strings.Repeat("─", 61))
			var hubStrs []string
			for _, h := range hubs {
				hubStrs = append(hubStrs, fmt.Sprintf("%s (%d←)", extPattern.ReplaceAllString(h.name, ""), h.count))
			}
			fmt.Printf("HUBS: %s\n", strings.Join(hubStrs, ", "))
		}
	}

	// Summary
	totalFuncs := 0
	totalTypes := 0
	for _, f := range files {
		totalFuncs += len(f.Functions)
		totalTypes += len(f.Types)
	}
	internalCount := 0
	for _, targets := range internalDeps {
		internalCount += len(targets)
	}
	if totalTypes > 0 {
		fmt.Printf("%d files · %d functions · %d types · %d deps\n", len(files), totalFuncs, totalTypes, internalCount)
	} else {
		fmt.Printf("%d files · %d functions · %d deps\n", len(files), totalFuncs, internalCount)
	}
	fmt.Println()
}
</file>
<file path="render/skyline.go">
package render

import (
	"fmt"
	"math"
	"math/rand"
	"os"
	"path/filepath"
	"sort"
	"strings"
	"time"

	"codemap/scanner"

	tea "github.com/charmbracelet/bubbletea"
	"golang.org/x/term"
)

// Code extensions for skyline (what counts as "source code")
var codeExtensions = map[string]bool{
	".py": true, ".js": true, ".ts": true, ".jsx": true, ".tsx": true, ".go": true, ".rs": true, ".rb": true, ".java": true,
	".swift": true, ".kt": true, ".scala": true, ".c": true, ".cpp": true, ".h": true, ".hpp": true, ".cs": true, ".fs": true,
	".php": true, ".lua": true, ".r": true, ".dart": true, ".vue": true, ".svelte": true, ".elm": true, ".ex": true, ".exs": true,
	".hs": true, ".ml": true, ".clj": true, ".erl": true, ".sh": true, ".bash": true, ".zsh": true, ".fish": true, ".ps1": true,
	".html": true, ".css": true, ".scss": true, ".sass": true, ".less": true,
	".sql": true, ".graphql": true, ".proto": true,
}

var codeFilenames = map[string]bool{
	"Makefile": true, "Dockerfile": true, "Rakefile": true, "Gemfile": true, "Procfile": true,
	"Vagrantfile": true, "Jenkinsfile": true, "Fastfile": true,
}

// Building dimensions
const (
	buildingWidth = 7
	maxHeight     = 12
	minHeight     = 2
	skyHeight     = 6
)

// Building colors
var buildingColors = []string{
	"\033[36m", // cyan
	"\033[33m", // yellow
	"\033[35m", // magenta
	"\033[31m", // red
	"\033[32m", // green
	"\033[34m", // blue
	"\033[96m", // bright cyan
	"\033[93m", // bright yellow
	"\033[95m", // bright magenta
	"\033[91m", // bright red
	"\033[92m", // bright green
	"\033[94m", // bright blue
}

// Building data
type building struct {
	height   int
	char     rune
	color    string
	ext      string
	extLabel string
	count    int
	size     int64
	gap      int
}

// Aggregated extension data
type extAgg struct {
	ext   string
	size  int64
	count int
}

// filterCodeFiles returns only source code files
func filterCodeFiles(files []scanner.FileInfo) []scanner.FileInfo {
	var result []scanner.FileInfo
	for _, f := range files {
		if codeExtensions[strings.ToLower(f.Ext)] || codeFilenames[filepath.Base(f.Path)] {
			result = append(result, f)
		}
	}
	if len(result) == 0 {
		return files
	}
	return result
}

// aggregateByExtension groups files by extension
func aggregateByExtension(files []scanner.FileInfo) []extAgg {
	groups := make(map[string]*extAgg)
	for _, f := range files {
		ext := strings.ToLower(f.Ext)
		if ext == "" {
			ext = filepath.Base(f.Path)
		}
		if groups[ext] == nil {
			groups[ext] = &extAgg{ext: ext}
		}
		groups[ext].size += f.Size
		groups[ext].count++
	}

	var result []extAgg
	for _, agg := range groups {
		result = append(result, *agg)
	}
	sort.Slice(result, func(i, j int) bool {
		return result[i].size > result[j].size
	})
	return result
}

// getBuildingChar returns building texture character
func getBuildingChar(ext string) rune {
	ext = strings.ToLower(ext)
	switch {
	case ext == ".go" || ext == ".dart":
		return '▓'
	case ext == ".py" || ext == ".js" || ext == ".ts" || ext == ".jsx" || ext == ".tsx":
		return '░'
	case ext == ".rb" || ext == ".erb":
		return '▒'
	case ext == ".sh" || ext == "makefile" || ext == "dockerfile":
		return '█'
	default:
		return '▓'
	}
}

// createBuildings creates building data from aggregated files
func createBuildings(sorted []extAgg, width int) []building {
	if len(sorted) == 0 {
		return nil
	}

	// Find size range
	var minSize, maxSize int64 = sorted[len(sorted)-1].size, sorted[0].size
	sizeRange := maxSize - minSize
	if sizeRange == 0 {
		sizeRange = 1
	}

	getHeight := func(size int64) int {
		ratio := float64(size-minSize) / float64(sizeRange)
		ratio = math.Sqrt(ratio)
		return minHeight + int(ratio*float64(maxHeight-minHeight))
	}

	rand.Seed(42)
	var buildings []building

	for idx, agg := range sorted {
		extLabel := agg.ext
		if strings.HasPrefix(extLabel, ".") && len(extLabel) > 5 {
			extLabel = extLabel[:5]
		}

		buildings = append(buildings, building{
			height:   getHeight(agg.size),
			char:     getBuildingChar(agg.ext),
			color:    buildingColors[idx%len(buildingColors)],
			ext:      agg.ext,
			extLabel: extLabel,
			count:    agg.count,
			size:     agg.size,
			gap:      []int{1, 2, 2, 3}[rand.Intn(4)],
		})
	}

	// Arrange: tallest in middle
	sort.Slice(buildings, func(i, j int) bool {
		return buildings[i].height > buildings[j].height
	})

	var arranged []building
	for i, b := range buildings {
		if i%2 == 0 {
			arranged = append(arranged, b)
		} else {
			arranged = append([]building{b}, arranged...)
		}
	}

	// Limit to fit width
	totalWidth := 0
	for _, b := range arranged {
		totalWidth += buildingWidth + b.gap
	}
	for totalWidth > width-8 && len(arranged) > 0 {
		if len(arranged)%2 == 0 {
			arranged = arranged[1:]
		} else {
			arranged = arranged[:len(arranged)-1]
		}
		totalWidth = 0
		for _, b := range arranged {
			totalWidth += buildingWidth + b.gap
		}
	}

	return arranged
}

// Skyline renders the city skyline visualization
func Skyline(project scanner.Project, animate bool) {
	files := project.Files
	projectName := filepath.Base(project.Root)

	width, _, err := term.GetSize(int(os.Stdout.Fd()))
	if err != nil || width <= 0 {
		width = 80
	}

	codeFiles := filterCodeFiles(files)
	sorted := aggregateByExtension(codeFiles)
	arranged := createBuildings(sorted, width)

	if len(arranged) == 0 {
		fmt.Println(Dim + "No source files to display" + Reset)
		return
	}

	// Calculate layout
	totalWidth := 0
	for _, b := range arranged {
		totalWidth += buildingWidth + b.gap
	}
	leftMargin := (width - totalWidth) / 2
	scenePadding := 4
	sceneLeft := max(0, leftMargin-scenePadding)
	sceneRight := min(width, leftMargin+totalWidth+scenePadding)
	sceneWidth := sceneRight - sceneLeft

	if animate {
		renderAnimated(arranged, width, leftMargin, sceneLeft, sceneRight, sceneWidth, codeFiles, projectName, sorted)
	} else {
		renderStatic(arranged, width, leftMargin, sceneLeft, sceneRight, sceneWidth, codeFiles, projectName, sorted)
	}
}

// renderStatic renders static skyline
func renderStatic(arranged []building, width, leftMargin, sceneLeft, sceneRight, sceneWidth int,
	codeFiles []scanner.FileInfo, projectName string, sorted []extAgg) {

	rand.Seed(42)

	// Build grid
	grid := make([][]rune, skyHeight+maxHeight+1)
	for i := range grid {
		grid[i] = make([]rune, width)
		for j := range grid[i] {
			grid[i][j] = ' '
		}
	}

	// Render sky
	for row := 0; row < skyHeight; row++ {
		for i := 0; i < sceneWidth/10; i++ {
			col := rand.Intn(sceneRight-sceneLeft) + sceneLeft
			if col >= 0 && col < width {
				stars := []rune{'·', '·', '·', '✦', '*', '·'}
				grid[row][col] = stars[rand.Intn(len(stars))]
			}
		}
	}

	// Moon
	moonCol := sceneRight - 3
	if moonCol >= 0 && moonCol < width {
		grid[1][moonCol] = '◐'
	}

	// Render buildings
	col := leftMargin
	for _, b := range arranged {
		buildingTop := skyHeight + maxHeight - b.height

		// Rooftop cap
		if buildingTop > skyHeight {
			for j := 0; j < buildingWidth; j++ {
				if col+j < width {
					grid[buildingTop][col+j] = '▄'
				}
			}
		}

		// Building body
		buildingHeight := skyHeight + maxHeight + 1 - buildingTop - 1
		centerRow := buildingTop + 1 + buildingHeight/2

		for row := buildingTop + 1; row < skyHeight+maxHeight+1; row++ {
			for j := 0; j < buildingWidth; j++ {
				if col+j < width {
					if row == centerRow && buildingHeight >= 3 {
						extStart := (buildingWidth - len(b.extLabel)) / 2
						extEnd := extStart + len(b.extLabel)
						if j >= extStart && j < extEnd {
							grid[row][col+j] = rune(b.extLabel[j-extStart])
						} else {
							grid[row][col+j] = b.char
						}
					} else {
						grid[row][col+j] = b.char
					}
				}
			}
		}
		col += buildingWidth + b.gap
	}

	fmt.Println()

	// Print with colors
	colPositions := make([][3]interface{}, 0) // start, end, color
	col = leftMargin
	for _, b := range arranged {
		colPositions = append(colPositions, [3]interface{}{col, col + buildingWidth, b.color})
		col += buildingWidth + b.gap
	}

	// Sky rows
	for row := 0; row < skyHeight; row++ {
		for c := 0; c < width; c++ {
			ch := grid[row][c]
			switch ch {
			case '◐':
				fmt.Print(Bold + Yellow + string(ch) + Reset)
			case '·', '✦', '*':
				fmt.Print(DimWhite + string(ch) + Reset)
			default:
				fmt.Print(" ")
			}
		}
		fmt.Println()
	}

	// Building rows
	for row := skyHeight; row < len(grid); row++ {
		for c := 0; c < width; c++ {
			ch := grid[row][c]
			if ch == ' ' {
				fmt.Print(" ")
			} else if ch == '▄' {
				color := White
				for _, pos := range colPositions {
					if c >= pos[0].(int) && c < pos[1].(int) {
						color = pos[2].(string)
						break
					}
				}
				fmt.Print(color + string(ch) + Reset)
			} else if ch == '.' || (ch >= 'a' && ch <= 'z') {
				fmt.Print(DimWhite + string(ch) + Reset)
			} else if (ch >= 'A' && ch <= 'Z') || (ch >= '0' && ch <= '9') || ch == '_' || ch == '-' {
				fmt.Print(BoldWhite + string(ch) + Reset)
			} else {
				color := White
				for _, pos := range colPositions {
					if c >= pos[0].(int) && c < pos[1].(int) {
						color = pos[2].(string)
						break
					}
				}
				fmt.Print(color + string(ch) + Reset)
			}
		}
		fmt.Println()
	}

	// Ground
	ground := strings.Repeat(" ", max(0, sceneLeft)) + strings.Repeat("▀", sceneWidth)
	fmt.Println(DimWhite + ground + Reset)

	// Stats
	fmt.Println()
	title := fmt.Sprintf("─── %s ───", projectName)
	fmt.Printf("%s%s%s\n", BoldWhite, CenterString(title, width), Reset)

	var codeSize int64
	for _, f := range codeFiles {
		codeSize += f.Size
	}
	stats := fmt.Sprintf("%d languages · %d files · %s", len(sorted), len(codeFiles), formatSize(codeSize))
	fmt.Printf("%s%s%s\n", Cyan, CenterString(stats, width), Reset)
	fmt.Println()
}

// animationModel holds state for bubbletea animation
type animationModel struct {
	arranged           []building
	width              int
	leftMargin         int
	sceneLeft          int
	sceneRight         int
	sceneWidth         int
	codeFiles          []scanner.FileInfo
	projectName        string
	sorted             []extAgg
	starPositions      [][2]int
	moonCol            int
	maxBuildingHeight  int
	phase              int // 1 = rising, 2 = twinkling
	frame              int
	visibleRows        int
	shootingStarRow    int
	shootingStarCol    int
	shootingStarActive bool
	done               bool
}

type tickMsg time.Time

func tickCmd() tea.Cmd {
	return tea.Tick(60*time.Millisecond, func(t time.Time) tea.Msg {
		return tickMsg(t)
	})
}

func (m animationModel) Init() tea.Cmd {
	return tickCmd()
}

func (m animationModel) Update(msg tea.Msg) (tea.Model, tea.Cmd) {
	switch msg.(type) {
	case tea.KeyMsg:
		// Any key exits
		m.done = true
		return m, tea.Quit
	case tickMsg:
		if m.phase == 1 {
			m.visibleRows++
			if m.visibleRows > m.maxBuildingHeight+2 {
				m.phase = 2
				m.frame = 0
			}
		} else {
			m.frame++
			// Shooting star logic
			if m.shootingStarActive {
				m.shootingStarCol += 3
				if m.shootingStarCol > m.sceneRight {
					m.shootingStarActive = false
				}
			} else if m.frame == 10 || m.frame == 28 {
				m.shootingStarActive = true
				m.shootingStarRow = rand.Intn(3)
				m.shootingStarCol = m.sceneLeft
			}
			if m.frame >= 40 {
				m.done = true
				return m, tea.Quit
			}
		}
		return m, tickCmd()
	}
	return m, nil
}

func (m animationModel) View() string {
	var sb strings.Builder

	// Draw sky
	for row := 0; row < skyHeight; row++ {
		line := make([]rune, m.width)
		for i := range line {
			line[i] = ' '
		}

		// Stars (random twinkling)
		for _, pos := range m.starPositions {
			if pos[0] == row && rand.Float32() > 0.25 {
				stars := []rune{'·', '·', '✦', '*'}
				line[pos[1]] = stars[rand.Intn(len(stars))]
			}
		}

		// Moon
		if row == 1 && m.moonCol >= 0 && m.moonCol < m.width {
			line[m.moonCol] = '◐'
		}

		// Render sky row
		for c, ch := range line {
			// Shooting star in phase 2
			if m.phase == 2 && m.shootingStarActive && row == m.shootingStarRow {
				if c >= m.shootingStarCol && c < m.shootingStarCol+3 {
					trail := []rune{'─', '─', '★'}
					sb.WriteString(Bold + Yellow + string(trail[c-m.shootingStarCol]) + Reset)
					continue
				}
			}
			switch ch {
			case '◐':
				sb.WriteString(Bold + Yellow + string(ch) + Reset)
			case '·', '✦', '*':
				sb.WriteString(DimWhite + string(ch) + Reset)
			default:
				sb.WriteString(" ")
			}
		}
		sb.WriteString("\n")
	}

	// Build column positions for coloring
	colPositions := make([][3]interface{}, 0)
	col := m.leftMargin
	for _, b := range m.arranged {
		colPositions = append(colPositions, [3]interface{}{col, col + buildingWidth, b.color})
		col += buildingWidth + b.gap
	}

	// Draw buildings
	for row := 0; row <= maxHeight; row++ {
		visibleTop := maxHeight + 1 - m.visibleRows
		if m.phase == 2 {
			visibleTop = 0 // Full buildings in phase 2
		}

		col := m.leftMargin
		line := make([]rune, m.width)
		for i := range line {
			line[i] = ' '
		}

		for _, b := range m.arranged {
			buildingTop := maxHeight - b.height
			buildingHeight := maxHeight + 1 - buildingTop - 1
			centerRow := buildingTop + 1 + buildingHeight/2

			if row >= max(buildingTop, visibleTop) && row <= maxHeight {
				if row == buildingTop && buildingTop > 0 && row >= visibleTop {
					for j := 0; j < buildingWidth; j++ {
						if col+j < m.width {
							line[col+j] = '▄'
						}
					}
				} else if row > buildingTop {
					for j := 0; j < buildingWidth; j++ {
						if col+j < m.width {
							if row == centerRow && buildingHeight >= 3 {
								extStart := (buildingWidth - len(b.extLabel)) / 2
								extEnd := extStart + len(b.extLabel)
								if j >= extStart && j < extEnd {
									line[col+j] = rune(b.extLabel[j-extStart])
								} else {
									line[col+j] = b.char
								}
							} else {
								line[col+j] = b.char
							}
						}
					}
				}
			}
			col += buildingWidth + b.gap
		}

		// Render building row
		for c, ch := range line {
			if ch == ' ' {
				sb.WriteString(" ")
			} else if ch == '▄' {
				color := White
				for _, pos := range colPositions {
					if c >= pos[0].(int) && c < pos[1].(int) {
						color = pos[2].(string)
						break
					}
				}
				sb.WriteString(color + string(ch) + Reset)
			} else if ch == '.' || (ch >= 'a' && ch <= 'z') {
				sb.WriteString(DimWhite + string(ch) + Reset)
			} else if (ch >= 'A' && ch <= 'Z') || (ch >= '0' && ch <= '9') {
				sb.WriteString(BoldWhite + string(ch) + Reset)
			} else {
				color := White
				for _, pos := range colPositions {
					if c >= pos[0].(int) && c < pos[1].(int) {
						color = pos[2].(string)
						break
					}
				}
				sb.WriteString(color + string(ch) + Reset)
			}
		}
		sb.WriteString("\n")
	}

	// Ground
	ground := strings.Repeat(" ", max(0, m.sceneLeft)) + strings.Repeat("▀", m.sceneWidth)
	sb.WriteString(DimWhite + ground + Reset + "\n")

	return sb.String()
}

// renderAnimated renders animated skyline using bubbletea
func renderAnimated(arranged []building, width, leftMargin, sceneLeft, sceneRight, sceneWidth int,
	codeFiles []scanner.FileInfo, projectName string, sorted []extAgg) {

	rand.Seed(42)

	// Generate star positions
	var starPositions [][2]int
	for row := 0; row < skyHeight; row++ {
		for i := 0; i < sceneWidth/8; i++ {
			col := rand.Intn(sceneRight-sceneLeft) + sceneLeft
			if col >= 0 && col < width {
				starPositions = append(starPositions, [2]int{row, col})
			}
		}
	}

	moonCol := sceneRight - 3
	maxBuildingHeight := 0
	for _, b := range arranged {
		if b.height > maxBuildingHeight {
			maxBuildingHeight = b.height
		}
	}

	m := animationModel{
		arranged:          arranged,
		width:             width,
		leftMargin:        leftMargin,
		sceneLeft:         sceneLeft,
		sceneRight:        sceneRight,
		sceneWidth:        sceneWidth,
		codeFiles:         codeFiles,
		projectName:       projectName,
		sorted:            sorted,
		starPositions:     starPositions,
		moonCol:           moonCol,
		maxBuildingHeight: maxBuildingHeight,
		phase:             1,
		visibleRows:       1,
	}

	p := tea.NewProgram(m, tea.WithAltScreen())
	p.Run()

	// After animation, print static final frame to main screen
	renderStatic(arranged, width, leftMargin, sceneLeft, sceneRight, sceneWidth, codeFiles, projectName, sorted)
}

func max(a, b int) int {
	if a > b {
		return a
	}
	return b
}

func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}
</file>
<file path="render/tree.go">
package render

import (
	"fmt"
	"os"
	"path/filepath"
	"sort"
	"strings"

	"codemap/scanner"
)

// treeNode represents a node in the file tree
type treeNode struct {
	name     string
	isFile   bool
	file     *scanner.FileInfo
	children map[string]*treeNode
}

// getTopLargeFiles returns paths of top 5 largest source code files
func getTopLargeFiles(files []scanner.FileInfo) map[string]bool {
	// Filter out assets and binaries (no extension = likely binary)
	var sourceFiles []scanner.FileInfo
	for _, f := range files {
		ext := strings.ToLower(f.Ext)
		// Skip if no extension (likely binary) or if it's an asset
		if ext == "" || IsAssetExtension(ext) {
			continue
		}
		sourceFiles = append(sourceFiles, f)
	}

	// Sort by size descending
	sort.Slice(sourceFiles, func(i, j int) bool {
		return sourceFiles[i].Size > sourceFiles[j].Size
	})

	// Return top 5 as set
	result := make(map[string]bool)
	for i := 0; i < len(sourceFiles) && i < 5; i++ {
		result[sourceFiles[i].Path] = true
	}
	return result
}

// getDirStats recursively calculates file count and total size
func getDirStats(node *treeNode) (int, int64) {
	if node.isFile {
		return 1, node.file.Size
	}
	count := 0
	var size int64 = 0
	for _, child := range node.children {
		c, s := getDirStats(child)
		count += c
		size += s
	}
	return count, size
}

// buildTreeStructure builds a nested tree from flat file list
func buildTreeStructure(files []scanner.FileInfo) *treeNode {
	root := &treeNode{children: make(map[string]*treeNode)}

	for _, f := range files {
		parts := strings.Split(f.Path, string(os.PathSeparator))
		current := root
		for i, part := range parts {
			if i == len(parts)-1 {
				// File
				fileCopy := f
				current.children[part] = &treeNode{
					name:   part,
					isFile: true,
					file:   &fileCopy,
				}
			} else {
				// Directory
				if current.children[part] == nil {
					current.children[part] = &treeNode{
						name:     part,
						children: make(map[string]*treeNode),
					}
				}
				current = current.children[part]
			}
		}
	}
	return root
}

// formatSize converts bytes to human readable format
func formatSize(size int64) string {
	units := []string{"B", "KB", "MB", "GB", "TB"}
	fsize := float64(size)
	for _, unit := range units[:len(units)-1] {
		if fsize < 1024 {
			return fmt.Sprintf("%.1f%s", fsize, unit)
		}
		fsize /= 1024
	}
	return fmt.Sprintf("%.1f%s", fsize, units[len(units)-1])
}

// formatTokens converts token count to human readable format (e.g., "12.5k")
func formatTokens(tokens int) string {
	if tokens < 1000 {
		return fmt.Sprintf("%d", tokens)
	}
	return fmt.Sprintf("%.1fk", float64(tokens)/1000)
}

// Tree renders the file tree to stdout
func Tree(project scanner.Project) {
	files := project.Files
	projectName := filepath.Base(project.Root)
	isDiffMode := project.DiffRef != ""

	// Calculate stats
	totalFiles := len(files)
	var totalSize int64 = 0
	var totalTokens int = 0
	var totalAdded, totalRemoved int = 0, 0
	extCount := make(map[string]int)
	for _, f := range files {
		totalSize += f.Size
		totalTokens += f.Tokens
		totalAdded += f.Added
		totalRemoved += f.Removed
		if f.Ext != "" {
			extCount[f.Ext]++
		}
	}

	// Get top extensions
	type extEntry struct {
		ext   string
		count int
	}
	var exts []extEntry
	for ext, count := range extCount {
		exts = append(exts, extEntry{ext, count})
	}
	sort.Slice(exts, func(i, j int) bool {
		return exts[i].count > exts[j].count
	})
	if len(exts) > 5 {
		exts = exts[:5]
	}

	// Get top large files
	topLarge := getTopLargeFiles(files)

	// Build stats line - different for diff mode
	var statsLine string
	if isDiffMode {
		if totalRemoved > 0 {
			statsLine = fmt.Sprintf("Changed: %d files | +%d -%d lines vs %s", totalFiles, totalAdded, totalRemoved, project.DiffRef)
		} else {
			statsLine = fmt.Sprintf("Changed: %d files | +%d lines vs %s", totalFiles, totalAdded, project.DiffRef)
		}
	} else {
		statsLine = fmt.Sprintf("Files: %d | Size: %s | Tokens: ~%s", totalFiles, formatSize(totalSize), formatTokens(totalTokens))
	}

	// Build extensions line
	var extLine string
	if len(exts) > 0 {
		extParts := make([]string, len(exts))
		for i, e := range exts {
			extParts[i] = fmt.Sprintf("%s (%d)", e.ext, e.count)
		}
		extLine = "Top Extensions: " + strings.Join(extParts, ", ")
	}

	// Calculate dynamic width based on content (minimum 64, or fit longest line)
	titleLine := fmt.Sprintf(" %s ", projectName)
	innerWidth := 64
	if len(statsLine)+4 > innerWidth {
		innerWidth = len(statsLine) + 4
	}
	if len(extLine)+4 > innerWidth {
		innerWidth = len(extLine) + 4
	}

	// Print header box
	padding := innerWidth - len(titleLine)
	leftPad := padding / 2
	rightPad := padding - leftPad
	fmt.Printf("╭%s%s%s╮\n", strings.Repeat("─", leftPad), titleLine, strings.Repeat("─", rightPad))
	fmt.Printf("│ %-*s │\n", innerWidth-2, statsLine)
	if extLine != "" {
		fmt.Printf("│ %-*s │\n", innerWidth-2, extLine)
	}
	fmt.Printf("╰%s╯\n", strings.Repeat("─", innerWidth))

	// Build and render tree
	root := buildTreeStructure(files)
	fmt.Printf("%s%s%s\n", Bold, projectName, Reset)
	printTreeNode(root, "", true, topLarge)

	// Print impact footer for diff mode
	if isDiffMode && len(project.Impact) > 0 {
		fmt.Println()
		for _, imp := range project.Impact {
			files := "files"
			if imp.UsedBy == 1 {
				files = "file"
			}
			fmt.Printf("%s⚠ %s is used by %d other %s%s\n", Yellow, imp.File, imp.UsedBy, files, Reset)
		}
	}
}

// printTreeNode recursively prints tree nodes
func printTreeNode(node *treeNode, prefix string, isLast bool, topLarge map[string]bool) {
	// Separate dirs and files
	var dirs, fileNodes []*treeNode
	for _, child := range node.children {
		if child.isFile {
			fileNodes = append(fileNodes, child)
		} else {
			dirs = append(dirs, child)
		}
	}

	// Sort
	sort.Slice(dirs, func(i, j int) bool { return dirs[i].name < dirs[j].name })
	sort.Slice(fileNodes, func(i, j int) bool { return fileNodes[i].name < fileNodes[j].name })

	// Print directories first
	for i, dir := range dirs {
		isLastDir := i == len(dirs)-1 && len(fileNodes) == 0

		// Flatten single-child directories
		mergedName := dir.name
		current := dir
		for len(current.children) == 1 {
			var onlyChild *treeNode
			for _, c := range current.children {
				onlyChild = c
			}
			if onlyChild.isFile {
				break
			}
			mergedName = mergedName + "/" + onlyChild.name
			current = onlyChild
		}

		// Get stats
		fileCount, totalSize := getDirStats(current)

		// Check for homogeneous extensions
		var commonExt string
		immediateFiles := make([]*scanner.FileInfo, 0)
		for _, c := range current.children {
			if c.isFile {
				immediateFiles = append(immediateFiles, c.file)
			}
		}
		if len(immediateFiles) > 1 {
			extSet := make(map[string]bool)
			for _, f := range immediateFiles {
				extSet[f.Ext] = true
			}
			if len(extSet) == 1 {
				for ext := range extSet {
					commonExt = ext
				}
			}
		}

		// Format stats
		var statsParts []string
		if fileCount == 1 {
			statsParts = append(statsParts, formatSize(totalSize))
		} else {
			statsParts = append(statsParts, fmt.Sprintf("%d files", fileCount))
			statsParts = append(statsParts, formatSize(totalSize))
		}
		if commonExt != "" {
			statsParts = append(statsParts, fmt.Sprintf("all %s", commonExt))
		}

		connector := "├── "
		if isLastDir {
			connector = "└── "
		}

		fmt.Printf("%s%s%s  %s/%s %s(%s)%s\n",
			prefix, connector, BoldBlue, mergedName, Reset, Dim, strings.Join(statsParts, ", "), Reset)

		newPrefix := prefix + "│   "
		if isLastDir {
			newPrefix = prefix + "    "
		}
		printTreeNode(current, newPrefix, isLastDir, topLarge)
	}

	// Print files as a grid (multi-column layout like Python)
	if len(fileNodes) > 0 {
		connector := "└── "
		termWidth := GetTerminalWidth()
		availableWidth := termWidth - len(prefix) - len(connector)
		if availableWidth < 40 {
			availableWidth = 40
		}

		// Check if all files have the same extension (strip if so, like Python)
		stripExt := ""
		if len(fileNodes) > 1 {
			extSet := make(map[string]bool)
			for _, f := range fileNodes {
				extSet[f.file.Ext] = true
			}
			if len(extSet) == 1 {
				for ext := range extSet {
					stripExt = ext
				}
			}
		}

		// Build file entries with colors
		type fileEntry struct {
			display string
			colored string
			width   int
		}
		var entries []fileEntry
		for _, f := range fileNodes {
			color := GetFileColor(f.file.Ext)
			displayName := f.name
			// Strip extension if all files have same extension
			if stripExt != "" {
				displayName = strings.TrimSuffix(displayName, stripExt)
				if displayName == "" {
					displayName = f.name // Keep original if stripping leaves empty
				}
			}

			// Prefix: diff status indicator OR star for large files
			prefix := ""
			prefixWidth := 0
			if f.file.IsNew {
				prefix = "(new) "
				prefixWidth = 6
				color = Bold + Green
			} else if f.file.Added > 0 || f.file.Removed > 0 {
				prefix = "✎ "
				prefixWidth = 3
				color = Bold + Yellow
			} else if topLarge[f.file.Path] {
				prefix = "⭐️ "
				prefixWidth = 3
				color = Bold + color
			}

			// Warning for large files (>8k tokens)
			tokenWarning := ""
			tokenWarningWidth := 0
			if f.file.Tokens >= scanner.LargeFileTokens {
				tokenWarning = " [!]"
				tokenWarningWidth = 4
			}

			// Suffix: diff stats
			suffix := ""
			suffixWidth := 0
			if f.file.IsNew && f.file.Added > 0 {
				// New file: just show total lines
				suffix = fmt.Sprintf(" (+%d)", f.file.Added)
				suffixWidth = len(suffix)
			} else if f.file.Added > 0 || f.file.Removed > 0 {
				// Modified file: show +/-
				if f.file.Removed > 0 {
					suffix = fmt.Sprintf(" (+%d -%d)", f.file.Added, f.file.Removed)
				} else {
					suffix = fmt.Sprintf(" (+%d)", f.file.Added)
				}
				suffixWidth = len(suffix)
			}

			display := prefix + displayName + suffix + tokenWarning
			colored := fmt.Sprintf("%s%s%s%s%s%s%s%s", color, prefix, displayName, Reset, Dim, suffix, Reset+Red+tokenWarning+Reset, "")
			width := prefixWidth + len(displayName) + suffixWidth + tokenWarningWidth
			entries = append(entries, fileEntry{display, colored, width})
		}

		// Calculate columns - find max width and fit columns
		maxWidth := 0
		for _, e := range entries {
			if e.width > maxWidth {
				maxWidth = e.width
			}
		}
		colWidth := maxWidth + 1
		numCols := availableWidth / colWidth
		if numCols < 1 {
			numCols = 1
		}
		if numCols > len(entries) {
			numCols = len(entries)
		}

		// Calculate number of rows
		numRows := (len(entries) + numCols - 1) / numCols

		// Print in column-major order (like Python)
		for row := 0; row < numRows; row++ {
			if row == 0 {
				fmt.Printf("%s%s", prefix, connector)
			} else {
				fmt.Printf("%s    ", prefix)
			}
			for col := 0; col < numCols; col++ {
				idx := col*numRows + row
				if idx < len(entries) {
					e := entries[idx]
					// Pad to column width
					padding := colWidth - e.width
					if padding < 0 {
						padding = 0
					}
					fmt.Printf("%s%s", e.colored, strings.Repeat(" ", padding))
				}
			}
			fmt.Println()
		}
	}
}
</file>
<file path="scanner/queries/bash.scm">
; Bash query for functions and source imports

; Function definitions
(function_definition
  name: (word) @function)

; source or . commands (imports)
(command
  name: (command_name) @_cmd
  argument: (word) @import
  (#match? @_cmd "^(source|\\.)$"))
</file>
<file path="scanner/queries/c.scm">
; C query for functions and imports

; Function definitions
(function_definition
  declarator: (function_declarator
    declarator: (identifier) @function))

; Function declarations (prototypes)
(declaration
  declarator: (function_declarator
    declarator: (identifier) @function))

; #include directives
(preproc_include
  path: (string_literal) @import)

(preproc_include
  path: (system_lib_string) @import)
</file>
<file path="scanner/queries/c_sharp.scm">
; C# query for methods, types, and using directives

; Method declarations with parameters
(method_declaration
  name: (identifier) @func.name
  parameters: (parameter_list) @func.params)

; Constructor declarations
(constructor_declaration
  name: (identifier) @func.name
  parameters: (parameter_list) @func.params)

; Class declarations
(class_declaration
  name: (identifier) @type.name) @type.class

; Interface declarations
(interface_declaration
  name: (identifier) @type.name) @type.interface

; Struct declarations
(struct_declaration
  name: (identifier) @type.name) @type.struct

; Enum declarations
(enum_declaration
  name: (identifier) @type.name) @type.enum

; Using directives
(using_directive
  (qualified_name) @import)

(using_directive
  (identifier) @import)
</file>
<file path="scanner/queries/cpp.scm">
; C++ query for functions and imports

; Function definitions
(function_definition
  declarator: (function_declarator
    declarator: (identifier) @function))

; Method definitions
(function_definition
  declarator: (function_declarator
    declarator: (qualified_identifier
      name: (identifier) @function)))

; #include directives
(preproc_include
  path: (string_literal) @import)

(preproc_include
  path: (system_lib_string) @import)
</file>
<file path="scanner/queries/dart.scm">
; Dart query for functions and imports

; Function signatures (top-level and class methods)
(function_signature
  name: (identifier) @function)

; Getter signatures
(getter_signature
  (identifier) @function)

; Setter signatures
(setter_signature
  name: (identifier) @function)

; Import statements - capture the string path
(import_specification
  (configurable_uri
    (uri
      (string_literal) @import)))

; Also try library_import structure
(library_import
  (import_specification
    (configurable_uri
      (uri
        (string_literal) @import))))
</file>
<file path="scanner/queries/go.scm">
; Go query for extracting functions, types, and imports

; Function declarations with parameters
(function_declaration
  name: (identifier) @func.name
  parameters: (parameter_list) @func.params)

; Method declarations (functions with receivers)
(method_declaration
  receiver: (parameter_list) @func.receiver
  name: (field_identifier) @func.name
  parameters: (parameter_list) @func.params)

; Struct type definitions
(type_declaration
  (type_spec
    name: (type_identifier) @type.name
    type: (struct_type))) @type.struct

; Interface type definitions
(type_declaration
  (type_spec
    name: (type_identifier) @type.name
    type: (interface_type))) @type.interface

; Import paths
(import_spec
  path: (interpreted_string_literal) @import)
</file>
<file path="scanner/queries/java.scm">
; Java query for functions, types, and imports

; Method declarations with parameters
(method_declaration
  name: (identifier) @func.name
  parameters: (formal_parameters) @func.params)

; Constructor declarations
(constructor_declaration
  name: (identifier) @func.name
  parameters: (formal_parameters) @func.params)

; Class declarations
(class_declaration
  name: (identifier) @type.name) @type.class

; Interface declarations
(interface_declaration
  name: (identifier) @type.name) @type.interface

; Enum declarations
(enum_declaration
  name: (identifier) @type.name) @type.enum

; Import declarations
(import_declaration
  (scoped_identifier) @import)
</file>
<file path="scanner/queries/javascript.scm">
; JavaScript/JSX query for functions, classes, and imports

; Function declarations with parameters
(function_declaration
  name: (identifier) @func.name
  parameters: (formal_parameters) @func.params)

; Arrow functions assigned to variables
(variable_declarator
  name: (identifier) @func.name
  value: (arrow_function
    parameters: (formal_parameters) @func.params))

; Method definitions in classes/objects
(method_definition
  name: (property_identifier) @func.name
  parameters: (formal_parameters) @func.params)

; Class declarations
(class_declaration
  name: (identifier) @type.name) @type.class

; ES6 imports: import x from 'y'
(import_statement
  source: (string) @import)

; CommonJS: require('x')
(call_expression
  function: (identifier) @_req (#eq? @_req "require")
  arguments: (arguments (string) @import))
</file>
<file path="scanner/queries/kotlin.scm">
; Kotlin query for functions and imports

; Function declarations
(function_declaration
  (simple_identifier) @function)

; Import statements
(import_header
  (identifier) @import)
</file>
<file path="scanner/queries/php.scm">
; PHP query for functions and imports

; Function definitions
(function_definition
  name: (name) @function)

; Method declarations
(method_declaration
  name: (name) @function)

; Use statements (imports)
(namespace_use_clause
  (qualified_name) @import)

; Require/include statements
(include_expression
  (string) @import)

(include_once_expression
  (string) @import)

(require_expression
  (string) @import)

(require_once_expression
  (string) @import)
</file>
<file path="scanner/queries/python.scm">
; Python query for extracting functions, classes, and imports

; Function definitions with parameters
(function_definition
  name: (identifier) @func.name
  parameters: (parameters) @func.params)

; Class definitions
(class_definition
  name: (identifier) @type.name) @type.class

; import x, import x.y.z
(import_statement
  name: (dotted_name) @import)

; from x import y
(import_from_statement
  module_name: (dotted_name) @import)

; from x import y, z (the module part)
(import_from_statement
  module_name: (relative_import) @import)
</file>
<file path="scanner/queries/r.scm">
; R query for functions and imports

; Function definitions using <- assignment
(binary_operator
  lhs: (identifier) @function
  operator: "<-"
  rhs: (function_definition))

; Function definitions using = assignment
(binary_operator
  lhs: (identifier) @function
  operator: "="
  rhs: (function_definition))

; library() calls
(call
  function: (identifier) @_fn
  arguments: (arguments
    (argument
      value: (identifier) @import))
  (#eq? @_fn "library"))

; require() calls
(call
  function: (identifier) @_fn
  arguments: (arguments
    (argument
      value: (identifier) @import))
  (#eq? @_fn "require"))

; source() calls for file imports
(call
  function: (identifier) @_fn
  arguments: (arguments
    (argument
      value: (string) @import))
  (#eq? @_fn "source"))
</file>
<file path="scanner/queries/ruby.scm">
; Ruby query for functions and imports

; Method definitions
(method
  name: (identifier) @function)

; Singleton method definitions
(singleton_method
  name: (identifier) @function)

; require 'x'
(call
  method: (identifier) @_req (#match? @_req "^require")
  arguments: (argument_list (string) @import))

; require_relative 'x'
(call
  method: (identifier) @_req (#match? @_req "^require_relative")
  arguments: (argument_list (string) @import))
</file>
<file path="scanner/queries/rust.scm">
; Rust query for functions, types, and imports

; Function definitions with parameters
(function_item
  name: (identifier) @func.name
  parameters: (parameters) @func.params)

; Struct definitions
(struct_item
  name: (type_identifier) @type.name) @type.struct

; Enum definitions
(enum_item
  name: (type_identifier) @type.name) @type.enum

; Trait definitions
(trait_item
  name: (type_identifier) @type.name) @type.trait

; use statements
(use_declaration
  argument: (scoped_identifier) @import)

(use_declaration
  argument: (identifier) @import)

; mod declarations (internal modules)
(mod_item
  name: (identifier) @module)
</file>
<file path="scanner/queries/swift.scm">
; Swift query for functions and imports

; Function declarations - capture the name
(function_declaration
  name: (simple_identifier) @function)

; Init declarations - no name field, so we skip them
; (they show as "init" which isn't very useful)

; Import statements - capture the identifier
(import_declaration
  (identifier
    (simple_identifier) @import))
</file>
<file path="scanner/queries/typescript.scm">
; TypeScript/TSX query for functions, types, and imports

; Function declarations with parameters
(function_declaration
  name: (identifier) @func.name
  parameters: (formal_parameters) @func.params)

; Arrow functions assigned to variables
(variable_declarator
  name: (identifier) @func.name
  value: (arrow_function
    parameters: (formal_parameters) @func.params))

; Method definitions
(method_definition
  name: (property_identifier) @func.name
  parameters: (formal_parameters) @func.params)

; Interface declarations
(interface_declaration
  name: (type_identifier) @type.name) @type.interface

; Class declarations
(class_declaration
  name: (type_identifier) @type.name) @type.class

; Type aliases
(type_alias_declaration
  name: (type_identifier) @type.name) @type.alias

; Enum declarations
(enum_declaration
  name: (identifier) @type.name) @type.enum

; ES6 imports
(import_statement
  source: (string) @import)
</file>
<file path="scanner/testdata/corpus/go/expected.json">
{
  "description": "Expected graph for Go test corpus",
  "nodes": {
    "count": 15,
    "files": ["main.go", "types.go"],
    "functions": [
      "main", "hello", "add", "process", "helper", "nested",
      "NewUser", "Greet", "AddUser", "ProcessAll"
    ],
    "types": ["User", "Service"],
    "imports": ["fmt"]
  },
  "edges": {
    "count": 19,
    "calls": [
      {"from": "main", "to": "hello"},
      {"from": "main", "to": "add"},
      {"from": "main", "to": "process"},
      {"from": "process", "to": "helper"},
      {"from": "helper", "to": "nested"}
    ]
  },
  "notes": "Cross-file method calls (Greet->hello) are not yet captured"
}
</file>
<file path="scanner/testdata/corpus/go/main.go">
// Package main is a test corpus for validating call graph extraction.
package main

import (
	"fmt"
)

// main is the entry point - calls multiple functions.
func main() {
	greeting := hello("World")
	fmt.Println(greeting)

	result := add(1, 2)
	fmt.Printf("Result: %d\n", result)

	process()
}

// hello returns a greeting string.
func hello(name string) string {
	return "Hello, " + name
}

// add returns the sum of two integers.
func add(a, b int) int {
	return a + b
}

// process demonstrates nested calls.
func process() {
	helper()
}

// helper is called by process.
func helper() {
	nested()
}

// nested is the deepest in the call chain.
func nested() {
	fmt.Println("nested called")
}
</file>
<file path="scanner/testdata/corpus/go/types.go">
// Package main demonstrates method calls and type definitions.
package main

// User represents a user entity.
type User struct {
	Name  string
	Email string
}

// NewUser creates a new User instance.
func NewUser(name, email string) *User {
	return &User{Name: name, Email: email}
}

// Greet returns a greeting for the user.
func (u *User) Greet() string {
	return hello(u.Name)
}

// Service handles user operations.
type Service struct {
	users []*User
}

// AddUser adds a user to the service.
func (s *Service) AddUser(u *User) {
	s.users = append(s.users, u)
}

// ProcessAll processes all users - calls variadic function.
func (s *Service) ProcessAll(opts ...string) {
	for _, u := range s.users {
		_ = u.Greet()
	}
}
</file>
<file path="scanner/testdata/corpus/python/classes.py">
"""Test corpus demonstrating Python classes and method calls."""

from typing import List, Optional


class User:
    """Represents a user entity."""

    def __init__(self, name: str, email: str):
        """Initialize user with name and email."""
        self.name = name
        self.email = email

    def greet(self) -> str:
        """Returns a greeting for the user."""
        from main import hello
        return hello(self.name)


class Service:
    """Handles user operations."""

    def __init__(self):
        """Initialize service with empty user list."""
        self.users: List[User] = []

    def add_user(self, user: User) -> None:
        """Adds a user to the service."""
        self.users.append(user)

    def process_all(self) -> None:
        """Processes all users."""
        for user in self.users:
            _ = user.greet()

    def _private_method(self) -> None:
        """Private method - should not be exported."""
        pass


def create_user(name: str, email: str) -> User:
    """Factory function for creating users."""
    return User(name, email)
</file>
<file path="scanner/testdata/corpus/python/expected.json">
{
  "description": "Expected graph for Python test corpus",
  "nodes": {
    "count": 19,
    "files": ["main.py", "classes.py"],
    "functions": [
      "main", "hello", "add", "process", "helper", "nested", "variadic_func",
      "__init__", "greet", "add_user", "process_all", "_private_method", "create_user"
    ],
    "types": ["User", "Service"]
  },
  "edges": {
    "count": 23,
    "calls": [
      {"from": "main", "to": "hello"},
      {"from": "main", "to": "add"},
      {"from": "main", "to": "process"},
      {"from": "process", "to": "helper"},
      {"from": "helper", "to": "nested"}
    ]
  },
  "notes": "Cross-file method calls are not yet captured"
}
</file>
<file path="scanner/testdata/corpus/python/main.py">
"""Test corpus for validating Python call graph extraction."""

from typing import List


def main():
    """Entry point - calls multiple functions."""
    greeting = hello("World")
    print(greeting)

    result = add(1, 2)
    print(f"Result: {result}")

    process()


def hello(name: str) -> str:
    """Returns a greeting string."""
    return f"Hello, {name}"


def add(a: int, b: int) -> int:
    """Returns the sum of two integers."""
    return a + b


def process():
    """Demonstrates nested calls."""
    helper()


def helper():
    """Called by process."""
    nested()


def nested():
    """Deepest in the call chain."""
    print("nested called")


def variadic_func(*args, **kwargs):
    """A variadic function for testing arity."""
    pass


if __name__ == "__main__":
    main()
</file>
<file path="scanner/testdata/corpus/typescript/expected.json">
{
  "description": "Expected graph for TypeScript test corpus",
  "nodes": {
    "count": 18,
    "files": ["main.ts", "types.ts"],
    "functions": [
      "main", "hello", "add", "process", "helper", "nested", "variadicFunc",
      "constructor", "greet", "addUser", "processAll", "createUser"
    ],
    "types": ["User", "Service"],
    "interfaces": ["IUser"]
  },
  "edges": {
    "count": 22,
    "calls": [
      {"from": "main", "to": "hello"},
      {"from": "main", "to": "add"},
      {"from": "main", "to": "process"},
      {"from": "process", "to": "helper"},
      {"from": "helper", "to": "nested"}
    ]
  },
  "notes": "Cross-file calls and class instantiation are not yet captured"
}
</file>
<file path="scanner/testdata/corpus/typescript/main.ts">
/**
 * Test corpus for validating TypeScript call graph extraction.
 */

/**
 * Entry point - calls multiple functions.
 */
function main(): void {
  const greeting = hello("World");
  console.log(greeting);

  const result = add(1, 2);
  console.log(`Result: ${result}`);

  process();
}

/**
 * Returns a greeting string.
 */
function hello(name: string): string {
  return `Hello, ${name}`;
}

/**
 * Returns the sum of two integers.
 */
function add(a: number, b: number): number {
  return a + b;
}

/**
 * Demonstrates nested calls.
 */
function process(): void {
  helper();
}

/**
 * Called by process.
 */
function helper(): void {
  nested();
}

/**
 * Deepest in the call chain.
 */
function nested(): void {
  console.log("nested called");
}

/**
 * A variadic function for testing arity.
 */
function variadicFunc(...args: string[]): void {
  // No-op
}

main();
</file>
<file path="scanner/testdata/corpus/typescript/types.ts">
/**
 * Test corpus demonstrating TypeScript classes and interfaces.
 */

import { hello } from "./main";

/**
 * User entity interface.
 */
interface IUser {
  name: string;
  email: string;
  greet(): string;
}

/**
 * Represents a user entity.
 */
class User implements IUser {
  constructor(public name: string, public email: string) {}

  /**
   * Returns a greeting for the user.
   */
  greet(): string {
    return hello(this.name);
  }
}

/**
 * Handles user operations.
 */
class Service {
  private users: User[] = [];

  /**
   * Adds a user to the service.
   */
  addUser(user: User): void {
    this.users.push(user);
  }

  /**
   * Processes all users.
   */
  processAll(): void {
    for (const user of this.users) {
      user.greet();
    }
  }
}

/**
 * Factory function for creating users.
 */
function createUser(name: string, email: string): User {
  return new User(name, email);
}

export { User, Service, IUser, createUser };
</file>
<file path="scanner/testdata/corpus/corpus_test.go">
package corpus_test

import (
	"encoding/json"
	"os"
	"path/filepath"
	"testing"

	"codemap/graph"
)

// Expected describes the expected graph structure for a test corpus.
type Expected struct {
	Description string `json:"description"`
	Nodes       struct {
		Count     int      `json:"count"`
		Files     []string `json:"files"`
		Functions []string `json:"functions"`
		Types     []string `json:"types,omitempty"`
	} `json:"nodes"`
	Edges struct {
		Count int `json:"count"`
		Calls []struct {
			From string `json:"from"`
			To   string `json:"to"`
		} `json:"calls"`
	} `json:"edges"`
}

func TestGoCorpus(t *testing.T) {
	testCorpus(t, "go")
}

func TestPythonCorpus(t *testing.T) {
	testCorpus(t, "python")
}

func TestTypeScriptCorpus(t *testing.T) {
	testCorpus(t, "typescript")
}

func testCorpus(t *testing.T, lang string) {
	corpusDir := filepath.Join(".", lang)
	graphPath := filepath.Join(corpusDir, ".codemap", "graph.gob")

	// Load expected
	expectedPath := filepath.Join(corpusDir, "expected.json")
	expectedData, err := os.ReadFile(expectedPath)
	if err != nil {
		t.Fatalf("Failed to read expected.json: %v", err)
	}
	var expected Expected
	if err := json.Unmarshal(expectedData, &expected); err != nil {
		t.Fatalf("Failed to parse expected.json: %v", err)
	}

	// Load graph
	g, err := graph.LoadBinary(graphPath)
	if err != nil {
		t.Fatalf("Failed to load graph: %v", err)
	}

	// Verify node count
	if g.NodeCount != expected.Nodes.Count {
		t.Errorf("Node count mismatch: got %d, want %d", g.NodeCount, expected.Nodes.Count)
	}

	// Verify edge count
	if g.EdgeCount != expected.Edges.Count {
		t.Errorf("Edge count mismatch: got %d, want %d", g.EdgeCount, expected.Edges.Count)
	}

	// Verify files exist
	for _, file := range expected.Nodes.Files {
		nodes := g.GetNodesByPath(file)
		if len(nodes) == 0 {
			t.Errorf("Expected file node not found: %s", file)
		}
	}

	// Verify functions exist
	for _, fn := range expected.Nodes.Functions {
		nodes := g.GetNodesByName(fn)
		if len(nodes) == 0 {
			t.Errorf("Expected function not found: %s", fn)
		}
	}

	// Verify types exist
	for _, typ := range expected.Nodes.Types {
		nodes := g.GetNodesByName(typ)
		if len(nodes) == 0 {
			t.Errorf("Expected type not found: %s", typ)
		}
	}

	// Verify expected call edges exist
	for _, call := range expected.Edges.Calls {
		found := false
		fromNodes := g.GetNodesByName(call.From)
		for _, fromNode := range fromNodes {
			edges := g.GetOutgoingEdges(fromNode.ID)
			for _, edge := range edges {
				if edge.Kind == graph.EdgeCalls {
					toNode := g.GetNode(edge.To)
					if toNode != nil && toNode.Name == call.To {
						found = true
						break
					}
				}
			}
			if found {
				break
			}
		}
		if !found {
			t.Errorf("Expected call edge not found: %s -> %s", call.From, call.To)
		}
	}
}
</file>
<file path="scanner/build-grammars.sh">
#!/usr/bin/env bash
set -e

# Build tree-sitter grammar shared libraries for codemap --deps mode
# Requires: git, cc (clang/gcc)

SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
GRAMMAR_DIR="$SCRIPT_DIR/grammars"
BUILD_DIR="$SCRIPT_DIR/.grammar-build"

# Detect OS for library extension
if [[ "$OSTYPE" == "darwin"* ]]; then
    LIB_EXT=".dylib"
    CC_FLAGS="-dynamiclib"
elif [[ "$OSTYPE" == "msys"* ]] || [[ "$OSTYPE" == "cygwin"* ]]; then
    LIB_EXT=".dll"
    CC_FLAGS="-shared"
else
    LIB_EXT=".so"
    CC_FLAGS="-shared -fPIC"
fi

mkdir -p "$GRAMMAR_DIR"
mkdir -p "$BUILD_DIR"

build_grammar() {
    local lang=$1
    local repo=$2
    local src_subdir=${3:-src}
    local output="$GRAMMAR_DIR/libtree-sitter-${lang}${LIB_EXT}"

    if [[ -f "$output" ]]; then
        echo "✓ $lang (already built)"
        return 0
    fi

    echo "Building $lang..."

    local clone_dir="$BUILD_DIR/tree-sitter-$lang"

    # Clone if needed
    if [[ ! -d "$clone_dir" ]]; then
        git clone --depth 1 "$repo" "$clone_dir" 2>/dev/null || {
            echo "✗ $lang (clone failed)"
            return 1
        }
    fi

    local src_dir="$clone_dir/$src_subdir"

    # Compile parser.c (and scanner.c if exists)
    local sources="$src_dir/parser.c"
    if [[ -f "$src_dir/scanner.c" ]]; then
        sources="$sources $src_dir/scanner.c"
    elif [[ -f "$src_dir/scanner.cc" ]]; then
        # C++ scanner
        c++ -c -fPIC "$src_dir/scanner.cc" -o "$BUILD_DIR/scanner_${lang}.o" -I "$src_dir" 2>/dev/null || {
            echo "✗ $lang (c++ compile failed)"
            return 1
        }
        cc $CC_FLAGS -o "$output" $sources "$BUILD_DIR/scanner_${lang}.o" -I "$src_dir" 2>/dev/null || {
            echo "✗ $lang (link failed)"
            return 1
        }
        echo "✓ $lang"
        return 0
    fi

    cc $CC_FLAGS -o "$output" $sources -I "$src_dir" 2>/dev/null || {
        echo "✗ $lang (compile failed)"
        return 1
    }
    echo "✓ $lang"
}

echo "Building tree-sitter grammars..."
echo "Output: $GRAMMAR_DIR"
echo ""

# Build each grammar (lang, repo, src_subdir)
build_grammar "go" "https://github.com/tree-sitter/tree-sitter-go" "src"
build_grammar "python" "https://github.com/tree-sitter/tree-sitter-python" "src"
build_grammar "javascript" "https://github.com/tree-sitter/tree-sitter-javascript" "src"
build_grammar "typescript" "https://github.com/tree-sitter/tree-sitter-typescript" "typescript/src"
build_grammar "rust" "https://github.com/tree-sitter/tree-sitter-rust" "src"
build_grammar "ruby" "https://github.com/tree-sitter/tree-sitter-ruby" "src"
build_grammar "c" "https://github.com/tree-sitter/tree-sitter-c" "src"
build_grammar "cpp" "https://github.com/tree-sitter/tree-sitter-cpp" "src"
build_grammar "java" "https://github.com/tree-sitter/tree-sitter-java" "src"
build_grammar "swift" "https://github.com/tree-sitter/tree-sitter-swift" "src"
build_grammar "bash" "https://github.com/tree-sitter/tree-sitter-bash" "src"
build_grammar "kotlin" "https://github.com/fwcd/tree-sitter-kotlin" "src"
build_grammar "c_sharp" "https://github.com/tree-sitter/tree-sitter-c-sharp" "src"
build_grammar "php" "https://github.com/tree-sitter/tree-sitter-php" "php/src"
build_grammar "dart" "https://github.com/UserNobody14/tree-sitter-dart" "src"
build_grammar "r" "https://github.com/r-lib/tree-sitter-r" "src"

echo ""
echo "Done. Built grammars:"
ls -1 "$GRAMMAR_DIR"/*.dylib "$GRAMMAR_DIR"/*.so "$GRAMMAR_DIR"/*.dll 2>/dev/null || echo "(none)"
</file>
<file path="scanner/calls.go">
package scanner

import (
	"os"
	"strings"

	tree_sitter "github.com/tree-sitter/go-tree-sitter"
)

// CallInfo represents a function call site.
type CallInfo struct {
	CallerFunc string `json:"caller"`             // Name of the function containing the call
	CallerLine int    `json:"caller_line"`        // Line where caller is defined
	CalleeName string `json:"callee"`             // Name of the called function
	CallLine   int    `json:"call_line"`          // Line where the call occurs
	Args       int    `json:"args"`               // Number of arguments
	Receiver   string `json:"receiver,omitempty"` // Object/receiver for method calls
}

// FileCallAnalysis contains all calls found in a file.
type FileCallAnalysis struct {
	Path     string     `json:"path"`
	Language string     `json:"language"`
	Calls    []CallInfo `json:"calls"`
}

// callQueryPatterns maps languages to their call expression query patterns.
// These are simpler inline patterns for common languages.
var callQueryPatterns = map[string]string{
	"go": `
; Function calls
(call_expression
  function: (identifier) @call.name
  arguments: (argument_list) @call.args)

; Method calls
(call_expression
  function: (selector_expression
    operand: (_) @call.receiver
    field: (field_identifier) @call.name)
  arguments: (argument_list) @call.args)
`,
	"python": `
; Function calls
(call
  function: (identifier) @call.name
  arguments: (argument_list) @call.args)

; Method calls
(call
  function: (attribute
    object: (_) @call.receiver
    attribute: (identifier) @call.name)
  arguments: (argument_list) @call.args)
`,
	"javascript": `
; Function calls
(call_expression
  function: (identifier) @call.name
  arguments: (arguments) @call.args)

; Method calls
(call_expression
  function: (member_expression
    object: (_) @call.receiver
    property: (property_identifier) @call.name)
  arguments: (arguments) @call.args)
`,
	"typescript": `
; Function calls
(call_expression
  function: (identifier) @call.name
  arguments: (arguments) @call.args)

; Method calls
(call_expression
  function: (member_expression
    object: (_) @call.receiver
    property: (property_identifier) @call.name)
  arguments: (arguments) @call.args)
`,
	"rust": `
; Function calls
(call_expression
  function: (identifier) @call.name
  arguments: (arguments) @call.args)

; Method calls
(call_expression
  function: (field_expression
    value: (_) @call.receiver
    field: (field_identifier) @call.name)
  arguments: (arguments) @call.args)
`,
	"java": `
; Method invocations
(method_invocation
  name: (identifier) @call.name
  arguments: (argument_list) @call.args)

; Method invocations with object
(method_invocation
  object: (_) @call.receiver
  name: (identifier) @call.name
  arguments: (argument_list) @call.args)
`,
}

// callConfigs caches compiled call queries per language.
var callConfigs = make(map[string]*tree_sitter.Query)

// ExtractCalls analyzes a file and extracts all function/method calls.
func (l *GrammarLoader) ExtractCalls(filePath string) (*FileCallAnalysis, error) {
	lang := DetectLanguage(filePath)
	if lang == "" {
		return nil, nil
	}

	// Check if we have a call query for this language first
	if _, ok := callQueryPatterns[lang]; !ok {
		return nil, nil // No call extraction support for this language
	}

	if err := l.LoadLanguage(lang); err != nil {
		return nil, nil
	}

	config := l.configs[lang]
	if config == nil || config.Language == nil {
		return nil, nil
	}

	content, err := os.ReadFile(filePath)
	if err != nil {
		return nil, err
	}

	// Get or compile the call query
	callQuery, err := l.getCallQuery(lang, config.Language)
	if err != nil || callQuery == nil {
		return nil, nil // No call query for this language
	}

	parser := tree_sitter.NewParser()
	defer parser.Close()
	parser.SetLanguage(config.Language)

	tree := parser.Parse(content, nil)
	defer tree.Close()

	cursor := tree_sitter.NewQueryCursor()
	defer cursor.Close()

	analysis := &FileCallAnalysis{Path: filePath, Language: lang}

	// First, build a map of line ranges to function names
	funcRanges := l.extractFunctionRanges(tree.RootNode(), content, config.Query)

	// Extract calls
	matches := cursor.Matches(callQuery, tree.RootNode(), content)

	var currentCall *CallInfo
	lastMatchID := uint(0xFFFFFFFF) // Use max value to ensure first match triggers initialization

	for match := matches.Next(); match != nil; match = matches.Next() {
		// New match = new call
		if lastMatchID != match.Id() {
			if currentCall != nil && currentCall.CalleeName != "" {
				// Find containing function
				currentCall.CallerFunc = findContainingFunction(currentCall.CallLine, funcRanges)
				analysis.Calls = append(analysis.Calls, *currentCall)
			}
			currentCall = &CallInfo{}
			lastMatchID = match.Id()
		}

		if currentCall == nil {
			currentCall = &CallInfo{}
		}

		for _, capture := range match.Captures {
			captureName := callQuery.CaptureNames()[capture.Index]
			text := capture.Node.Utf8Text(content)
			line := int(capture.Node.StartPosition().Row) + 1

			switch captureName {
			case "call.name":
				currentCall.CalleeName = text
				currentCall.CallLine = line
			case "call.receiver":
				currentCall.Receiver = text
			case "call.args":
				// Count arguments by counting commas + 1 (if not empty)
				currentCall.Args = countArgs(text)
			}
		}
	}

	// Don't forget last call
	if currentCall != nil && currentCall.CalleeName != "" {
		currentCall.CallerFunc = findContainingFunction(currentCall.CallLine, funcRanges)
		analysis.Calls = append(analysis.Calls, *currentCall)
	}

	return analysis, nil
}

// funcRange represents a function's line range for caller detection.
type funcRange struct {
	name      string
	startLine int
	endLine   int
}

// extractFunctionRanges builds a list of function ranges from the AST.
func (l *GrammarLoader) extractFunctionRanges(root *tree_sitter.Node, content []byte, query *tree_sitter.Query) []funcRange {
	var ranges []funcRange

	cursor := tree_sitter.NewQueryCursor()
	defer cursor.Close()

	matches := cursor.Matches(query, root, content)

	var currentFunc *funcRange

	for match := matches.Next(); match != nil; match = matches.Next() {
		for _, capture := range match.Captures {
			captureName := query.CaptureNames()[capture.Index]

			if strings.HasPrefix(captureName, "func.name") || captureName == "function" || captureName == "method" {
				// Get the function node (parent of name)
				funcNode := capture.Node.Parent()
				if funcNode != nil {
					currentFunc = &funcRange{
						name:      capture.Node.Utf8Text(content),
						startLine: int(funcNode.StartPosition().Row) + 1,
						endLine:   int(funcNode.EndPosition().Row) + 1,
					}
					ranges = append(ranges, *currentFunc)
				}
			}
		}
	}

	return ranges
}

// getCallQuery returns the compiled call query for a language.
func (l *GrammarLoader) getCallQuery(lang string, tsLang *tree_sitter.Language) (*tree_sitter.Query, error) {
	if q, ok := callConfigs[lang]; ok {
		return q, nil
	}

	pattern, ok := callQueryPatterns[lang]
	if !ok {
		return nil, nil // No call query for this language
	}

	query, err := tree_sitter.NewQuery(tsLang, pattern)
	if err != nil {
		return nil, err
	}

	callConfigs[lang] = query
	return query, nil
}

// findContainingFunction finds which function contains a given line.
func findContainingFunction(line int, ranges []funcRange) string {
	for i := len(ranges) - 1; i >= 0; i-- {
		r := ranges[i]
		if line >= r.startLine && line <= r.endLine {
			return r.name
		}
	}
	return "" // Global scope
}

// countArgs counts the number of arguments in an argument list.
func countArgs(argsText string) int {
	// Remove outer parens/brackets
	argsText = strings.TrimSpace(argsText)
	if len(argsText) < 2 {
		return 0
	}
	inner := argsText[1 : len(argsText)-1]
	inner = strings.TrimSpace(inner)
	if inner == "" {
		return 0
	}
	// Simple comma count (doesn't handle nested commas perfectly)
	return strings.Count(inner, ",") + 1
}
</file>
<file path="scanner/deps.go">
package scanner

import (
	"os"
	"path/filepath"
	"strings"
)

// ReadExternalDeps reads manifest files (go.mod, requirements.txt, package.json)
func ReadExternalDeps(root string) map[string][]string {
	deps := make(map[string][]string)

	// Walk tree to find all manifest files
	filepath.Walk(root, func(path string, info os.FileInfo, _ error) error {
		if info == nil {
			return nil
		}
		if info.IsDir() {
			if IgnoredDirs[info.Name()] {
				return filepath.SkipDir
			}
			return nil
		}
		switch info.Name() {
		case "go.mod":
			if c, err := os.ReadFile(path); err == nil {
				deps["go"] = append(deps["go"], parseGoMod(string(c))...)
			}
		case "requirements.txt":
			if c, err := os.ReadFile(path); err == nil {
				deps["python"] = append(deps["python"], parseRequirements(string(c))...)
			}
		case "package.json":
			if c, err := os.ReadFile(path); err == nil {
				deps["javascript"] = append(deps["javascript"], parsePackageJson(string(c))...)
			}
		case "Podfile":
			if c, err := os.ReadFile(path); err == nil {
				deps["swift"] = append(deps["swift"], parsePodfile(string(c))...)
			}
		case "Package.swift":
			if c, err := os.ReadFile(path); err == nil {
				deps["swift"] = append(deps["swift"], parsePackageSwift(string(c))...)
			}
		}
		return nil
	})

	for k, v := range deps {
		deps[k] = dedupe(v)
	}
	return deps
}

func parseGoMod(c string) (deps []string) {
	inReq := false
	for _, line := range strings.Split(c, "\n") {
		line = strings.TrimSpace(line)
		if strings.HasPrefix(line, "require (") {
			inReq = true
		} else if inReq && line == ")" {
			inReq = false
		} else if inReq && line != "" && !strings.HasPrefix(line, "//") {
			if parts := strings.Fields(line); len(parts) >= 1 {
				deps = append(deps, parts[0])
			}
		}
	}
	return
}

func parseRequirements(c string) (deps []string) {
	for _, line := range strings.Split(c, "\n") {
		line = strings.TrimSpace(line)
		if line == "" || strings.HasPrefix(line, "#") {
			continue
		}
		for _, sep := range []string{"==", ">=", "<=", "~=", "<", ">", "[", ";", "#"} {
			if i := strings.Index(line, sep); i != -1 {
				line = line[:i]
			}
		}
		if line != "" {
			deps = append(deps, line)
		}
	}
	return
}

func parsePackageJson(c string) (deps []string) {
	inDeps := false
	for _, line := range strings.Split(c, "\n") {
		if strings.Contains(line, `"dependencies"`) || strings.Contains(line, `"devDependencies"`) {
			inDeps = true
		} else if inDeps && strings.Contains(line, "}") {
			inDeps = false
		} else if inDeps && strings.Contains(line, ":") {
			parts := strings.SplitN(line, ":", 2)
			name := strings.Trim(strings.TrimSpace(parts[0]), `"`)
			if name != "" {
				deps = append(deps, name)
			}
		}
	}
	return
}

func parsePodfile(c string) (deps []string) {
	// Parse Podfile: pod 'Name' or pod 'Name', '~> 1.0'
	for _, line := range strings.Split(c, "\n") {
		line = strings.TrimSpace(line)
		if strings.HasPrefix(line, "pod ") {
			// Extract pod name from: pod 'Name' or pod 'Name', ...
			line = strings.TrimPrefix(line, "pod ")
			line = strings.Trim(line, "'\"")
			if i := strings.Index(line, "'"); i != -1 {
				line = line[:i]
			}
			if i := strings.Index(line, "\""); i != -1 {
				line = line[:i]
			}
			if i := strings.Index(line, ","); i != -1 {
				line = line[:i]
			}
			line = strings.Trim(line, "'\" ")
			if line != "" {
				deps = append(deps, line)
			}
		}
	}
	return
}

func parsePackageSwift(c string) (deps []string) {
	// Parse Package.swift: .package(url: "...", ...) or .package(name: "Name", ...)
	// Look for package names in .product(name: "Name", package: "Package")
	for _, line := range strings.Split(c, "\n") {
		// Match .package(url: "https://github.com/user/repo", ...)
		if strings.Contains(line, ".package(") && strings.Contains(line, "url:") {
			// Extract repo name from URL
			if i := strings.Index(line, "url:"); i != -1 {
				rest := line[i+4:]
				rest = strings.Trim(rest, " \"'")
				if j := strings.Index(rest, "\""); j != -1 {
					url := rest[:j]
					// Get repo name from URL
					parts := strings.Split(url, "/")
					if len(parts) > 0 {
						name := parts[len(parts)-1]
						name = strings.TrimSuffix(name, ".git")
						if name != "" {
							deps = append(deps, name)
						}
					}
				}
			}
		}
	}
	return
}
</file>
<file path="scanner/git.go">
package scanner

import (
	"fmt"
	"os"
	"os/exec"
	"path/filepath"
	"sort"
	"strings"
)

// DiffInfo holds all diff-related data for changed files
type DiffInfo struct {
	Changed   map[string]bool     // all changed files (modified + untracked)
	Untracked map[string]bool     // new/untracked files only
	Stats     map[string]DiffStat // +/- line counts
}

// GitDiffInfo returns comprehensive diff information for the repo
func GitDiffInfo(root, ref string) (*DiffInfo, error) {
	info := &DiffInfo{
		Changed:   make(map[string]bool),
		Untracked: make(map[string]bool),
		Stats:     make(map[string]DiffStat),
	}

	// Get modified files vs ref with stats
	cmd := exec.Command("git", "diff", "--numstat", ref)
	cmd.Dir = root
	output, err := cmd.Output()
	if err != nil {
		return nil, err
	}

	for _, line := range strings.Split(strings.TrimSpace(string(output)), "\n") {
		if line == "" {
			continue
		}
		parts := strings.Fields(line)
		if len(parts) >= 3 {
			var added, removed int
			if parts[0] != "-" {
				fmt.Sscanf(parts[0], "%d", &added)
			}
			if parts[1] != "-" {
				fmt.Sscanf(parts[1], "%d", &removed)
			}
			filename := strings.Join(parts[2:], " ")
			info.Changed[filename] = true
			info.Stats[filename] = DiffStat{Added: added, Removed: removed}
		}
	}

	// Get untracked files (new files)
	cmd2 := exec.Command("git", "ls-files", "--others", "--exclude-standard")
	cmd2.Dir = root
	output2, _ := cmd2.Output()
	for _, line := range strings.Split(strings.TrimSpace(string(output2)), "\n") {
		if line != "" {
			info.Changed[line] = true
			info.Untracked[line] = true
		}
	}

	return info, nil
}

// GitDiffFiles returns files changed between current HEAD and the given branch/ref
// Also includes untracked files (new files not yet committed)
func GitDiffFiles(root, ref string) (map[string]bool, error) {
	info, err := GitDiffInfo(root, ref)
	if err != nil {
		return nil, err
	}
	return info.Changed, nil
}

// GitDiffStats returns +/- line counts for changed files
type DiffStat struct {
	Added   int
	Removed int
}

func GitDiffStats(root, ref string) (map[string]DiffStat, error) {
	cmd := exec.Command("git", "diff", "--numstat", ref)
	cmd.Dir = root
	output, err := cmd.Output()
	if err != nil {
		return nil, err
	}

	stats := make(map[string]DiffStat)
	for _, line := range strings.Split(strings.TrimSpace(string(output)), "\n") {
		if line == "" {
			continue
		}
		parts := strings.Fields(line)
		if len(parts) >= 3 {
			var added, removed int
			if parts[0] != "-" {
				fmt.Sscanf(parts[0], "%d", &added)
			}
			if parts[1] != "-" {
				fmt.Sscanf(parts[1], "%d", &removed)
			}
			// parts[2] is the filename, but could have spaces - rejoin
			filename := strings.Join(parts[2:], " ")
			stats[filename] = DiffStat{Added: added, Removed: removed}
		}
	}
	return stats, nil
}

// FilterToChanged filters a slice of FileInfo to only include changed files
func FilterToChanged(files []FileInfo, changed map[string]bool) []FileInfo {
	var result []FileInfo
	for _, f := range files {
		if changed[f.Path] || changed[filepath.ToSlash(f.Path)] {
			result = append(result, f)
		}
	}
	return result
}

// FilterToChangedWithInfo filters and annotates files with diff info
func FilterToChangedWithInfo(files []FileInfo, info *DiffInfo) []FileInfo {
	var result []FileInfo
	for _, f := range files {
		path := f.Path
		slashPath := filepath.ToSlash(f.Path)
		if info.Changed[path] || info.Changed[slashPath] {
			// Annotate with diff info
			f.IsNew = info.Untracked[path] || info.Untracked[slashPath]
			if stat, ok := info.Stats[path]; ok {
				f.Added = stat.Added
				f.Removed = stat.Removed
			} else if stat, ok := info.Stats[slashPath]; ok {
				f.Added = stat.Added
				f.Removed = stat.Removed
			}
			result = append(result, f)
		}
	}
	return result
}

// FilterAnalysisToChanged filters FileAnalysis slice to only changed files
func FilterAnalysisToChanged(files []FileAnalysis, changed map[string]bool) []FileAnalysis {
	var result []FileAnalysis
	for _, f := range files {
		if changed[f.Path] || changed[filepath.ToSlash(f.Path)] {
			result = append(result, f)
		}
	}
	return result
}

// ImpactInfo describes which changed files are used by other files
type ImpactInfo struct {
	File   string // the file that changed
	UsedBy int    // number of other files that import/use this file
}

// AnalyzeImpact checks which changed files are imported by other files
// Uses tree-sitter to extract actual imports for accuracy
func AnalyzeImpact(root string, changedFiles []FileInfo) []ImpactInfo {
	if len(changedFiles) == 0 {
		return nil
	}

	// Build set of changed file base names and directories
	changedBases := make(map[string]string) // base name -> full path
	changedDirs := make(map[string]string)  // dir name -> representative file
	for _, f := range changedFiles {
		base := strings.TrimSuffix(filepath.Base(f.Path), filepath.Ext(f.Path))
		changedBases[base] = f.Path

		// Also track directories for Go-style package imports
		dir := filepath.Dir(f.Path)
		if dir != "." && dir != "" {
			dirBase := filepath.Base(dir)
			if _, exists := changedDirs[dirBase]; !exists {
				changedDirs[dirBase] = f.Path
			}
		}
	}

	// Scan all files to get their imports using tree-sitter
	loader := NewGrammarLoader()
	usageCounts := make(map[string]int)

	filepath.Walk(root, func(path string, info os.FileInfo, err error) error {
		if err != nil {
			return nil
		}

		// Skip ignored directories
		if info.IsDir() {
			if IgnoredDirs[info.Name()] {
				return filepath.SkipDir
			}
			return nil
		}

		// Analyze file imports (only need basic info for impact analysis)
		analysis, err := loader.AnalyzeFile(path, DetailNone)
		if err != nil || analysis == nil {
			return nil
		}

		relPath, _ := filepath.Rel(root, path)

		// Check each import to see if it references a changed file
		for _, imp := range analysis.Imports {
			// Extract the last component of the import path
			// e.g., "./components/Header" -> "Header"
			// e.g., "codemap/scanner" -> "scanner"
			impBase := filepath.Base(imp)
			impBase = strings.TrimSuffix(impBase, filepath.Ext(impBase))

			// Check if this import matches a changed file (by filename)
			if changedPath, ok := changedBases[impBase]; ok {
				if relPath != changedPath {
					usageCounts[changedPath]++
				}
			}

			// Also check if import matches a changed directory (Go packages)
			if changedPath, ok := changedDirs[impBase]; ok {
				changedDir := filepath.Dir(changedPath)
				// Don't count imports from same directory
				if filepath.Dir(relPath) != changedDir {
					usageCounts[changedDir+"/"]++
				}
			}
		}
		return nil
	})

	// Build impact info
	var impacts []ImpactInfo
	for file, count := range usageCounts {
		if count > 0 {
			impacts = append(impacts, ImpactInfo{
				File:   filepath.Base(file),
				UsedBy: count,
			})
		}
	}

	// Sort by usage count descending
	sort.Slice(impacts, func(i, j int) bool {
		return impacts[i].UsedBy > impacts[j].UsedBy
	})

	return impacts
}
</file>
<file path="scanner/grammar.go">
package scanner

import (
	"embed"
	"fmt"
	"os"
	"path/filepath"
	"runtime"
	"strings"

	tree_sitter "github.com/tree-sitter/go-tree-sitter"
)

//go:embed queries/*.scm
var queryFiles embed.FS

// LanguageConfig holds dynamically loaded parser and query
type LanguageConfig struct {
	Language *tree_sitter.Language
	Query    *tree_sitter.Query
}

// GrammarLoader handles dynamic loading of tree-sitter grammars
type GrammarLoader struct {
	configs    map[string]*LanguageConfig
	grammarDir string
}

// LangInfo holds display names for a language
type LangInfo struct {
	Short string // Compact label: "JS", "Py"
	Full  string // Full name: "JavaScript", "Python"
}

// LangDisplay maps internal language names to display names
var LangDisplay = map[string]LangInfo{
	"go":         {"Go", "Go"},
	"python":     {"Py", "Python"},
	"javascript": {"JS", "JavaScript"},
	"typescript": {"TS", "TypeScript"},
	"rust":       {"Rs", "Rust"},
	"ruby":       {"Rb", "Ruby"},
	"c":          {"C", "C"},
	"cpp":        {"C++", "C++"},
	"java":       {"Java", "Java"},
	"swift":      {"Swift", "Swift"},
	"bash":       {"Sh", "Bash"},
	"kotlin":     {"Kt", "Kotlin"},
	"c_sharp":    {"C#", "C#"},
	"php":        {"PHP", "PHP"},
	"dart":       {"Dart", "Dart"},
	"r":          {"R", "R"},
}

// Extension to language mapping
var extToLang = map[string]string{
	".go":    "go",
	".py":    "python",
	".js":    "javascript",
	".jsx":   "javascript",
	".mjs":   "javascript",
	".ts":    "typescript",
	".tsx":   "typescript",
	".rs":    "rust",
	".rb":    "ruby",
	".c":     "c",
	".h":     "c",
	".cpp":   "cpp",
	".hpp":   "cpp",
	".cc":    "cpp",
	".java":  "java",
	".swift": "swift",
	".sh":    "bash",
	".bash":  "bash",
	".kt":    "kotlin",
	".kts":   "kotlin",
	".cs":    "c_sharp",
	".php":   "php",
	".dart":  "dart",
	".r":     "r",
	".R":     "r",
}

// NewGrammarLoader creates a loader that searches for grammars
func NewGrammarLoader() *GrammarLoader {
	loader := &GrammarLoader{
		configs: make(map[string]*LanguageConfig),
	}

	// Find grammar directory - check env var first (for Homebrew install)
	possibleDirs := []string{}
	if envDir := os.Getenv("CODEMAP_GRAMMAR_DIR"); envDir != "" {
		possibleDirs = append(possibleDirs, envDir)
	}
	possibleDirs = append(possibleDirs,
		filepath.Join(getExecutableDir(), "grammars"),
		filepath.Join(getExecutableDir(), "..", "lib", "grammars"),
		"/opt/homebrew/opt/codemap/libexec/grammars", // Homebrew Apple Silicon
		"/usr/local/opt/codemap/libexec/grammars",    // Homebrew Intel Mac
		"/usr/local/lib/codemap/grammars",
		filepath.Join(os.Getenv("HOME"), ".codemap", "grammars"),
		"./grammars",         // For development
		"./scanner/grammars", // For development from root
	)

	for _, dir := range possibleDirs {
		if info, err := os.Stat(dir); err == nil && info.IsDir() {
			loader.grammarDir = dir
			break
		}
	}

	return loader
}

// HasGrammars returns true if grammar directory was found
func (l *GrammarLoader) HasGrammars() bool {
	return l.grammarDir != ""
}

// GrammarDir returns the grammar directory path (for diagnostics)
func (l *GrammarLoader) GrammarDir() string {
	return l.grammarDir
}

// LoadLanguage dynamically loads a grammar from .so/.dylib
func (l *GrammarLoader) LoadLanguage(lang string) error {
	if _, exists := l.configs[lang]; exists {
		return nil // Already loaded
	}

	if l.grammarDir == "" {
		return fmt.Errorf("no grammar directory found")
	}

	// OS-specific library extension
	var libExt string
	switch runtime.GOOS {
	case "darwin":
		libExt = ".dylib"
	case "windows":
		libExt = ".dll"
	default:
		libExt = ".so"
	}

	// Load shared library
	libPath := filepath.Join(l.grammarDir, fmt.Sprintf("libtree-sitter-%s%s", lang, libExt))
	lib, err := loadLibrary(libPath)
	if err != nil {
		return fmt.Errorf("load %s: %w", libPath, err)
	}

	// Get language function
	langFunc, err := getLanguageFunc(lib, lang)
	if err != nil {
		return fmt.Errorf("get func for %s: %w", lang, err)
	}
	language := tree_sitter.NewLanguage(langFunc())

	// Load query
	queryBytes, err := queryFiles.ReadFile(fmt.Sprintf("queries/%s.scm", lang))
	if err != nil {
		return fmt.Errorf("no query for %s", lang)
	}

	query, qerr := tree_sitter.NewQuery(language, string(queryBytes))
	if qerr != nil {
		return fmt.Errorf("bad query for %s: %v", lang, qerr)
	}

	l.configs[lang] = &LanguageConfig{Language: language, Query: query}
	return nil
}

// DetectLanguage returns the language name for a file path
func DetectLanguage(filePath string) string {
	ext := strings.ToLower(filepath.Ext(filePath))
	return extToLang[ext]
}

// AnalyzeFile extracts functions and imports
// detailLevel controls depth of extraction (0=names, 1=signatures, 2=full)
func (l *GrammarLoader) AnalyzeFile(filePath string, detailLevel DetailLevel) (*FileAnalysis, error) {
	lang := DetectLanguage(filePath)
	if lang == "" {
		return nil, nil
	}

	if err := l.LoadLanguage(lang); err != nil {
		return nil, nil // Skip if grammar unavailable
	}

	config := l.configs[lang]
	content, err := os.ReadFile(filePath)
	if err != nil {
		return nil, err
	}

	parser := tree_sitter.NewParser()
	defer parser.Close()
	parser.SetLanguage(config.Language)

	tree := parser.Parse(content, nil)
	defer tree.Close()

	cursor := tree_sitter.NewQueryCursor()
	defer cursor.Close()

	analysis := &FileAnalysis{Path: filePath, Language: lang}

	// Temporary storage for building composite captures
	funcBuilder := make(map[uint]*funcCapture)
	typeBuilder := make(map[uint]*typeCapture)

	// Use Matches() API - iterate over query matches
	matches := cursor.Matches(config.Query, tree.RootNode(), content)
	for match := matches.Next(); match != nil; match = matches.Next() {
		for _, capture := range match.Captures {
			captureName := config.Query.CaptureNames()[capture.Index]
			text := strings.Trim(capture.Node.Utf8Text(content), `"'`)
			// Extract line number (1-indexed)
			line := int(capture.Node.StartPosition().Row) + 1

			// Route to appropriate handler based on capture name prefix
			switch {
			case strings.HasPrefix(captureName, "func."):
				handleFuncCapture(funcBuilder, match.Id(), captureName, text, line)
			case strings.HasPrefix(captureName, "type."):
				handleTypeCapture(typeBuilder, match.Id(), captureName, text, line, detailLevel)
			case captureName == "import" || captureName == "module":
				analysis.Imports = append(analysis.Imports, text)
			// Legacy support: plain @function/@method capture (current queries)
			case captureName == "function" || captureName == "method":
				analysis.Functions = append(analysis.Functions, FuncInfo{Name: text, Line: line})
			}
		}
	}

	// Build final function list from captured components
	for _, fc := range funcBuilder {
		funcInfo := fc.Build(detailLevel, lang)
		analysis.Functions = append(analysis.Functions, funcInfo)
	}

	// Build final type list
	for _, tc := range typeBuilder {
		typeInfo := tc.Build(detailLevel, lang)
		analysis.Types = append(analysis.Types, typeInfo)
	}

	analysis.Functions = dedupeFuncs(analysis.Functions)
	analysis.Types = dedupeTypes(analysis.Types)
	analysis.Imports = dedupe(analysis.Imports)
	return analysis, nil
}

// funcCapture collects components of a function signature
type funcCapture struct {
	name     string
	params   string
	result   string
	receiver string
	line     int
}

// Build constructs FuncInfo from captured components
func (fc *funcCapture) Build(detail DetailLevel, lang string) FuncInfo {
	info := FuncInfo{
		Name:       fc.name,
		IsExported: IsExportedName(fc.name, lang),
		Line:       fc.line,
		ParamCount: countParams(fc.params),
	}

	if detail >= DetailSignature && fc.params != "" {
		info.Signature = buildSignature(fc, lang)
	}

	if fc.receiver != "" {
		info.Receiver = fc.receiver
	}

	return info
}

// countParams counts the number of parameters from a parameter list string.
// Returns -1 for variadic functions or empty params, 0 for no params.
func countParams(params string) int {
	params = strings.TrimSpace(params)
	if params == "" || params == "()" {
		return 0
	}

	// Remove outer parentheses if present
	if strings.HasPrefix(params, "(") && strings.HasSuffix(params, ")") {
		params = params[1 : len(params)-1]
	}
	params = strings.TrimSpace(params)
	if params == "" {
		return 0
	}

	// Check for variadic indicators
	if strings.Contains(params, "...") || strings.Contains(params, "*args") || strings.Contains(params, "**kwargs") {
		return -1 // Variadic
	}

	// Count parameters by tracking parentheses/brackets depth
	// to avoid counting commas inside nested types like func(int, int)
	count := 1
	depth := 0
	for _, ch := range params {
		switch ch {
		case '(', '[', '{', '<':
			depth++
		case ')', ']', '}', '>':
			depth--
		case ',':
			if depth == 0 {
				count++
			}
		}
	}

	return count
}

// handleFuncCapture routes function-related captures to builder
func handleFuncCapture(builders map[uint]*funcCapture, matchID uint, name, text string, line int) {
	if builders[matchID] == nil {
		builders[matchID] = &funcCapture{}
	}
	fc := builders[matchID]

	switch name {
	case "func.name":
		fc.name = text
		fc.line = line // Capture line on name (primary identifier)
	case "func.params":
		fc.params = text
	case "func.result":
		fc.result = text
	case "func.receiver":
		fc.receiver = text
	}
}

// typeCapture collects components of a type definition
type typeCapture struct {
	name   string
	kind   TypeKind
	fields string
	line   int
}

// Build constructs TypeInfo from captured components
func (tc *typeCapture) Build(detail DetailLevel, lang string) TypeInfo {
	info := TypeInfo{
		Name:       tc.name,
		Kind:       tc.kind,
		IsExported: IsExportedName(tc.name, lang),
		Line:       tc.line,
	}

	if detail >= DetailFull && tc.fields != "" {
		info.Fields = parseFieldNames(tc.fields)
	}

	return info
}

// handleTypeCapture routes type-related captures to builder
func handleTypeCapture(builders map[uint]*typeCapture, matchID uint, name, text string, line int, detail DetailLevel) {
	if builders[matchID] == nil {
		builders[matchID] = &typeCapture{}
	}
	tc := builders[matchID]

	switch name {
	case "type.name":
		tc.name = text
		tc.line = line // Capture line on name (primary identifier)
	case "type.fields", "type.methods":
		if detail >= DetailFull {
			tc.fields = text
		}
	case "type.struct":
		tc.kind = KindStruct
	case "type.class":
		tc.kind = KindClass
	case "type.interface":
		tc.kind = KindInterface
	case "type.trait":
		tc.kind = KindTrait
	case "type.enum":
		tc.kind = KindEnum
	case "type.alias":
		tc.kind = KindTypeAlias
	case "type.protocol":
		tc.kind = KindProtocol
	}
}

// buildSignature reconstructs function signature from components
func buildSignature(fc *funcCapture, lang string) string {
	var sig strings.Builder

	switch lang {
	case "go":
		sig.WriteString("func ")
		if fc.receiver != "" {
			sig.WriteString(fc.receiver)
			sig.WriteString(" ")
		}
		sig.WriteString(fc.name)
		sig.WriteString(fc.params)
		if fc.result != "" {
			sig.WriteString(" ")
			sig.WriteString(fc.result)
		}

	case "python":
		sig.WriteString("def ")
		sig.WriteString(fc.name)
		sig.WriteString(fc.params)
		if fc.result != "" {
			sig.WriteString(" -> ")
			sig.WriteString(fc.result)
		}

	case "typescript", "javascript":
		sig.WriteString("function ")
		sig.WriteString(fc.name)
		sig.WriteString(fc.params)
		if fc.result != "" {
			sig.WriteString(": ")
			sig.WriteString(fc.result)
		}

	case "rust":
		sig.WriteString("fn ")
		sig.WriteString(fc.name)
		sig.WriteString(fc.params)
		if fc.result != "" {
			sig.WriteString(" -> ")
			sig.WriteString(fc.result)
		}

	case "java", "c_sharp":
		if fc.result != "" {
			sig.WriteString(fc.result)
			sig.WriteString(" ")
		}
		sig.WriteString(fc.name)
		sig.WriteString(fc.params)

	default:
		sig.WriteString(fc.name)
		sig.WriteString(fc.params)
	}

	return sig.String()
}

// parseFieldNames extracts field/member names from raw block text
func parseFieldNames(fieldsText string) []string {
	var fields []string
	lines := strings.Split(fieldsText, "\n")

	for _, line := range lines {
		line = strings.TrimSpace(line)
		if line == "" || line == "{" || line == "}" {
			continue
		}

		// Extract first identifier (field name)
		parts := strings.Fields(line)
		if len(parts) > 0 {
			name := strings.TrimSuffix(parts[0], ":")
			name = strings.TrimSuffix(name, ",")
			if name != "" && !strings.HasPrefix(name, "//") && !strings.HasPrefix(name, "#") {
				fields = append(fields, name)
			}
		}
	}

	return fields
}

// dedupeFuncs removes duplicate functions by name
func dedupeFuncs(funcs []FuncInfo) []FuncInfo {
	seen := make(map[string]bool)
	var out []FuncInfo
	for _, f := range funcs {
		if !seen[f.Name] {
			seen[f.Name] = true
			out = append(out, f)
		}
	}
	return out
}

// dedupeTypes removes duplicate types by name
func dedupeTypes(types []TypeInfo) []TypeInfo {
	seen := make(map[string]bool)
	var out []TypeInfo
	for _, t := range types {
		if !seen[t.Name] {
			seen[t.Name] = true
			out = append(out, t)
		}
	}
	return out
}

func getExecutableDir() string {
	if exe, err := os.Executable(); err == nil {
		return filepath.Dir(exe)
	}
	return "."
}

func dedupe(s []string) []string {
	seen := make(map[string]bool)
	var out []string
	for _, v := range s {
		if !seen[v] {
			seen[v] = true
			out = append(out, v)
		}
	}
	return out
}
</file>
<file path="scanner/grammar_unix.go">
//go:build !windows

package scanner

import (
	"fmt"
	"unsafe"

	"github.com/ebitengine/purego"
)

// loadLibrary loads a shared library on Unix systems
func loadLibrary(path string) (uintptr, error) {
	lib, err := purego.Dlopen(path, purego.RTLD_NOW|purego.RTLD_GLOBAL)
	if err != nil {
		return 0, err
	}
	return lib, nil
}

// getLanguageFunc gets the tree_sitter_<lang> function from the library
func getLanguageFunc(lib uintptr, lang string) (func() unsafe.Pointer, error) {
	var langFunc func() unsafe.Pointer
	purego.RegisterLibFunc(&langFunc, lib, fmt.Sprintf("tree_sitter_%s", lang))
	return langFunc, nil
}
</file>
<file path="scanner/grammar_windows.go">
//go:build windows

package scanner

import (
	"fmt"
	"syscall"
	"unsafe"
)

// loadLibrary loads a DLL on Windows
func loadLibrary(path string) (uintptr, error) {
	handle, err := syscall.LoadLibrary(path)
	if err != nil {
		return 0, err
	}
	return uintptr(handle), nil
}

// getLanguageFunc gets the tree_sitter_<lang> function from the DLL
func getLanguageFunc(lib uintptr, lang string) (func() unsafe.Pointer, error) {
	procName := fmt.Sprintf("tree_sitter_%s", lang)
	proc, err := syscall.GetProcAddress(syscall.Handle(lib), procName)
	if err != nil {
		return nil, fmt.Errorf("GetProcAddress %s: %w", procName, err)
	}

	// Create a wrapper function that calls the proc
	fn := func() unsafe.Pointer {
		ret, _, _ := syscall.SyscallN(proc)
		return unsafe.Pointer(ret)
	}
	return fn, nil
}
</file>
<file path="scanner/symbol.go">
package scanner

import (
	"strings"
)

// SymbolQuery represents the filters for symbol search
type SymbolQuery struct {
	Name string // Substring match (case-insensitive)
	Kind string // "function", "type", "all"
	File string // Filter by specific file (optional)
}

// SymbolMatch represents a found symbol
type SymbolMatch struct {
	Name      string `json:"name"`
	Kind      string `json:"kind"`      // "function" or "type"
	Signature string `json:"signature"` // For functions
	TypeKind  string `json:"type_kind"` // For types (struct, class, etc.)
	File      string `json:"file"`
	Line      int    `json:"line"`
	Exported  bool   `json:"exported"`
}

// SearchSymbols searches for symbols in the analyzed files
func SearchSymbols(analyses []FileAnalysis, query SymbolQuery) []SymbolMatch {
	var matches []SymbolMatch
	searchName := strings.ToLower(query.Name)

	for _, analysis := range analyses {
		// Skip if file filter is set and doesn't match
		if query.File != "" && !strings.Contains(analysis.Path, query.File) {
			continue
		}

		// Search functions
		if query.Kind == "" || query.Kind == "all" || query.Kind == "function" {
			for _, fn := range analysis.Functions {
				if searchName == "" || strings.Contains(strings.ToLower(fn.Name), searchName) {
					matches = append(matches, SymbolMatch{
						Name:      fn.Name,
						Kind:      "function",
						Signature: fn.Signature,
						File:      analysis.Path,
						Line:      fn.Line,
						Exported:  fn.IsExported,
					})
				}
			}
		}

		// Search types
		if query.Kind == "" || query.Kind == "all" || query.Kind == "type" {
			for _, t := range analysis.Types {
				if searchName == "" || strings.Contains(strings.ToLower(t.Name), searchName) {
					matches = append(matches, SymbolMatch{
						Name:     t.Name,
						Kind:     "type",
						TypeKind: string(t.Kind),
						File:     analysis.Path,
						Line:     t.Line,
						Exported: t.IsExported,
					})
				}
			}
		}
	}

	return matches
}
</file>
<file path="scanner/types.go">
package scanner

import (
	"encoding/json"
	"strings"
	"unicode"
)

// DetailLevel controls how much information is extracted
type DetailLevel int

const (
	DetailNone      DetailLevel = 0 // Only names (current behavior)
	DetailSignature DetailLevel = 1 // Names + signatures
	DetailFull      DetailLevel = 2 // Signatures + type fields
)

// Token estimation constants
const (
	CharsPerToken   = 3.5  // Conservative estimate for BPE tokenizers
	LargeFileTokens = 8000 // Threshold for warning indicator
)

// EstimateTokens estimates the number of tokens for a given file size
func EstimateTokens(size int64) int {
	return int(float64(size) / CharsPerToken)
}

// FileInfo represents a single file in the codebase.
type FileInfo struct {
	Path    string `json:"path"`
	Size    int64  `json:"size"`
	Ext     string `json:"ext"`
	Tokens  int    `json:"tokens,omitempty"` // Estimated token count
	IsNew   bool   `json:"is_new,omitempty"`
	Added   int    `json:"added,omitempty"`
	Removed int    `json:"removed,omitempty"`
}

// Project represents the root of the codebase for tree/skyline mode.
type Project struct {
	Root    string       `json:"root"`
	Mode    string       `json:"mode"`
	Animate bool         `json:"animate"`
	Files   []FileInfo   `json:"files"`
	DiffRef string       `json:"diff_ref,omitempty"`
	Impact  []ImpactInfo `json:"impact,omitempty"`
}

// FuncInfo represents a function/method with optional detail
type FuncInfo struct {
	Name       string `json:"name"`
	Signature  string `json:"signature,omitempty"`   // Full signature when detail >= 1
	Receiver   string `json:"receiver,omitempty"`    // For methods (Go, Rust, etc.)
	IsExported bool   `json:"exported,omitempty"`    // Public visibility
	Line       int    `json:"line,omitempty"`        // Line number of definition (1-indexed)
	ParamCount int    `json:"param_count,omitempty"` // Number of parameters (-1 for variadic)
}

// MarshalJSON customizes JSON output for backward compatibility
// When no extended info is present, serialize as plain string
func (f FuncInfo) MarshalJSON() ([]byte, error) {
	if f.Signature == "" && f.Receiver == "" && !f.IsExported && f.Line == 0 {
		return json.Marshal(f.Name)
	}
	type Alias FuncInfo
	return json.Marshal(Alias(f))
}

// UnmarshalJSON handles both string and object formats
func (f *FuncInfo) UnmarshalJSON(data []byte) error {
	// Try string first
	var name string
	if err := json.Unmarshal(data, &name); err == nil {
		f.Name = name
		return nil
	}
	// Fall back to object
	type Alias FuncInfo
	var alias Alias
	if err := json.Unmarshal(data, &alias); err != nil {
		return err
	}
	*f = FuncInfo(alias)
	return nil
}

// TypeKind represents normalized type categories across languages
type TypeKind string

const (
	KindStruct    TypeKind = "struct"    // Go struct, C struct, Rust struct
	KindClass     TypeKind = "class"     // Python/Java/C#/TS class
	KindInterface TypeKind = "interface" // Go interface, Java/C#/TS interface
	KindTrait     TypeKind = "trait"     // Rust trait
	KindEnum      TypeKind = "enum"      // All languages
	KindTypeAlias TypeKind = "alias"     // Go type alias, TS type
	KindProtocol  TypeKind = "protocol"  // Swift protocol
)

// TypeInfo represents a type definition
type TypeInfo struct {
	Name       string   `json:"name"`
	Kind       TypeKind `json:"kind"`
	Fields     []string `json:"fields,omitempty"`  // Field names when detail = 2
	Methods    []string `json:"methods,omitempty"` // Method names (for classes)
	IsExported bool     `json:"exported,omitempty"`
	Line       int      `json:"line,omitempty"` // Line number of definition (1-indexed)
}

// FileAnalysis holds extracted info about a single file for deps mode.
type FileAnalysis struct {
	Path      string     `json:"path"`
	Language  string     `json:"language"`
	Functions []FuncInfo `json:"functions"`
	Types     []TypeInfo `json:"types,omitempty"`
	Imports   []string   `json:"imports"`
}

// DepsProject is the JSON output for --deps mode.
type DepsProject struct {
	Root         string              `json:"root"`
	Mode         string              `json:"mode"`
	Files        []FileAnalysis      `json:"files"`
	ExternalDeps map[string][]string `json:"external_deps"`
	DiffRef      string              `json:"diff_ref,omitempty"`
	DetailLevel  int                 `json:"detail_level,omitempty"`
}

// IsExportedName checks if a symbol name is exported based on language conventions
func IsExportedName(name, lang string) bool {
	if name == "" {
		return false
	}

	switch lang {
	case "go":
		// Go: exported if starts with uppercase
		r := []rune(name)
		return unicode.IsUpper(r[0])

	case "python":
		// Python: exported if doesn't start with _
		return !strings.HasPrefix(name, "_")

	case "rust":
		// Rust: would need `pub` keyword analysis
		// For now, assume all are potentially public
		return true

	default:
		// Most languages: assume public unless private keyword
		return true
	}
}
</file>
<file path="scanner/walker.go">
package scanner

import (
	"os"
	"path/filepath"

	ignore "github.com/sabhiram/go-gitignore"
)

// IgnoredDirs are directories to skip during scanning
var IgnoredDirs = map[string]bool{
	".git":           true,
	"node_modules":   true,
	"vendor":         true,
	"Pods":           true,
	"build":          true,
	"DerivedData":    true,
	".idea":          true,
	".vscode":        true,
	"__pycache__":    true,
	".DS_Store":      true,
	"venv":           true,
	".venv":          true,
	".env":           true,
	".pytest_cache":  true,
	".mypy_cache":    true,
	".ruff_cache":    true,
	".coverage":      true,
	"htmlcov":        true,
	".tox":           true,
	"dist":           true,
	".next":          true,
	".nuxt":          true,
	"target":         true,
	".gradle":        true,
	".cargo":         true,
	".grammar-build": true,
	"grammars":       true,
}

// WalkOptions configures the file walking behavior.
type WalkOptions struct {
	// Gitignore patterns to apply (can be nil)
	Gitignore *ignore.GitIgnore

	// LanguageFilter if true, only visits files with supported languages
	LanguageFilter bool
}

// WalkFunc is the callback function type for WalkFiles.
// It receives the absolute path, relative path, and file info for each file.
// Return filepath.SkipDir to skip a directory, or any other error to stop walking.
type WalkFunc func(absPath, relPath string, info os.FileInfo) error

// WalkFiles walks the directory tree and calls fn for each file.
// This decouples file system traversal from data generation, allowing
// callers to process files as needed without duplicating walk logic.
func WalkFiles(root string, opts WalkOptions, fn WalkFunc) error {
	return filepath.Walk(root, func(path string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		relPath, err := filepath.Rel(root, path)
		if err != nil {
			return err
		}

		// Skip if matched by common ignore patterns
		if info.IsDir() {
			if IgnoredDirs[info.Name()] {
				return filepath.SkipDir
			}
		} else {
			if IgnoredDirs[info.Name()] {
				return nil
			}
		}

		// Skip if matched by .gitignore
		if opts.Gitignore != nil && opts.Gitignore.MatchesPath(relPath) {
			if info.IsDir() {
				return filepath.SkipDir
			}
			return nil
		}

		// Skip directories (we only process files)
		if info.IsDir() {
			return nil
		}

		// Apply language filter if enabled
		if opts.LanguageFilter && DetectLanguage(path) == "" {
			return nil
		}

		// Call the user-provided function
		return fn(path, relPath, info)
	})
}

// LoadGitignore loads .gitignore from root if it exists
func LoadGitignore(root string) *ignore.GitIgnore {
	gitignorePath := filepath.Join(root, ".gitignore")

	if _, err := os.Stat(gitignorePath); err == nil {
		if gitignore, err := ignore.CompileIgnoreFile(gitignorePath); err == nil {
			return gitignore
		}
	}

	return nil
}

// ScanFiles walks the directory tree and returns all files.
// This is a convenience wrapper around WalkFiles for collecting FileInfo.
func ScanFiles(root string, gitignore *ignore.GitIgnore) ([]FileInfo, error) {
	var files []FileInfo

	opts := WalkOptions{
		Gitignore: gitignore,
	}

	err := WalkFiles(root, opts, func(absPath, relPath string, info os.FileInfo) error {
		files = append(files, FileInfo{
			Path:   relPath,
			Size:   info.Size(),
			Ext:    filepath.Ext(absPath),
			Tokens: EstimateTokens(info.Size()),
		})
		return nil
	})

	return files, err
}

// ScanForDeps walks the directory tree and analyzes files for dependencies.
// This is a convenience wrapper around WalkFiles for collecting FileAnalysis.
// detailLevel controls the depth of extraction (0=names, 1=signatures, 2=full)
func ScanForDeps(root string, gitignore *ignore.GitIgnore, loader *GrammarLoader, detailLevel DetailLevel) ([]FileAnalysis, error) {
	var analyses []FileAnalysis

	opts := WalkOptions{
		Gitignore:      gitignore,
		LanguageFilter: true, // Only analyze supported languages
	}

	err := WalkFiles(root, opts, func(absPath, relPath string, info os.FileInfo) error {
		// Analyze file with the specified detail level
		analysis, err := loader.AnalyzeFile(absPath, detailLevel)
		if err != nil || analysis == nil {
			return nil // Skip files that can't be analyzed
		}

		// Use relative path in output
		analysis.Path = relPath
		analyses = append(analyses, *analysis)

		return nil
	})

	return analyses, err
}
</file>
<file path="scripts/build-grammars.sh">
#!/bin/bash
# build-grammars.sh - Build tree-sitter grammars for the current platform
#
# Usage: ./scripts/build-grammars.sh [target]
#   target: darwin, linux, or windows (defaults to current OS)
#
# Environment variables:
#   CC  - C compiler (auto-detected if not set)
#   CXX - C++ compiler (auto-detected if not set)

set -e

# Grammar definitions: lang:repo:branch:src_dir
GRAMMARS=(
  "go:tree-sitter/tree-sitter-go:master:src"
  "python:tree-sitter/tree-sitter-python:master:src"
  "javascript:tree-sitter/tree-sitter-javascript:master:src"
  "typescript:tree-sitter/tree-sitter-typescript:master:typescript/src"
  "rust:tree-sitter/tree-sitter-rust:master:src"
  "ruby:tree-sitter/tree-sitter-ruby:master:src"
  "c:tree-sitter/tree-sitter-c:master:src"
  "cpp:tree-sitter/tree-sitter-cpp:master:src"
  "java:tree-sitter/tree-sitter-java:master:src"
  "swift:tree-sitter/tree-sitter-swift:master:src"
  "bash:tree-sitter/tree-sitter-bash:master:src"
  "kotlin:fwcd/tree-sitter-kotlin:main:src"
  "c_sharp:tree-sitter/tree-sitter-c-sharp:master:src"
  "php:tree-sitter/tree-sitter-php:master:php/src"
  "dart:UserNobody14/tree-sitter-dart:master:src"
  "r:r-lib/tree-sitter-r:main:src"
)

# Detect target platform
TARGET="${1:-$(uname -s | tr '[:upper:]' '[:lower:]')}"
case "$TARGET" in
  darwin|macos)
    TARGET="darwin"
    EXT="dylib"
    CC="${CC:-clang}"
    CXX="${CXX:-clang++}"
    SHARED_FLAGS="-shared -fPIC"
    ;;
  linux)
    EXT="so"
    CC="${CC:-gcc}"
    CXX="${CXX:-g++}"
    SHARED_FLAGS="-shared -fPIC"
    ;;
  windows|win)
    TARGET="windows"
    EXT="dll"
    # Default to Zig for Windows cross-compilation
    CC="${CC:-zig cc -target x86_64-windows-gnu}"
    CXX="${CXX:-zig c++ -target x86_64-windows-gnu}"
    SHARED_FLAGS="-shared -fPIC"
    ;;
  *)
    echo "Unknown target: $TARGET"
    echo "Usage: $0 [darwin|linux|windows]"
    exit 1
    ;;
esac

echo "Building grammars for $TARGET (.$EXT)"
echo "CC=$CC"
echo "CXX=$CXX"
echo ""

mkdir -p grammars
BUILD_DIR=$(mktemp -d)
trap "rm -rf $BUILD_DIR" EXIT

for grammar in "${GRAMMARS[@]}"; do
  IFS=: read -r lang repo branch src_dir <<< "$grammar"

  echo "Building $lang..."

  # Download and extract to clean temp dir
  rm -rf "$BUILD_DIR"/*
  cd "$BUILD_DIR"
  curl -sL "https://github.com/$repo/archive/refs/heads/$branch.tar.gz" | tar xz
  extracted_dir=$(ls -d */ | head -1)
  src_path="${extracted_dir}${src_dir}"

  if [ ! -d "$src_path" ]; then
    echo "  ✗ Source not found: $src_path"
    exit 1
  fi

  # Compile
  sources="$src_path/parser.c"
  use_cxx=false

  if [ -f "$src_path/scanner.c" ]; then
    sources="$sources $src_path/scanner.c"
  elif [ -f "$src_path/scanner.cc" ]; then
    use_cxx=true
    $CXX -c -fPIC -I"$src_path" "$src_path/scanner.cc" -o scanner.o
    sources="$sources scanner.o"
  fi

  output_file="$OLDPWD/grammars/libtree-sitter-$lang.$EXT"

  if $use_cxx; then
    $CXX $SHARED_FLAGS -I"$src_path" $sources -o "$output_file"
  else
    $CC $SHARED_FLAGS -I"$src_path" $sources -o "$output_file"
  fi

  cd "$OLDPWD"
  echo "  ✓ Built $lang"
done

echo ""
echo "Built ${#GRAMMARS[@]} grammars:"
ls -la grammars/
</file>
<file path="AGENTS.md">
# AGENTS.md: Universal AI Agent Configuration

## Project Summary
`codemap` is a Go CLI tool that transforms a codebase into a persistent **Knowledge Graph** using `tree-sitter` for structural analysis. It uses **Retrieval-Augmented Generation (RAG)** via the `/analyze` package to provide context-aware analysis and semantic search using various LLMs.

## Build & Operational Commands

### Build Commands
| Action | Command | Notes |
| :--- | :--- | :--- |
| Build CLI | `go build -o codemap .` | Creates the main executable. |
| Build Grammars | `make grammars` | **CRITICAL:** Must be run once and after any change to `scanner/queries/*.scm`. |
| Format Code | `go fmt ./...` | Standard Go formatting. |

### Key Operational Modes
1.  **Index Graph:** `./codemap --index .` (Builds/updates `.codemap/graph.gob`)
2.  **LLM Explain:** `./codemap --explain --symbol <Name> .` (RAG-powered analysis)
3.  **Semantic Search:** `./codemap --search --q "query" .` (Vector search)
4.  **Trace Path:** `./codemap --query --from <SymA> --to <SymB> .` (Finds call path)

## Architecture Overview
The system is a Layered Pipeline centered on the `graph` package, utilizing interfaces for decoupling.

*   **Scanner (`/scanner`):** Data Acquisition Layer. Uses `tree-sitter` and `purego` for AST parsing.
*   **Graph (`/graph`):** Knowledge Layer. Central data repository, implementing the Repository Pattern for persistence and vector indexing.
*   **Analyze (`/analyze`):** Intelligence Layer. Abstracts LLM clients using Factory/Strategy patterns and implements the core RAG pipeline.
*   **Render (`/render`):** Presentation Layer. Handles TUI and visualization (e.g., dependency graphs).

## Key Conventions and Patterns

### Go Style and Error Handling
*   **Language:** Go (Golang). Adhere to standard Go idioms.
*   **Dependencies:** Dependencies are wired in `main.go` using **Constructor Injection**.
*   **Errors:** Critical errors must print to `os.Stderr` and exit with `os.Exit(1)`.

### Data Persistence
*   **Graph File:** The knowledge graph is stored in `.codemap/graph.gob`.
*   **Format:** Uses `encoding/gob` for efficient serialization of Go structs.
*   **IDs:** Graph nodes use deterministic IDs generated from file path and symbol name.

### LLM Integration
*   **Interface:** All LLM clients adhere to the `analyze.Client` interface.
*   **Authentication:** API keys (e.g., `ANTHROPIC_API_KEY`) are loaded primarily from **environment variables**.
*   **Resilience:** The `analyze` package implements a local file system cache and **retry logic (max 3 attempts)** for transient API errors.
*   **Tooling:** The project exposes its capabilities via the **Model Context Protocol (MCP)**, defining tools like `explain_symbol` and `trace_path`.
</file>
<file path="bug.md">
# Bug Analysis Report: Ferramentas MCP em Processamento Eterno

## 1\. Executive Summary

A causa mais provável para as ferramentas do servidor MCP (`explain_symbol` e `summarize_module`) ficarem em **processamento eterno** (requisição não concluída/timeout) é um **erro no loop de retry ou na manipulação do contexto (`context.Context`)** dentro da lógica de comunicação com os provedores de LLM. Considerando que as funcionalidades na CLI funcionam, o problema está na execução via servidor MCP, onde o contexto da requisição pode estar sendo cancelado ou expirado, mas o loop de retry não o respeita corretamente, ou o tempo de espera total excede o timeout do cliente HTTP.

-----

## 2\. Bug Description and Context (from `User Task`)

  * **Observed Behavior:** As ferramentas do servidor MCP `explain_symbol` e `summarize_module`, quando acionadas, ficam processando eternamente e a requisição nunca é concluída.
  * **Expected Behavior:** A requisição deveria ser concluída dentro do tempo limite configurado (`config.LLM.Timeout`) ou retornar um erro claro (ex: timeout, erro de API).
  * **Steps to Reproduce (STR):** Acionar as ferramentas `explain_symbol` ou `summarize_module` via cliente MCP.
  * **Environment (if provided):** Go project (`codemap`). O problema é exclusivo da versão *server/MCP*, enquanto a versão *CLI* funciona corretamente.
  * **Error Messages (if any):** Não especificado, mas o sintoma é um timeout silencioso (processamento eterno).

-----

## 3\. Code Execution Path Analysis

O problema de "processamento eterno" (hanging/timeout) em um servidor web/API que faz chamadas externas (LLM) geralmente ocorre em um dos seguintes pontos:

1.  **Bloqueio Síncrono:** Uma chamada de rede bloqueando indefinidamente sem respeitar o timeout do contexto.
2.  **Retry Infinito/Longo:** O loop de retry de uma chamada de rede está configurado para tentar novamente mesmo após o timeout do contexto pai ter expirado.

As duas ferramentas problemáticas (`explain_symbol` e `summarize_module`) utilizam o mesmo fluxo principal: `handle*()` $\rightarrow$ `analyze.NewClient()` $\rightarrow$ `client.Complete()`.

### 3.1. Entry Point(s) and Initial State

  * **Entry Point:** `mcp/main.go`: `handleExplainSymbol` ou `handleSummarizeModule`.
  * **Initial State:** O `context.Context` (`ctx`) da requisição MCP é criado (pode ter um timeout implícito ou ser o contexto padrão do servidor).

### 3.2. Key Functions/Modules/Components in the Execution Path

| Componente | Função/Caminho | Responsabilidade Presumida |
| :--- | :--- | :--- |
| **MCP Handler** | `mcp/main.go` | Carrega `config`, cria `LLMClient`, inicia `reqCtx` com timeout. |
| **Client Factory** | `analyze/factory.go` | Instancia o cliente LLM (`OllamaClient`, `OpenAIClient`, etc.) a partir da config. |
| **Client Adapter** | `analyze/*client*.go` | Executa a chamada `Complete()` HTTP para a API externa, incluindo retry logic. |
| **Context** | `context.Context` | Limita o tempo total de execução. O timeout configurado é de `60s` por padrão (`config/config.go:DefaultConfig().LLM.Timeout`). |

### 3.3. Execution Flow Tracing (Exemplo: `OpenAIClient.Complete`)

O fluxo de execução é crucial aqui, especialmente o loop de retry e a forma como o `context.Context` é usado. O `handleExplainSymbol` cria um contexto com timeout:

```go
// mcp/main.go:1020 (handleExplainSymbol)
reqCtx, reqCancel := context.WithTimeout(ctx, time.Duration(cfg.LLM.Timeout)*time.Second)
defer reqCancel()

resp, err := client.Complete(reqCtx, &analyze.CompletionRequest{...})
```

A chamada `client.Complete(reqCtx, ...)` (por exemplo, em `analyze/openai.go`):

```go
// analyze/openai.go:167 (Complete - Retry Logic)
// Loop: for attempt := 0; attempt <= c.config.MaxRetries; attempt++ { // MaxRetries defaults to 3 (4 attempts total)
	// 1. New Request: httpReq, err := http.NewRequestWithContext(ctx, "POST", ...)
	// 2. Do Request: resp, lastErr = c.httpClient.Do(httpReq)
	// 3. Check for Timeout/Cancellation: if ctx.Err() != nil { return nil, ErrTimeout }
	// ...
// }
```

**Análise Crítica: `GeminiClient.Complete` (Suspeito)**

O `GeminiClient` introduzido em `analyze/gemini.go` tem um erro na lógica de retry que é uma causa comum de *hangups* em Go.

```go
// analyze/gemini.go:217 (Complete - inside retry loop)
		// Check context before each attempt
		if ctx.Err() != nil { // <-- CORRETO, checa antes da tentativa
			return nil, ErrTimeout
		}

		// Create new request for each attempt (body reader is consumed after each request)
		httpReq, err := http.NewRequestWithContext(ctx, "POST", endpoint, bytes.NewReader(body)) // <-- OK
		if err != nil {
			return nil, err
		}
		httpReq.Header.Set("Content-Type", "application/json")
		httpReq.Header.Set("x-goog-api-key", c.apiKey)

		resp, lastErr = c.httpClient.Do(httpReq) // <-- BLOQUEIA: Usa o contexto (ctx)

		if lastErr != nil {
			// Network error - retry
			if ctx.Err() != nil { // <-- CORRETO, checa depois da falha
				return nil, ErrTimeout
			}
			continue
		}
// ...
```

O `OpenAIClient` e o `AnthropicClient` usam a variável `c.httpClient.Timeout` em vez do contexto para `http.Client.Do()`. A menos que o `c.httpClient` seja inicializado com o `reqCtx` (o que não ocorre), o timeout primário da requisição é determinado pelo `http.Client.Timeout` (`ClientConfig.Timeout`, default 60s), e o `reqCtx` da MCP é usado apenas para cancelamento.

O `GeminiClient` (e `AnthropicClient` em `analyze/anthropic.go`) *re-cria* o `http.NewRequestWithContext(ctx, ...)` em cada retry, o que é correto para usar o `reqCtx` da MCP.

O ponto crítico de falha é a **lógica de *backoff* do retry**:

```go
// analyze/gemini.go:213 (Complete)
	for attempt := 0; attempt <= c.config.MaxRetries; attempt++ {
		if attempt > 0 {
			time.Sleep(time.Duration(attempt*attempt) * time.Second) // <-- Ex: 1s, 4s, 9s...
		}
        // ... faz a requisição
	}
```

Com `MaxRetries=3`, o tempo de *backoff* é de $1s + 4s + 9s = 14s$. Isso é aceitável, mas se o `http.Client` for configurado com um timeout longo, o tempo total pode exceder o timeout da requisição MCP.

**O real suspeito é a manipulação de timeouts do `http.Client` (analyze/client.go) em conjunto com a lógica de retry:**

O `http.Client` é criado em `analyze/gemini.go:34`:

```go
// analyze/gemini.go:34
		httpClient: &http.Client{
			Timeout: cfg.Timeout,
		},
```

O `cfg.Timeout` padrão é **60 segundos** (`config/config.go:DefaultConfig`). O loop de retry tentará **4 vezes** (0 a 3) com até $14s$ de *backoff* total. O tempo total de *wait* (4 \* $60s$ (timeout) + $14s$ (backoff)) é muito longo (aprox. $4$ minutos) e o cliente MCP (que tipicamente usa um timeout muito menor) falhará, mas o loop de retry no servidor continuará rodando até que o `reqCtx` finalmente expire.

O **problema original de "processando eternamente"** provavelmente se deve ao cliente LLM não honrando o `ctx.Err() != nil` **após uma falha de rede inicial**, ou o cliente não sendo configurado com o `reqCtx` *e* o `httpClient.Timeout` sendo muito longo.

**Em `analyze/anthropic.go`:**

```go
// analyze/anthropic.go:174 (Complete)
		resp, lastErr = c.httpClient.Do(httpReq)
		// ...
		if lastErr != nil {
			// ...
		}
		if resp != nil {
			resp.Body.Close()
		}

		if ctx.Err() != nil { // <-- Checagem de cancelamento/timeout do contexto
			return nil, ErrTimeout
		}
```

A única maneira de este loop travar é se `c.httpClient.Do(httpReq)` **retornar `lastErr == nil`** (conexão bem-sucedida com erro de status, ex: 400 Bad Request) mas o `resp.StatusCode` **não for `http.StatusOK` ou `http.StatusTooManyRequests`**, e o loop **quebrar o `if lastErr == nil`**, mas **não houver `break`**, fazendo o loop rodar as 4 tentativas, consumindo muito tempo.

**Identificando o Bug Lógico no Retry:**

Em `analyze/anthropic.go:176`:

```go
// analyze/anthropic.go:176
		if lastErr == nil {
			// Check for rate limiting
			if resp.StatusCode == http.StatusTooManyRequests {
				resp.Body.Close()
				lastErr = ErrRateLimited
				continue
			}
			if resp.StatusCode == http.StatusOK {
				break // <-- SUCESSO! Sai do loop.
			}
		}

		if resp != nil {
			resp.Body.Close()
		}
		// Se lastErr != nil (erro de rede) ou if (sucesso, mas não 200/429), o loop continua.

		if ctx.Err() != nil {
			return nil, ErrTimeout // <-- Esta é a única checagem de timeout no final da iteração.
		}
```

Se a API Anthropic retornar um `500 Internal Server Error` ou um erro `400 Bad Request` que **não seja `429`**, o código **não cai no `break`** (pois `resp.StatusCode != http.StatusOK`) e o `lastErr` **é `nil`** (pois o `http.Client` recebeu uma resposta). O loop continua para a próxima iteração **sem um `break`** ou **retorno de erro imediato**.

O loop tentará novamente 4 vezes, com `time.Sleep` somado, resultando em um atraso significativo antes de finalmente sair do loop no final de 4 tentativas e retornar um erro genérico:

```go
// analyze/anthropic.go:193
	if lastErr != nil {
		return nil, fmt.Errorf("request failed after %d retries: %w", c.config.MaxRetries, lastErr)
	}
	// ... (lógica de erro para status != 200 fora do loop)
```

Se o erro for `400 Bad Request`, o loop continuará 4 vezes sem necessidade, resultando no atraso, mas **não no travamento eterno** (o loop termina).

**O verdadeiro travamento eterno (se ocorrer) só pode acontecer se o `ctx.Err() != nil` nunca for verdadeiro ou se o `http.Client.Do()` travar indefinidamente, o que é mitigado pelo `c.httpClient.Timeout` de 60s.**

**Revisitando `GeminiClient`:**

Em `analyze/gemini.go:230-244`, a lógica de `GeminiClient` é mais robusta e **inclui `continue` e `return`** para diferentes status HTTP, minimizando o risco de retries desnecessários:

```go
// analyze/gemini.go:230 (GeminiClient.Complete - switch statement)
		switch {
		case resp.StatusCode == http.StatusOK:
			lastErr = nil
		case resp.StatusCode == http.StatusNotFound:
			resp.Body.Close()
			return nil, ErrModelNotFound // <-- RETORNO IMEDIATO: NÃO GERA MAIS RETRIES
		case resp.StatusCode == http.StatusTooManyRequests:
			resp.Body.Close()
			lastErr = ErrRateLimited
			continue // <-- RETRY
		case resp.StatusCode >= 500:
			resp.Body.Close()
			lastErr = fmt.Errorf("server error: %d", resp.StatusCode)
			continue // <-- RETRY
		default:
			lastErr = nil // <-- 4xx client errors that are not 404/429. Will exit loop with break below.
		}
		break // <-- Sai do loop se for OK ou 4xx não-retryable.
	}
```

Esta lógica é mais correta que a de Anthropic e deve sair do loop rapidamente.

### 3.4. Data State and Flow Analysis

O fluxo de dados (`CompletionRequest` $\rightarrow$ `LLMClient` $\rightarrow$ `CompletionResponse`) é padrão. A falha não parece ser de manipulação de dados.

**Conclusão da Execução:** O sintoma sugere que o tempo de execução excede o tempo limite do cliente MCP que faz a chamada para o `codemap-mcp`, ou que o `codemap-mcp` está gastando muito tempo em **retries desnecessários** ou **sleeps de backoff** quando encontra um erro não-retryable (como um `400 Bad Request` na API Anthropic).

-----

## 4\. Potential Root Causes and Hypotheses

### 4.1. Hypothesis 1: Retry Loop Não Honra Contexto em Erros de Status (OpenAI/Anthropic)

  - **Rationale/Evidence:** Em `analyze/anthropic.go`, um `resp.StatusCode` diferente de `200` ou `429` (ex: `400` Bad Request) resulta em `lastErr == nil`, mas não quebra o loop de retry. O loop continua por 4 iterações, introduzindo um atraso de $14$ segundos (no backoff), somado a 4 timeouts de $60$ segundos (se o timeout do cliente for usado). Este comportamento, embora não seja um *loop infinito*, causa um **atraso significativo** que excede o timeout do cliente chamador do MCP, fazendo a requisição parecer "eterna" (até o cliente MCP desistir).
  - **Code (if relevant):**
    ```go
    // analyze/anthropic.go:176
    if lastErr == nil {
        // ... check for 429 and 200 (break)
    }
    // No explicit 'else if resp.StatusCode != 200' break/return for other codes.
    // The logic continues, leading to unnecessary retries and time.Sleep.
    ```
  - **How it leads to the bug:** Um erro de API (ex: Bad Request por prompt inválido) dispara 4 tentativas de retry com backoff, fazendo o servidor MCP gastar no mínimo $14s$ (backoff) + $4 \times (\text{latência de rede})$ antes de falhar fora do loop e retornar o erro final. Se o cliente MCP tiver um timeout menor, ele desiste e a requisição no servidor continua em background até o final do loop/timeout.

### 4.2. Hypothesis 2: LLM Client Configurado com Timeout Excessivo

  - **Rationale/Evidence:** O `ClientConfig.Timeout` padrão é de `60 * time.Second` (em `config/config.go`). O `MaxRetries` padrão é `3`. O tempo total de espera do cliente LLM no servidor MCP é de $4 \times (\text{Timeout do HTTP Client}) + \text{Backoff}$. Se o timeout do cliente MCP for menor que, digamos, $30$ segundos, o servidor MCP fará um trabalho considerável (e lento) antes de desistir, mas a resposta nunca chegará ao cliente.
  - **Code (if relevant):**
    ```go
    // analyze/client.go:88
    Timeout: 60 * time.Second, // Default timeout
    // analyze/client.go:89
    MaxRetries: 3, // Default max retries (4 attempts total)
    ```
  - **How it leads to the bug:** O tempo de espera configurado na lógica de retry é muito longo para a expectativa de resposta do cliente MCP.

### 4.3. Most Likely Cause(s)

**Hypothesis 1** é a causa mais provável para o **atraso excessivo** que faz a requisição parecer travada, especialmente quando o LLM retorna um erro imediato (400) que não é retryable (429) ou sucesso (200). O loop de retry em `analyze/anthropic.go` (e possivelmente `openai.go`, que tem uma estrutura similar) deve ser corrigido para evitar retries em erros de cliente (4xx) não relacionados a rate limiting.

-----

## 5\. Supporting Evidence from Code (if `File Structure` is provided)

A lógica em `analyze/anthropic.go` é o exemplo mais claro da falha de *break* no loop:

```go
// analyze/anthropic.go:173
	for attempt := 0; attempt <= c.config.MaxRetries; attempt++ {
		// ... (sleep de backoff)
		resp, lastErr = c.httpClient.Do(httpReq)
		if lastErr == nil {
			// Check for rate limiting
			if resp.StatusCode == http.StatusTooManyRequests {
				resp.Body.Close()
				lastErr = ErrRateLimited
				continue
			}
			if resp.StatusCode == http.StatusOK {
				break // <-- Só quebra aqui (sucesso)
			}
			// *** PROBLEMA: Outros status 4xx/5xx (onde lastErr é nil) continuam o loop e o sleep
		}
		// ...
	}
	
	// A única lógica que trata o erro 400 está fora do loop:
	// analyze/anthropic.go:198-202
	if resp.StatusCode != http.StatusOK {
		// ... lê body e retorna erro
	}
```

## 6\. Recommended Steps for Debugging and Verification

O foco deve ser garantir que o loop de retry seja estrito quanto ao que é retryable.

  * **Logging:**

      * Adicionar logging detalhado no loop de retry de **todos os clientes** (`OpenAIClient`, `AnthropicClient`, `GeminiClient`, `OllamaClient`).
          * Logar o `attempt` atual: `logger.debug("Attempt %d for %s", attempt, model)`
          * Logar o `resp.StatusCode` e o `lastErr` após `c.httpClient.Do(httpReq)`.
          * Logar o valor de `ctx.Err()` antes de `return nil, ErrTimeout` para confirmar o cancelamento.
      * Em `mcp/main.go` nos handlers, logar o valor de `cfg.LLM.Timeout` e `cfg.LLM.MaxRetries` antes de iniciar a requisição.

  * **Breakpoints:**

      * Setar um breakpoint em `analyze/anthropic.go:176` (dentro do `if lastErr == nil`) para inspecionar o `resp.StatusCode` quando for diferente de 200 e 429.
      * Setar um breakpoint em `time.Sleep(...)` em todos os clientes para verificar se o backoff é executado em erros não-retryable.

  * **Test Scenarios/Requests:**

      * **Cenário 1 (400 Bad Request):** Use uma API key válida (Anthropic) e um prompt conhecido por falhar (ex: prompt muito longo ou formato inválido - se possível simular) e observe se o loop faz 4 retries desnecessários.
      * **Cenário 2 (Timeout):** Configure `cfg.LLM.Timeout = 1s` e `cfg.LLM.MaxRetries = 1` e verifique se o erro retorna rapidamente.

  * **Refinamento do Código (Correção Sugerida):**

      * Modificar a lógica de retry para quebrar o loop (usando `return` ou `break`) imediatamente em *qualquer* `resp.StatusCode` que não seja `429` (Rate Limit) ou `5xx` (Server Error). Códigos `4xx` (exceto 429) são erros de cliente e não devem ser repetidos.

<!-- end list -->

```go
// Correção Sugerida para analyze/anthropic.go:176
// ...
		if lastErr == nil {
			// Check for retryable HTTP codes
			switch {
			case resp.StatusCode == http.StatusOK:
				break // Success
			case resp.StatusCode == http.StatusTooManyRequests:
				resp.Body.Close()
				lastErr = ErrRateLimited
				continue // Retry for Rate Limit
			case resp.StatusCode >= 500:
				resp.Body.Close()
				lastErr = fmt.Errorf("server error: %d", resp.StatusCode)
				continue // Retry for Server Error
			default:
				// All other codes (e.g., 400 Bad Request) are not retryable client errors
				break // Exit the retry loop to handle the error outside
			}
		}
// ...
```

## 7\. Bug Impact Assessment

O bug impede o uso das funcionalidades de LLM do servidor MCP, forçando o cliente a aguardar (e possivelmente fazer timeout) por um período de tempo desnecessariamente longo ($4$ minutos se o timeout do cliente HTTP for $60$s) em caso de erro. Isso degrada a experiência do usuário e desperdiça recursos do servidor.

## 8\. Assumptions Made During Analysis

  * A versão CLI está usando a mesma lógica de `analyze/` package, mas possivelmente com uma configuração de retry/timeout diferente.
  * O cliente MCP tem um timeout menor que o tempo total de execução do loop de retry do servidor.
  * O sintoma de "processando eternamente" é, na verdade, um timeout prolongado que o cliente MCP não gerencia bem.
  * Os `GeminiClient` e `OpenAIClient` têm lógica de retry semelhante ao `AnthropicClient` para o loop principal de decisão de `break`/`continue`/`retry`.

## 9\. Open Questions / Areas for Further Investigation

  * Qual é o provedor LLM configurado (`config.LLM.Provider`) quando o bug ocorre? (Gemini, Anthropic, OpenAI ou Ollama?)
  * Qual é o timeout configurado no cliente que faz a chamada ao `codemap-mcp`?
  * O erro de status (ex: 400) está ocorrendo devido ao tamanho/formato do prompt (relacionado ao `explain_symbol`/`summarize_module` em particular) ou é um erro de credenciais/configuração?
</file>
<file path="CLAUDE.md">
# CLAUDE.md: Claude Code Configuration

Welcome, Claude. This document provides persistent context for working with the `codemap` repository.

## Project Overview
`codemap` is a sophisticated command-line interface (CLI) tool written in Go. Its core function is to transform a codebase into a structural **Knowledge Graph** using `tree-sitter` for deep parsing. This graph is then leveraged for intelligent, context-aware analysis via **Retrieval-Augmented Generation (RAG)** with various LLMs (Anthropic, OpenAI, Ollama, Gemini). The architecture is a highly decoupled, layered pipeline.

## Build & Run Commands

The project is a CLI tool. All interactions are via the main executable.

```bash
# 1. Build the main CLI executable
go build -o codemap .

# 2. Build tree-sitter grammars (CRITICAL SETUP)
# This compiles the C grammars using purego and is a necessary one-time setup.
# MUST be re-run if any file in scanner/queries/*.scm is modified.
make grammars

# 3. Common Operational Modes

# Index Mode: Builds or updates the knowledge graph index (.codemap/graph.gob)
./codemap --index .
./codemap --index --force . # Force a full rebuild

# LLM Explain Mode: Uses LLM to explain a specific symbol, retrieving context from the graph
./codemap --explain --symbol "graph.NewBuilder" --model claude-3-opus .

# LLM Search Mode: Performs hybrid semantic and graph-based search
./codemap --search --q "where is the LLM client initialized" .

# Trace Path Mode: Finds the call chain path between two symbols
./codemap --query --from "main.main" --to "graph.Store.Save" .

# Dependency Mode: Generates a dependency flow map (functions, types, imports)
./codemap --deps .

# Default Mode: Generates a file tree view with token/size estimates
./codemap .

# JSON Output: Use the --json flag for machine-readable output in any mode
./codemap --deps . --json
```

## Architecture Overview

The system operates as a data processing pipeline, with clear separation of concerns across four layers.

### 1. Core Components and Responsibilities
| Component (Package) | Layer | Purpose and Responsibility | Design Pattern |
| :--- | :--- | :--- | :--- |
| **`scanner`** | Data Acquisition | Orchestrates `tree-sitter` to generate ASTs and extract raw structural data (Symbols, Calls, Dependencies) using language-specific `.scm` queries. | Data Mapper |
| **`graph`** | Knowledge/Data | The central domain layer. Manages `Node`/`Edge` creation, persistence, and vector store management (`vectors.go`). Acts as the Repository for code knowledge. | Repository Pattern |
| **`analyze`** | Intelligence | Abstracts all LLM interactions. Handles prompt construction, token counting, embedding generation, and the RAG pipeline via `retriever.go`. | Strategy/Factory Pattern |
| **`render`** | Presentation | Formats analyzed data for user consumption (TUI via `bubbletea`, dependency graphs, skyline views). | Presentation Layer |
| **`config`** | Configuration | Centralized management of application settings, including LLM provider selection and API keys. | Configuration Root |

### 2. LLM Integration Strategy (RAG Pipeline)
The `/analyze` package isolates the core analysis logic from external LLM services.

*   **Interface Contract:** The `analyze.Client` interface (in `analyze/client.go`) defines the contract for all LLM operations (`GenerateResponse`, `GetTokenCount`).
*   **Factory Pattern:** `analyze/factory.go` instantiates the correct concrete client (`anthropic.go`, `openai.go`, `ollama.go`, `gemini.go`) based on the `LLM.Provider` configuration.
*   **RAG Flow:** The `analyze.Retriever` service performs vector similarity searches against the `graph/vectors.go` store to retrieve relevant code context, which is then injected into the LLM prompt. The logic in `/analyze/tokens.go` is crucial for managing context window limits.

## Development Conventions & Gotchas

### Go Style and Error Handling
*   **Language:** Standard Go (Golang). Ensure all code passes `go fmt` and `go vet`.
*   **Dependency Injection:** Dependencies are explicitly instantiated and passed in `main.go` (Constructor Injection). Avoid global state where possible.
*   **Error Handling:** Errors are checked immediately. Critical failures (e.g., missing configuration, failed graph load) print a descriptive message to `os.Stderr` and exit with `os.Exit(1)`.

### Data Persistence and State Management
*   **Primary State:** The `graph.Store` is the single source of truth. It is persisted to `.codemap/graph.gob` using the **`encoding/gob` format** for fast, efficient serialization of complex Go objects.
*   **Deterministic IDs:** Graph nodes use a stable, deterministic ID generation scheme (`graph.GenerateNodeID`) based on file path and symbol name. This is crucial for incremental indexing.
*   **Caching:** LLM responses are cached in the local file system (`.codemap/cache`). When testing new prompt engineering or model changes, always use the `--no-cache` flag to force a fresh API call and bypass the `cache` package.

### Tree-sitter Gotcha (CRITICAL)
The `scanner` package relies on compiled C libraries for `tree-sitter` grammars, managed via `github.com/ebitengine/purego`. If you modify any grammar query file (`scanner/queries/*.scm`), you **must** run `make grammars` to recompile the shared libraries before the `scanner` package will reflect the changes. The application will fail with a "missing grammar" error if this step is skipped.

### LLM Resilience
The `analyze` package implements **retry logic** for transient external API errors (e.g., network issues, rate limits). It attempts a maximum of **3 retries** with exponential backoff before failing the request. When debugging LLM issues, check the `analyze` package for the specific client's error handling.

### Configuration Priority
The `config` package prioritizes settings in this order:
1. Environment Variables (e.g., `ANTHROPIC_API_KEY`)
2. Project-level YAML (`.codemap/config.yaml`)
3. User-level YAML (`~/.config/codemap/config.yaml`)
4. Hardcoded defaults.
</file>
<file path="CODE_OF_CONDUCT.md">
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, religion, or sexual identity
and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the
  overall community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or
  advances of any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email
  address, without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
jordancoinjackson@gmail.com.
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series
of actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or
permanent ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior,  harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within
the community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.0, available at
https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.

Community Impact Guidelines were inspired by [Mozilla's code of conduct
enforcement ladder](https://github.com/mozilla/diversity).

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see the FAQ at
https://www.contributor-covenant.org/faq. Translations are available at
https://www.contributor-covenant.org/translations.
</file>
<file path="codemap.rb">
class Codemap < Formula
  desc "Generate a brain map of your codebase for LLM context"
  homepage "https://github.com/JordanCoin/codemap"
  url "https://github.com/JordanCoin/codemap/archive/refs/tags/v2.8.tar.gz"
  sha256 "d8c709f21de099277474f25c4d02ed9ab065d5f3036b1de1c0cbf89ad800e746"
  license "MIT"

  depends_on "go" => :build

  resource "tree-sitter-go" do
    url "https://github.com/tree-sitter/tree-sitter-go/archive/refs/heads/master.tar.gz"
    sha256 "1c746cac06741178f3b0b21258a8ff04599f3bef176044121b985ff17917bf25"
  end

  resource "tree-sitter-python" do
    url "https://github.com/tree-sitter/tree-sitter-python/archive/refs/heads/master.tar.gz"
    sha256 "874479d1f7058159f417fdeb706f279e4edd6b2c392bd8a642bc5d552f45e9cd"
  end

  resource "tree-sitter-javascript" do
    url "https://github.com/tree-sitter/tree-sitter-javascript/archive/refs/heads/master.tar.gz"
    sha256 "0ee41a73f53ddc31bd6567cafa5864f0678e71ebb9c9c618f6d67ca788a22455"
  end

  resource "tree-sitter-typescript" do
    url "https://github.com/tree-sitter/tree-sitter-typescript/archive/refs/heads/master.tar.gz"
    sha256 "c5bf6e925d299bce34e7ae373b46d5363f2e896e6ea282091d163fd1f831b497"
  end

  resource "tree-sitter-rust" do
    url "https://github.com/tree-sitter/tree-sitter-rust/archive/refs/heads/master.tar.gz"
    sha256 "dc93e09d9ea2b20e91c87c6202b53dc57513e0db3730231c60d154582a74dfc0"
  end

  resource "tree-sitter-ruby" do
    url "https://github.com/tree-sitter/tree-sitter-ruby/archive/refs/heads/master.tar.gz"
    sha256 "93df5566a3bee1c16f6fb7da492d58a34840d488347dc05a2327e3730e53aa20"
  end

  resource "tree-sitter-c" do
    url "https://github.com/tree-sitter/tree-sitter-c/archive/refs/heads/master.tar.gz"
    sha256 "d4e3f07154466ad60c7bb1cc9719be3bc2e8b9212d39849f6aa4a2b90cac2415"
  end

  resource "tree-sitter-cpp" do
    url "https://github.com/tree-sitter/tree-sitter-cpp/archive/refs/heads/master.tar.gz"
    sha256 "6c8f7d6bf2203b35490f5c91f81ed644a8dfb0a1e0274f15fa8e47b0b17f0d9c"
  end

  resource "tree-sitter-java" do
    url "https://github.com/tree-sitter/tree-sitter-java/archive/refs/heads/master.tar.gz"
    sha256 "a41ad5bd64ed71a026b001434319e7bcdc7e44ca1e28c39a87ab9e7ea1b1c575"
  end

  resource "tree-sitter-swift" do
    url "https://github.com/tree-sitter/tree-sitter-swift/archive/refs/heads/master.tar.gz"
    sha256 "2576f1f8da5ffa199a5b4bf1a21564106fa1f94fec19e4a375796d2286ac96bd"
  end

  resource "tree-sitter-bash" do
    url "https://github.com/tree-sitter/tree-sitter-bash/archive/refs/heads/master.tar.gz"
    sha256 "86ffaeafc9d3a01a3da828b0fab5ef1b9d765d245764b066794577daee512f23"
  end

  resource "tree-sitter-kotlin" do
    url "https://github.com/fwcd/tree-sitter-kotlin/archive/refs/heads/main.tar.gz"
    sha256 "4c63bab70f70bb884d97446a14ff78037944ff7c389f0482b5f49e4a32fe24c4"
  end

  resource "tree-sitter-c-sharp" do
    url "https://github.com/tree-sitter/tree-sitter-c-sharp/archive/refs/heads/master.tar.gz"
    sha256 "81d4d39508dad98a56969acb76e979945f1eb8c9bf3e5921a4b66bd634dc1266"
  end

  resource "tree-sitter-php" do
    url "https://github.com/tree-sitter/tree-sitter-php/archive/refs/heads/master.tar.gz"
    sha256 "df40197bb1bee56f96e52dc301ddd7e2ef0b33284dcd50de0a2bc53bd140e9a1"
  end

  resource "tree-sitter-dart" do
    url "https://github.com/UserNobody14/tree-sitter-dart/archive/refs/heads/master.tar.gz"
    sha256 "a38f682088b39813ed271eae942a3ad3c42c3c6c0086bd3484e8cdea26e04e0c"
  end

  resource "tree-sitter-r" do
    url "https://github.com/r-lib/tree-sitter-r/archive/refs/heads/main.tar.gz"
    sha256 "b6e0df92204cbd956790b20165adf13f2a9879cda5e86b70949d918302634e0f"
  end

  def install
    # Build the main Go binary
    system "go", "build", "-o", libexec/"codemap", "."

    # Create grammars directory
    (libexec/"grammars").mkpath

    # Build and install each grammar resource
    resources.each do |r|
      r.stage do
        lang = r.name.sub("tree-sitter-", "").tr("-", "_")

        # Handle special source directories
        src_subdir = "src"
        src_subdir = "typescript/src" if lang == "typescript"
        src_subdir = "php/src" if lang == "php"

        src_dir = Pathname.pwd/src_subdir

        # Determine library extension and flags
        lib_ext = OS.mac? ? "dylib" : "so"
        cflags = OS.mac? ? %w[-dynamiclib -fPIC] : %w[-shared -fPIC]

        output_lib = libexec/"grammars/libtree-sitter-#{lang}.#{lib_ext}"

        # Prepare sources
        sources = [src_dir/"parser.c"]

        if (src_dir/"scanner.c").exist?
          sources << (src_dir/"scanner.c")
        elsif (src_dir/"scanner.cc").exist?
          # Compile C++ scanner first
          system ENV.cxx, "-c", "-fPIC", src_dir/"scanner.cc", "-o", "scanner.o", "-I#{src_dir}"
          sources << "scanner.o"
        end

        # Compile and link
        system ENV.cc, *cflags, "-o", output_lib, *sources, "-I#{src_dir}"
      end
    end

    # Create wrapper script
    (bin/"codemap").write <<~EOS
      #!/bin/bash
      export CODEMAP_GRAMMAR_DIR="#{libexec}/grammars"
      export CODEMAP_QUERY_DIR="#{libexec}/queries"
      exec "#{libexec}/codemap" "$@"
    EOS
  end

  test do
    system bin/"codemap", "."
  end
end
</file>
<file path="CONTRIBUTING.md">
# Contributing to codemap

Thanks for your interest in contributing! Here's how to get involved.

## Quick Contributions

- **Bug reports**: Open an issue with reproduction steps
- **Feature ideas**: Open an issue to discuss first
- **Documentation**: PRs welcome for README, examples, etc.

## Adding a New Language

Want to add support for a language like Clojure, Elixir, Scala, etc.? Here's what's needed:

### 1. Add grammar to `release.yml`

In `.github/workflows/release.yml`, add a line to the `GRAMMARS` env var:

```yaml
env:
  GRAMMARS: |
    go:tree-sitter/tree-sitter-go:master:src
    # ... existing grammars ...
    clojure:sogaiu/tree-sitter-clojure:main:src   # <- add yours
```

Format: `name:github_org/repo:branch:src_dir`

Most grammars use `src` as the source directory, but check the repo structure.

### 2. Create a query file

Create `scanner/queries/<lang>.scm` to define what to capture:

```scm
; Functions - capture function/method definitions
(function_definition
  name: (identifier) @function)

; Imports - capture import/require statements
(import_clause
  (identifier) @import)
```

**Finding the right node types:**
- Use [tree-sitter playground](https://tree-sitter.github.io/tree-sitter/7-playground.html)
- Check the grammar repo's `grammar.js` for node names
- Look at existing queries in `scanner/queries/` for examples

### 3. Add extension mapping

In `scanner/grammar.go`, add to the `extToLang` map:

```go
var extToLang = map[string]string{
    // ... existing mappings ...
    ".clj":  "clojure",
    ".cljs": "clojure",
    ".cljc": "clojure",
}
```

### 4. Open a PR

That's it from your side! Open a PR with these 3 changes.

**What happens next:**
- Maintainer reviews the PR
- Maintainer updates the Homebrew formula (`codemap.rb`) with the new grammar
- On next release, all platforms get the new language automatically

## Development Setup

```bash
git clone https://github.com/JordanCoin/codemap.git
cd codemap

# Build the binary
go build -o codemap .

# Build grammars for --deps mode (requires clang/gcc)
make deps

# Test it
./codemap .
./codemap --deps .
```

## Code Style

- Keep it simple - this is a CLI tool, not a framework
- Run `go fmt` before committing
- Test your changes with `./codemap` on a real project

## Questions?

Open an issue or reach out. We're happy to help!
</file>
<file path="GEMINI.md">
# codemap: Codebase Analysis and Visualization Tool

## Project Context
**codemap** is a Go-based CLI tool designed to analyze, map, and visualize codebase structures. It bridges the gap between raw source code and high-level architectural understanding, specifically optimized for:
1.  **Developer Insight:** Providing visual maps (File Tree, Dependency Graph, City Skyline) of complex projects.
2.  **AI Context:** Generating structured, token-efficient JSON outputs to feed Large Language Models (LLMs) via the Model Context Protocol (MCP).

## Development Workflow

### Prerequisites
- **Go**: 1.24+
- **C/C++ Compiler**: Required for building Tree-sitter grammars (gcc/clang).

### Key Commands
The project uses a `Makefile` for common tasks:

| Task | Command | Description |
| :--- | :--- | :--- |
| **Build CLI** | `make build` | Compiles the `codemap` binary. |
| **Build MCP** | `make build-mcp` | Compiles the MCP server binary (`codemap-mcp`). |
| **Setup Grammars** | `make grammars` | **Required for --deps mode.** Downloads and builds Tree-sitter libraries. |
| **Run (Tree)** | `make run DIR=.` | Runs the tool in default tree mode on the current directory. |
| **Run (Deps)** | `make deps DIR=.` | Runs dependency analysis (requires grammars). |
| **Clean** | `make clean` | Removes binaries and grammar artifacts. |

### Manual Execution
```bash
# Basic file tree structure
./codemap .

# Deep dependency analysis (requires make grammars first)
./codemap --deps .

# Skyline visualization with animation
./codemap --skyline --animate .

# JSON output for LLM consumption
./codemap --json .
```

## Architecture

The application follows a strict unidirectional pipeline:
`Orchestration (main)` → `Acquisition (scanner)` → `Presentation (render)`

### Core Packages
*   **`scanner/`**: The engine room.
    *   **`walker.go`**: Handles file system traversal and Git integration (respecting `.gitignore`).
    *   **`deps.go`**: Uses **Tree-sitter** to parse code and extract symbols (functions, types) and imports.
    *   **`types.go`**: Defines the domain models (`Project`, `DepsProject`, `FileInfo`). **This is the contract between scanner and render.**
*   **`render/`**: The presentation layer.
    *   **`tree.go`**: Renders the standard file tree.
    *   **`skyline.go`**: Renders the 3D ASCII city visualization.
    *   **`depgraph.go`**: Renders the dependency flow.
*   **`mcp/`**: Model Context Protocol server implementation.
    *   Exposes analysis capabilities as tools (`get_structure`, `get_dependencies`, etc.) to LLM agents.

## Roadmap & Future Features (from PLAN.md)
The project is evolving from a structural mapper to a semantic understanding system ("GraphRAG").

*   **Knowledge Graph**: Moving to a graph-based data model (Nodes/Edges) to track calls and complex relationships.
*   **Hybrid Retrieval**: Combining structural search (graph) with semantic search (embeddings).
*   **LLM Integration**: Adding features to "explain" symbols or "trace" paths using local LLMs.

## Conventions
*   **Style**: Standard Go formatting (`go fmt`).
*   **Error Handling**: Fail fast with clear error messages to stderr.
*   **Dependencies**: Explicit dependency injection is preferred over global state.
*   **Grammars**: Tree-sitter grammars are loaded dynamically via `purego`. New languages require adding a `.scm` query file in `scanner/queries/` and updating `release.yml`.
</file>
<file path="go.mod">
module codemap

go 1.24.0

require (
	github.com/charmbracelet/bubbletea v1.3.10
	github.com/ebitengine/purego v0.9.1
	github.com/modelcontextprotocol/go-sdk v1.1.0
	github.com/sabhiram/go-gitignore v0.0.0-20210923224102-525f6e181f06
	github.com/tree-sitter/go-tree-sitter v0.25.0
	golang.org/x/term v0.37.0
	gopkg.in/yaml.v3 v3.0.1
)

require (
	github.com/aymanbagabas/go-osc52/v2 v2.0.1 // indirect
	github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc // indirect
	github.com/charmbracelet/lipgloss v1.1.0 // indirect
	github.com/charmbracelet/x/ansi v0.10.1 // indirect
	github.com/charmbracelet/x/cellbuf v0.0.13-0.20250311204145-2c3ea96c31dd // indirect
	github.com/charmbracelet/x/term v0.2.1 // indirect
	github.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f // indirect
	github.com/google/jsonschema-go v0.3.0 // indirect
	github.com/joho/godotenv v1.5.1 // indirect
	github.com/lucasb-eyer/go-colorful v1.2.0 // indirect
	github.com/mattn/go-isatty v0.0.20 // indirect
	github.com/mattn/go-localereader v0.0.1 // indirect
	github.com/mattn/go-pointer v0.0.1 // indirect
	github.com/mattn/go-runewidth v0.0.16 // indirect
	github.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6 // indirect
	github.com/muesli/cancelreader v0.2.2 // indirect
	github.com/muesli/termenv v0.16.0 // indirect
	github.com/rivo/uniseg v0.4.7 // indirect
	github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e // indirect
	github.com/yosida95/uritemplate/v3 v3.0.2 // indirect
	golang.org/x/oauth2 v0.30.0 // indirect
	golang.org/x/sys v0.38.0 // indirect
	golang.org/x/text v0.3.8 // indirect
)
</file>
<file path="go.sum">
github.com/aymanbagabas/go-osc52/v2 v2.0.1 h1:HwpRHbFMcZLEVr42D4p7XBqjyuxQH5SMiErDT4WkJ2k=
github.com/aymanbagabas/go-osc52/v2 v2.0.1/go.mod h1:uYgXzlJ7ZpABp8OJ+exZzJJhRNQ2ASbcXHWsFqH8hp8=
github.com/charmbracelet/bubbletea v1.3.10 h1:otUDHWMMzQSB0Pkc87rm691KZ3SWa4KUlvF9nRvCICw=
github.com/charmbracelet/bubbletea v1.3.10/go.mod h1:ORQfo0fk8U+po9VaNvnV95UPWA1BitP1E0N6xJPlHr4=
github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc h1:4pZI35227imm7yK2bGPcfpFEmuY1gc2YSTShr4iJBfs=
github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc/go.mod h1:X4/0JoqgTIPSFcRA/P6INZzIuyqdFY5rm8tb41s9okk=
github.com/charmbracelet/lipgloss v1.1.0 h1:vYXsiLHVkK7fp74RkV7b2kq9+zDLoEU4MZoFqR/noCY=
github.com/charmbracelet/lipgloss v1.1.0/go.mod h1:/6Q8FR2o+kj8rz4Dq0zQc3vYf7X+B0binUUBwA0aL30=
github.com/charmbracelet/x/ansi v0.10.1 h1:rL3Koar5XvX0pHGfovN03f5cxLbCF2YvLeyz7D2jVDQ=
github.com/charmbracelet/x/ansi v0.10.1/go.mod h1:3RQDQ6lDnROptfpWuUVIUG64bD2g2BgntdxH0Ya5TeE=
github.com/charmbracelet/x/cellbuf v0.0.13-0.20250311204145-2c3ea96c31dd h1:vy0GVL4jeHEwG5YOXDmi86oYw2yuYUGqz6a8sLwg0X8=
github.com/charmbracelet/x/cellbuf v0.0.13-0.20250311204145-2c3ea96c31dd/go.mod h1:xe0nKWGd3eJgtqZRaN9RjMtK7xUYchjzPr7q6kcvCCs=
github.com/charmbracelet/x/term v0.2.1 h1:AQeHeLZ1OqSXhrAWpYUtZyX1T3zVxfpZuEQMIQaGIAQ=
github.com/charmbracelet/x/term v0.2.1/go.mod h1:oQ4enTYFV7QN4m0i9mzHrViD7TQKvNEEkHUMCmsxdUg=
github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/ebitengine/purego v0.9.1 h1:a/k2f2HQU3Pi399RPW1MOaZyhKJL9w/xFpKAg4q1s0A=
github.com/ebitengine/purego v0.9.1/go.mod h1:iIjxzd6CiRiOG0UyXP+V1+jWqUXVjPKLAI0mRfJZTmQ=
github.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f h1:Y/CXytFA4m6baUTXGLOoWe4PQhGxaX0KpnayAqC48p4=
github.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f/go.mod h1:vw97MGsxSvLiUE2X8qFplwetxpGLQrlU1Q9AUEIzCaM=
github.com/google/go-cmp v0.7.0 h1:wk8382ETsv4JYUZwIsn6YpYiWiBsYLSJiTsyBybVuN8=
github.com/google/go-cmp v0.7.0/go.mod h1:pXiqmnSA92OHEEa9HXL2W4E7lf9JzCmGVUdgjX3N/iU=
github.com/google/jsonschema-go v0.3.0 h1:6AH2TxVNtk3IlvkkhjrtbUc4S8AvO0Xii0DxIygDg+Q=
github.com/google/jsonschema-go v0.3.0/go.mod h1:r5quNTdLOYEz95Ru18zA0ydNbBuYoo9tgaYcxEYhJVE=
github.com/joho/godotenv v1.5.1 h1:7eLL/+HRGLY0ldzfGMeQkb7vMd0as4CfYvUVzLqw0N0=
github.com/joho/godotenv v1.5.1/go.mod h1:f4LDr5Voq0i2e/R5DDNOoa2zzDfwtkZa6DnEwAbqwq4=
github.com/lucasb-eyer/go-colorful v1.2.0 h1:1nnpGOrhyZZuNyfu1QjKiUICQ74+3FNCN69Aj6K7nkY=
github.com/lucasb-eyer/go-colorful v1.2.0/go.mod h1:R4dSotOR9KMtayYi1e77YzuveK+i7ruzyGqttikkLy0=
github.com/mattn/go-isatty v0.0.20 h1:xfD0iDuEKnDkl03q4limB+vH+GxLEtL/jb4xVJSWWEY=
github.com/mattn/go-isatty v0.0.20/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=
github.com/mattn/go-localereader v0.0.1 h1:ygSAOl7ZXTx4RdPYinUpg6W99U8jWvWi9Ye2JC/oIi4=
github.com/mattn/go-localereader v0.0.1/go.mod h1:8fBrzywKY7BI3czFoHkuzRoWE9C+EiG4R1k4Cjx5p88=
github.com/mattn/go-pointer v0.0.1 h1:n+XhsuGeVO6MEAp7xyEukFINEa+Quek5psIR/ylA6o0=
github.com/mattn/go-pointer v0.0.1/go.mod h1:2zXcozF6qYGgmsG+SeTZz3oAbFLdD3OWqnUbNvJZAlc=
github.com/mattn/go-runewidth v0.0.16 h1:E5ScNMtiwvlvB5paMFdw9p4kSQzbXFikJ5SQO6TULQc=
github.com/mattn/go-runewidth v0.0.16/go.mod h1:Jdepj2loyihRzMpdS35Xk/zdY8IAYHsh153qUoGf23w=
github.com/modelcontextprotocol/go-sdk v1.1.0 h1:Qjayg53dnKC4UZ+792W21e4BpwEZBzwgRW6LrjLWSwA=
github.com/modelcontextprotocol/go-sdk v1.1.0/go.mod h1:6fM3LCm3yV7pAs8isnKLn07oKtB0MP9LHd3DfAcKw10=
github.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6 h1:ZK8zHtRHOkbHy6Mmr5D264iyp3TiX5OmNcI5cIARiQI=
github.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6/go.mod h1:CJlz5H+gyd6CUWT45Oy4q24RdLyn7Md9Vj2/ldJBSIo=
github.com/muesli/cancelreader v0.2.2 h1:3I4Kt4BQjOR54NavqnDogx/MIoWBFa0StPA8ELUXHmA=
github.com/muesli/cancelreader v0.2.2/go.mod h1:3XuTXfFS2VjM+HTLZY9Ak0l6eUKfijIfMUZ4EgX0QYo=
github.com/muesli/termenv v0.16.0 h1:S5AlUN9dENB57rsbnkPyfdGuWIlkmzJjbFf0Tf5FWUc=
github.com/muesli/termenv v0.16.0/go.mod h1:ZRfOIKPFDYQoDFF4Olj7/QJbW60Ol/kL1pU3VfY/Cnk=
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/rivo/uniseg v0.2.0/go.mod h1:J6wj4VEh+S6ZtnVlnTBMWIodfgj8LQOQFoIToxlJtxc=
github.com/rivo/uniseg v0.4.7 h1:WUdvkW8uEhrYfLC4ZzdpI2ztxP1I582+49Oc5Mq64VQ=
github.com/rivo/uniseg v0.4.7/go.mod h1:FN3SvrM+Zdj16jyLfmOkMNblXMcoc8DfTHruCPUcx88=
github.com/sabhiram/go-gitignore v0.0.0-20210923224102-525f6e181f06 h1:OkMGxebDjyw0ULyrTYWeN0UNCCkmCWfjPnIA2W6oviI=
github.com/sabhiram/go-gitignore v0.0.0-20210923224102-525f6e181f06/go.mod h1:+ePHsJ1keEjQtpvf9HHw0f4ZeJ0TLRsxhunSI2hYJSs=
github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
github.com/stretchr/testify v1.6.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
github.com/stretchr/testify v1.10.0 h1:Xv5erBjTwe/5IxqUQTdXv5kgmIvbHo3QQyRwhJsOfJA=
github.com/stretchr/testify v1.10.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=
github.com/tree-sitter/go-tree-sitter v0.25.0 h1:sx6kcg8raRFCvc9BnXglke6axya12krCJF5xJ2sftRU=
github.com/tree-sitter/go-tree-sitter v0.25.0/go.mod h1:r77ig7BikoZhHrrsjAnv8RqGti5rtSyvDHPzgTPsUuU=
github.com/tree-sitter/tree-sitter-c v0.23.4 h1:nBPH3FV07DzAD7p0GfNvXM+Y7pNIoPenQWBpvM++t4c=
github.com/tree-sitter/tree-sitter-c v0.23.4/go.mod h1:MkI5dOiIpeN94LNjeCp8ljXN/953JCwAby4bClMr6bw=
github.com/tree-sitter/tree-sitter-cpp v0.23.4 h1:LaWZsiqQKvR65yHgKmnaqA+uz6tlDJTJFCyFIeZU/8w=
github.com/tree-sitter/tree-sitter-cpp v0.23.4/go.mod h1:doqNW64BriC7WBCQ1klf0KmJpdEvfxyXtoEybnBo6v8=
github.com/tree-sitter/tree-sitter-embedded-template v0.23.2 h1:nFkkH6Sbe56EXLmZBqHHcamTpmz3TId97I16EnGy4rg=
github.com/tree-sitter/tree-sitter-embedded-template v0.23.2/go.mod h1:HNPOhN0qF3hWluYLdxWs5WbzP/iE4aaRVPMsdxuzIaQ=
github.com/tree-sitter/tree-sitter-go v0.23.4 h1:yt5KMGnTHS+86pJmLIAZMWxukr8W7Ae1STPvQUuNROA=
github.com/tree-sitter/tree-sitter-go v0.23.4/go.mod h1:Jrx8QqYN0v7npv1fJRH1AznddllYiCMUChtVjxPK040=
github.com/tree-sitter/tree-sitter-html v0.23.2 h1:1UYDV+Yd05GGRhVnTcbP58GkKLSHHZwVaN+lBZV11Lc=
github.com/tree-sitter/tree-sitter-html v0.23.2/go.mod h1:gpUv/dG3Xl/eebqgeYeFMt+JLOY9cgFinb/Nw08a9og=
github.com/tree-sitter/tree-sitter-java v0.23.5 h1:J9YeMGMwXYlKSP3K4Us8CitC6hjtMjqpeOf2GGo6tig=
github.com/tree-sitter/tree-sitter-java v0.23.5/go.mod h1:NRKlI8+EznxA7t1Yt3xtraPk1Wzqh3GAIC46wxvc320=
github.com/tree-sitter/tree-sitter-javascript v0.23.1 h1:1fWupaRC0ArlHJ/QJzsfQ3Ibyopw7ZfQK4xXc40Zveo=
github.com/tree-sitter/tree-sitter-javascript v0.23.1/go.mod h1:lmGD1EJdCA+v0S1u2fFgepMg/opzSg/4pgFym2FPGAs=
github.com/tree-sitter/tree-sitter-json v0.24.8 h1:tV5rMkihgtiOe14a9LHfDY5kzTl5GNUYe6carZBn0fQ=
github.com/tree-sitter/tree-sitter-json v0.24.8/go.mod h1:F351KK0KGvCaYbZ5zxwx/gWWvZhIDl0eMtn+1r+gQbo=
github.com/tree-sitter/tree-sitter-php v0.23.11 h1:iHewsLNDmznh8kgGyfWfujsZxIz1YGbSd2ZTEM0ZiP8=
github.com/tree-sitter/tree-sitter-php v0.23.11/go.mod h1:T/kbfi+UcCywQfUNAJnGTN/fMSUjnwPXA8k4yoIks74=
github.com/tree-sitter/tree-sitter-python v0.23.6 h1:qHnWFR5WhtMQpxBZRwiaU5Hk/29vGju6CVtmvu5Haas=
github.com/tree-sitter/tree-sitter-python v0.23.6/go.mod h1:cpdthSy/Yoa28aJFBscFHlGiU+cnSiSh1kuDVtI8YeM=
github.com/tree-sitter/tree-sitter-ruby v0.23.1 h1:T/NKHUA+iVbHM440hFx+lzVOzS4dV6z8Qw8ai+72bYo=
github.com/tree-sitter/tree-sitter-ruby v0.23.1/go.mod h1:kUS4kCCQloFcdX6sdpr8p6r2rogbM6ZjTox5ZOQy8cA=
github.com/tree-sitter/tree-sitter-rust v0.23.2 h1:6AtoooCW5GqNrRpfnvl0iUhxTAZEovEmLKDbyHlfw90=
github.com/tree-sitter/tree-sitter-rust v0.23.2/go.mod h1:hfeGWic9BAfgTrc7Xf6FaOAguCFJRo3RBbs7QJ6D7MI=
github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e h1:JVG44RsyaB9T2KIHavMF/ppJZNG9ZpyihvCd0w101no=
github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e/go.mod h1:RbqR21r5mrJuqunuUZ/Dhy/avygyECGrLceyNeo4LiM=
github.com/yosida95/uritemplate/v3 v3.0.2 h1:Ed3Oyj9yrmi9087+NczuL5BwkIc4wvTb5zIM+UJPGz4=
github.com/yosida95/uritemplate/v3 v3.0.2/go.mod h1:ILOh0sOhIJR3+L/8afwt/kE++YT040gmv5BQTMR2HP4=
golang.org/x/exp v0.0.0-20220909182711-5c715a9e8561 h1:MDc5xs78ZrZr3HMQugiXOAkSZtfTpbJLDr/lwfgO53E=
golang.org/x/exp v0.0.0-20220909182711-5c715a9e8561/go.mod h1:cyybsKvd6eL0RnXn6p/Grxp8F5bW7iYuBgsNCOHpMYE=
golang.org/x/oauth2 v0.30.0 h1:dnDm7JmhM45NNpd8FDDeLhK6FwqbOf4MLCM9zb1BOHI=
golang.org/x/oauth2 v0.30.0/go.mod h1:B++QgG3ZKulg6sRPGD/mqlHQs5rB3Ml9erfeDY7xKlU=
golang.org/x/sys v0.0.0-20210809222454-d867a43fc93e/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.38.0 h1:3yZWxaJjBmCWXqhN1qh02AkOnCQ1poK6oF+a7xWL6Gc=
golang.org/x/sys v0.38.0/go.mod h1:OgkHotnGiDImocRcuBABYBEXf8A9a87e/uXjp9XT3ks=
golang.org/x/term v0.37.0 h1:8EGAD0qCmHYZg6J17DvsMy9/wJ7/D/4pV/wfnld5lTU=
golang.org/x/term v0.37.0/go.mod h1:5pB4lxRNYYVZuTLmy8oR2BH8dflOR+IbTYFD8fi3254=
golang.org/x/text v0.3.8 h1:nAL+RVCQ9uMn3vJZbV+MRnydTJFPf8qqY42YiA6MrqY=
golang.org/x/text v0.3.8/go.mod h1:E6s5w1FMmriuDzIBO73fBruAKo1PCIq6d2Q6DHfQ8WQ=
golang.org/x/tools v0.34.0 h1:qIpSLOxeCYGg9TrcJokLBG4KFA6d795g0xkBkiESGlo=
golang.org/x/tools v0.34.0/go.mod h1:pAP9OwEaY1CAW3HOmg3hLZC5Z0CCmzjAF2UQMSqNARg=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405 h1:yhCVgyC4o1eVCa2tZl7eS0r+SDo693bJlVdllGtEeKM=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
</file>
<file path="LICENSE">
MIT License

Copyright (c) 2025 Jordan

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>
<file path="main.go">
package main

import (
	"context"
	"encoding/json"
	"flag"
	"fmt"
	"os"
	"path/filepath"
	"time"

	"codemap/analyze"
	"codemap/cache"
	"codemap/config"
	"codemap/graph"
	"codemap/render"
	"codemap/scanner"

	ignore "github.com/sabhiram/go-gitignore"
)

func main() {
	skylineMode := flag.Bool("skyline", false, "Enable skyline visualization mode")
	animateMode := flag.Bool("animate", false, "Enable animation (use with --skyline)")
	depsMode := flag.Bool("deps", false, "Enable dependency graph mode (function/import analysis)")
	diffMode := flag.Bool("diff", false, "Only show files changed vs main (or use --ref to specify branch)")
	diffRef := flag.String("ref", "main", "Branch/ref to compare against (use with --diff)")
	jsonMode := flag.Bool("json", false, "Output JSON (for Python renderer compatibility)")
	debugMode := flag.Bool("debug", false, "Show debug info (gitignore loading, paths, etc.)")
	helpMode := flag.Bool("help", false, "Show help")

	// New flags for enhanced analysis
	detailLevel := flag.Int("detail", 0, "Detail level: 0=names, 1=signatures, 2=full (use with --deps)")
	apiMode := flag.Bool("api", false, "Show public API surface only (compact view, use with --deps)")

	// Graph/RAG mode flags
	indexMode := flag.Bool("index", false, "Build knowledge graph index")
	queryMode := flag.Bool("query", false, "Query the knowledge graph")
	queryFrom := flag.String("from", "", "Query: symbol to trace from")
	queryTo := flag.String("to", "", "Query: symbol to trace to")
	queryDepth := flag.Int("depth", 5, "Query: max traversal depth")
	forceReindex := flag.Bool("force", false, "Force rebuild index even if up-to-date")
	graphOutput := flag.String("output", "", "Output path for graph file (default: .codemap/graph.gob)")

	// LLM analysis flags
	explainMode := flag.Bool("explain", false, "Explain a symbol using LLM")
	explainSymbol := flag.String("symbol", "", "Symbol name to explain (use with --explain)")
	summarizeMode := flag.Bool("summarize", false, "Summarize a module/directory using LLM")
	llmModel := flag.String("model", "", "LLM model to use (overrides config)")
	noCache := flag.Bool("no-cache", false, "Bypass cache for LLM requests")

	// Search mode flags
	searchMode := flag.Bool("search", false, "Search the codebase using natural language")
	searchQuery := flag.String("q", "", "Search query (use with --search)")
	searchLimit := flag.Int("limit", 10, "Number of search results to return")
	searchExpand := flag.Bool("expand", false, "Expand results with callers/callees context")
	embedMode := flag.Bool("embed", false, "Generate embeddings for the knowledge graph")

	flag.Parse()

	if *helpMode {
		fmt.Println("codemap - Generate a brain map of your codebase for LLM context")
		fmt.Println()
		fmt.Println("Usage: codemap [options] [path]")
		fmt.Println()
		fmt.Println("Modes:")
		fmt.Println("  (default)          Tree view with token estimates and file sizes")
		fmt.Println("  --deps             Dependency flow map (functions, types & imports)")
		fmt.Println("  --skyline          City skyline visualization")
		fmt.Println("  --diff             Only show files changed vs a branch")
		fmt.Println("  --index            Build knowledge graph index (.codemap/graph.gob)")
		fmt.Println("  --query            Query the knowledge graph")
		fmt.Println()
		fmt.Println("Options:")
		fmt.Println("  --help             Show this help message")
		fmt.Println("  --json             Output JSON (for programmatic use)")
		fmt.Println()
		fmt.Println("Dependency mode (--deps):")
		fmt.Println("  --detail <level>   Detail level: 0=names, 1=signatures, 2=full")
		fmt.Println("  --api              Show public API surface only (compact view)")
		fmt.Println()
		fmt.Println("Diff mode (--diff):")
		fmt.Println("  --ref <branch>     Branch to compare against (default: main)")
		fmt.Println()
		fmt.Println("Skyline mode (--skyline):")
		fmt.Println("  --animate          Enable terminal animation")
		fmt.Println()
		fmt.Println("Index mode (--index):")
		fmt.Println("  --force            Force rebuild even if index is up-to-date")
		fmt.Println("  --output <path>    Output path for graph file (default: .codemap/graph.gob)")
		fmt.Println()
		fmt.Println("Query mode (--query):")
		fmt.Println("  --from <symbol>    Find outgoing edges from symbol")
		fmt.Println("  --to <symbol>      Find incoming edges to symbol")
		fmt.Println("  --depth <n>        Max traversal depth (default: 5)")
		fmt.Println()
		fmt.Println("Explain mode (--explain):")
		fmt.Println("  --symbol <name>    Symbol name to explain")
		fmt.Println("  --model <name>     LLM model to use (overrides config)")
		fmt.Println("  --no-cache         Bypass cache for LLM requests")
		fmt.Println()
		fmt.Println("Summarize mode (--summarize):")
		fmt.Println("  --model <name>     LLM model to use (overrides config)")
		fmt.Println("  --no-cache         Bypass cache for LLM requests")
		fmt.Println()
		fmt.Println("Search mode (--search):")
		fmt.Println("  --q <query>        Natural language search query")
		fmt.Println("  --limit <n>        Number of results (default: 10)")
		fmt.Println("  --expand           Include callers/callees context")
		fmt.Println()
		fmt.Println("Embed mode (--embed):")
		fmt.Println("  --force            Force re-embedding of all symbols")
		fmt.Println()
		fmt.Println("Examples:")
		fmt.Println("  codemap .                              # Tree with tokens")
		fmt.Println("  codemap --deps .                       # Dependencies")
		fmt.Println("  codemap --index .                      # Build graph index")
		fmt.Println("  codemap --query --from main .          # Find what main calls")
		fmt.Println("  codemap --query --to Scanner .         # Find what calls Scanner")
		fmt.Println("  codemap --query --from A --to B .      # Find path from A to B")
		fmt.Println("  codemap --explain --symbol main .      # Explain main function")
		fmt.Println("  codemap --summarize src/              # Summarize directory")
		fmt.Println("  codemap --embed .                      # Generate embeddings")
		fmt.Println("  codemap --search --q \"parse config\" . # Semantic search")
		fmt.Println("  codemap --skyline --animate .          # Animated skyline")
		fmt.Println()
		fmt.Println("Output notes:")
		fmt.Println("  ⭐️  = Top 5 largest source files")
		fmt.Println("  [!] = Large file (>8k tokens) - may need chunking for LLMs")
		os.Exit(0)
	}

	root := flag.Arg(0)
	if root == "" {
		root = "."
	}

	absRoot, err := filepath.Abs(root)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error getting absolute path: %v\n", err)
		os.Exit(1)
	}

	// Load .gitignore if it exists
	gitignore := scanner.LoadGitignore(root)

	if *debugMode {
		fmt.Fprintf(os.Stderr, "[debug] Root path: %s\n", root)
		fmt.Fprintf(os.Stderr, "[debug] Absolute path: %s\n", absRoot)
		gitignorePath := filepath.Join(root, ".gitignore")
		if gitignore != nil {
			fmt.Fprintf(os.Stderr, "[debug] Loaded .gitignore from: %s\n", gitignorePath)
		} else {
			fmt.Fprintf(os.Stderr, "[debug] No .gitignore found at: %s\n", gitignorePath)
		}
	}

	// Get changed files if --diff is specified
	var diffInfo *scanner.DiffInfo
	if *diffMode {
		var err error
		diffInfo, err = scanner.GitDiffInfo(absRoot, *diffRef)
		if err != nil {
			fmt.Fprintf(os.Stderr, "Error getting git diff: %v\n", err)
			fmt.Fprintf(os.Stderr, "Make sure '%s' is a valid branch/ref\n", *diffRef)
			os.Exit(1)
		}
		if len(diffInfo.Changed) == 0 {
			fmt.Printf("No files changed vs %s\n", *diffRef)
			os.Exit(0)
		}
	}

	// Handle --index mode
	if *indexMode {
		runIndexMode(absRoot, root, gitignore, *forceReindex, *jsonMode, *graphOutput)
		return
	}

	// Handle --query mode
	if *queryMode {
		runQueryMode(absRoot, *queryFrom, *queryTo, *queryDepth, *jsonMode)
		return
	}

	// Handle --explain mode
	if *explainMode {
		runExplainMode(absRoot, *explainSymbol, *llmModel, *noCache, *jsonMode)
		return
	}

	// Handle --summarize mode
	if *summarizeMode {
		runSummarizeMode(root, *llmModel, *noCache, *jsonMode)
		return
	}

	// Handle --embed mode
	if *embedMode {
		runEmbedMode(absRoot, *llmModel, *forceReindex, *jsonMode)
		return
	}

	// Handle --search mode
	if *searchMode {
		runSearchMode(absRoot, *searchQuery, *searchLimit, *searchExpand, *llmModel, *jsonMode)
		return
	}

	// Handle --deps mode separately
	if *depsMode {
		var changedFiles map[string]bool
		if diffInfo != nil {
			changedFiles = diffInfo.Changed
		}
		runDepsMode(absRoot, root, gitignore, *jsonMode, *diffRef, changedFiles, *detailLevel, *apiMode)
		return
	}

	mode := "tree"
	if *skylineMode {
		mode = "skyline"
	}

	// Scan files
	files, err := scanner.ScanFiles(root, gitignore)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error walking tree: %v\n", err)
		os.Exit(1)
	}

	// Filter to changed files if --diff specified (with diff info annotations)
	var impact []scanner.ImpactInfo
	var activeDiffRef string
	if diffInfo != nil {
		files = scanner.FilterToChangedWithInfo(files, diffInfo)
		impact = scanner.AnalyzeImpact(absRoot, files)
		activeDiffRef = *diffRef
	}

	project := scanner.Project{
		Root:    absRoot,
		Mode:    mode,
		Animate: *animateMode,
		Files:   files,
		DiffRef: activeDiffRef,
		Impact:  impact,
	}

	// Render or output JSON
	if *jsonMode {
		json.NewEncoder(os.Stdout).Encode(project)
	} else if *skylineMode {
		render.Skyline(project, *animateMode)
	} else {
		render.Tree(project)
	}
}

func runDepsMode(absRoot, root string, gitignore *ignore.GitIgnore, jsonMode bool, diffRef string, changedFiles map[string]bool, detailLevel int, apiMode bool) {
	loader := scanner.NewGrammarLoader()

	// Check if grammars are available
	if !loader.HasGrammars() {
		fmt.Fprintln(os.Stderr, "")
		fmt.Fprintln(os.Stderr, "⚠️  No tree-sitter grammars found for --deps mode.")
		fmt.Fprintln(os.Stderr, "")
		fmt.Fprintln(os.Stderr, "To enable dependency analysis, either:")
		fmt.Fprintln(os.Stderr, "  • Install via Homebrew: brew install JordanCoin/tap/codemap")
		fmt.Fprintln(os.Stderr, "  • Download release with grammars: https://github.com/JordanCoin/codemap/releases")
		fmt.Fprintln(os.Stderr, "  • Build from source: make deps && go build")
		fmt.Fprintln(os.Stderr, "")
		fmt.Fprintln(os.Stderr, "Or set CODEMAP_GRAMMAR_DIR to your grammars directory.")
		fmt.Fprintln(os.Stderr, "")
		os.Exit(1)
	}

	analyses, err := scanner.ScanForDeps(root, gitignore, loader, scanner.DetailLevel(detailLevel))
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error scanning for deps: %v\n", err)
		os.Exit(1)
	}

	// Filter to changed files if --diff specified
	if changedFiles != nil {
		analyses = scanner.FilterAnalysisToChanged(analyses, changedFiles)
	}

	depsProject := scanner.DepsProject{
		Root:         absRoot,
		Mode:         "deps",
		Files:        analyses,
		ExternalDeps: scanner.ReadExternalDeps(absRoot),
		DiffRef:      diffRef,
		DetailLevel:  detailLevel,
	}

	// Render or output JSON
	if jsonMode {
		json.NewEncoder(os.Stdout).Encode(depsProject)
	} else if apiMode {
		render.APIView(depsProject)
	} else {
		render.Depgraph(depsProject)
	}
}

func runIndexMode(absRoot, root string, gitignore *ignore.GitIgnore, forceReindex, jsonMode bool, graphOutput string) {
	graphPath := graphOutput
	if graphPath == "" {
		graphPath = graph.GraphPath(absRoot)
	}
	loader := scanner.NewGrammarLoader()

	// Check if grammars are available
	if !loader.HasGrammars() {
		fmt.Fprintln(os.Stderr, "⚠️  No tree-sitter grammars found. Index requires --deps mode grammars.")
		os.Exit(1)
	}

	// Check if we can do an incremental update
	var existingGraph *graph.CodeGraph
	var modifiedFiles, deletedFiles []string
	isIncremental := false

	if !forceReindex && graph.Exists(graphPath) {
		existing, err := graph.LoadBinary(graphPath)
		if err == nil {
			stale, _ := graph.IsStale(existing, absRoot)
			if !stale {
				stats := existing.GetStats()
				if jsonMode {
					json.NewEncoder(os.Stdout).Encode(map[string]interface{}{
						"status":     "up-to-date",
						"path":       graphPath,
						"nodes":      stats.TotalNodes,
						"edges":      stats.TotalEdges,
						"indexed_at": time.Unix(existing.LastIndexed, 0).Format(time.RFC3339),
					})
				} else {
					fmt.Printf("✓ Index is up-to-date (%d nodes, %d edges)\n", stats.TotalNodes, stats.TotalEdges)
					fmt.Printf("  Path: %s\n", graphPath)
					fmt.Printf("  Last indexed: %s\n", time.Unix(existing.LastIndexed, 0).Format(time.RFC3339))
				}
				return
			}
			// Graph is stale - use incremental update
			existingGraph = existing
			modifiedFiles, _ = graph.GetModifiedFiles(existing, absRoot)
			deletedFiles = graph.GetDeletedFiles(existing, absRoot)
			isIncremental = true
		}
	}

	start := time.Now()

	// Scan all files to get current state
	analyses, err := scanner.ScanForDeps(root, gitignore, loader, scanner.DetailFull)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error scanning: %v\n", err)
		os.Exit(1)
	}

	// Build set of files that need processing
	filesToProcess := make(map[string]bool)
	if isIncremental {
		// Remove deleted files from graph
		for _, path := range deletedFiles {
			existingGraph.RemoveNodesForPath(path)
			if !jsonMode {
				fmt.Fprintf(os.Stderr, "  Removed %s\n", path)
			}
		}

		// Mark modified files for reprocessing
		for _, path := range modifiedFiles {
			existingGraph.RemoveNodesForPath(path)
			filesToProcess[path] = true
		}

		// Find new files (not in existing graph)
		for _, a := range analyses {
			if !existingGraph.IsFileInGraph(a.Path) {
				filesToProcess[a.Path] = true
			}
		}

		// If no files need updating, index is up-to-date
		if len(filesToProcess) == 0 {
			stats := existingGraph.GetStats()
			if jsonMode {
				json.NewEncoder(os.Stdout).Encode(map[string]interface{}{
					"status":     "up-to-date",
					"path":       graphPath,
					"nodes":      stats.TotalNodes,
					"edges":      stats.TotalEdges,
					"indexed_at": time.Unix(existingGraph.LastIndexed, 0).Format(time.RFC3339),
				})
			} else {
				fmt.Printf("✓ Index is up-to-date (%d nodes, %d edges)\n", stats.TotalNodes, stats.TotalEdges)
				fmt.Printf("  Path: %s\n", graphPath)
			}
			return
		}

		if !jsonMode {
			fmt.Fprintf(os.Stderr, "Incremental update: %d files\n", len(filesToProcess))
		}
	} else {
		fmt.Fprintf(os.Stderr, "Building knowledge graph index...\n")
		// Full rebuild - process all files
		for _, a := range analyses {
			filesToProcess[a.Path] = true
		}
	}

	// Create builder (with existing graph for incremental, new for full rebuild)
	var builder *graph.Builder
	if isIncremental {
		builder = graph.NewBuilder(absRoot, graph.WithProgress(func(msg string) {
			if !jsonMode {
				fmt.Fprintf(os.Stderr, "  %s\n", msg)
			}
		}), graph.WithGitignore(gitignore), graph.WithExistingGraph(existingGraph))
	} else {
		builder = graph.NewBuilder(absRoot, graph.WithProgress(func(msg string) {
			if !jsonMode {
				fmt.Fprintf(os.Stderr, "  %s\n", msg)
			}
		}), graph.WithGitignore(gitignore))
	}

	// Process files that need updating
	for _, a := range analyses {
		if !filesToProcess[a.Path] {
			continue
		}

		fa := &graph.FileAnalysis{
			Path:     a.Path,
			Language: a.Language,
			Imports:  a.Imports,
		}

		// Convert functions
		for _, f := range a.Functions {
			fa.Functions = append(fa.Functions, graph.FuncInfo{
				Name:       f.Name,
				Signature:  f.Signature,
				Receiver:   f.Receiver,
				IsExported: f.IsExported,
				Line:       f.Line,
				ParamCount: f.ParamCount,
			})
		}

		// Convert types
		for _, t := range a.Types {
			fa.Types = append(fa.Types, graph.TypeInfo{
				Name:       t.Name,
				Kind:       string(t.Kind),
				IsExported: t.IsExported,
				Line:       t.Line,
			})
		}

		// Extract calls
		callAnalysis, err := loader.ExtractCalls(filepath.Join(absRoot, a.Path))
		if err == nil && callAnalysis != nil {
			for _, c := range callAnalysis.Calls {
				fa.Calls = append(fa.Calls, graph.CallInfo{
					CallerFunc: c.CallerFunc,
					CallerLine: c.CallerLine,
					CalleeName: c.CalleeName,
					CallLine:   c.CallLine,
					Args:       c.Args,
					Receiver:   c.Receiver,
				})
			}
		}

		builder.AddFile(fa)
	}

	// Finalize graph
	builder.ResolveCallEdges()
	builder.FilterCallEdges()
	codeGraph := builder.Build()

	// Save to disk
	if err := codeGraph.SaveBinary(graphPath); err != nil {
		fmt.Fprintf(os.Stderr, "Error saving index: %v\n", err)
		os.Exit(1)
	}

	elapsed := time.Since(start)
	stats := codeGraph.GetStats()

	status := "created"
	statusMsg := "Index built"
	if isIncremental {
		status = "updated"
		statusMsg = "Index updated"
	}

	if jsonMode {
		json.NewEncoder(os.Stdout).Encode(map[string]interface{}{
			"status":        status,
			"incremental":   isIncremental,
			"files_updated": len(filesToProcess),
			"path":          graphPath,
			"nodes":         stats.TotalNodes,
			"edges":         stats.TotalEdges,
			"files":         stats.FileCount,
			"functions":     stats.FunctionCount,
			"elapsed_ms":    elapsed.Milliseconds(),
		})
	} else {
		fmt.Printf("\n✓ %s in %v\n", statusMsg, elapsed.Round(time.Millisecond))
		if isIncremental {
			fmt.Printf("  Updated: %d files\n", len(filesToProcess))
		}
		fmt.Printf("  Path: %s\n", graphPath)
		fmt.Printf("  Nodes: %d (files: %d, functions: %d)\n", stats.TotalNodes, stats.FileCount, stats.FunctionCount)
		fmt.Printf("  Edges: %d\n", stats.TotalEdges)
	}
}

func runQueryMode(absRoot, fromSymbol, toSymbol string, maxDepth int, jsonMode bool) {
	graphPath := graph.GraphPath(absRoot)

	if !graph.Exists(graphPath) {
		fmt.Fprintln(os.Stderr, "No index found. Run 'codemap --index' first.")
		os.Exit(1)
	}

	codeGraph, err := graph.LoadBinary(graphPath)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error loading index: %v\n", err)
		os.Exit(1)
	}

	// No symbols specified - show stats
	if fromSymbol == "" && toSymbol == "" {
		stats := codeGraph.GetStats()
		if jsonMode {
			json.NewEncoder(os.Stdout).Encode(stats)
		} else {
			fmt.Printf("Graph Statistics:\n")
			fmt.Printf("  Total nodes: %d\n", stats.TotalNodes)
			fmt.Printf("  Total edges: %d\n", stats.TotalEdges)
			fmt.Printf("  Files: %d\n", stats.FileCount)
			fmt.Printf("  Functions: %d\n", stats.FunctionCount)
			fmt.Printf("\nNode types:\n")
			for kind, count := range stats.NodesByKind {
				fmt.Printf("  %s: %d\n", kind, count)
			}
			fmt.Printf("\nEdge types:\n")
			for kind, count := range stats.EdgesByKind {
				fmt.Printf("  %s: %d\n", kind, count)
			}
		}
		return
	}

	// Path query: from A to B
	if fromSymbol != "" && toSymbol != "" {
		fromNodes := codeGraph.FindNodesByPattern(fromSymbol, nil)
		toNodes := codeGraph.FindNodesByPattern(toSymbol, nil)

		if len(fromNodes) == 0 {
			fmt.Fprintf(os.Stderr, "No nodes found matching '%s'\n", fromSymbol)
			os.Exit(1)
		}
		if len(toNodes) == 0 {
			fmt.Fprintf(os.Stderr, "No nodes found matching '%s'\n", toSymbol)
			os.Exit(1)
		}

		// Find path between first matches
		path := codeGraph.FindPath(fromNodes[0].ID, toNodes[0].ID, maxDepth)
		if path == nil {
			fmt.Printf("No path found from '%s' to '%s' (depth=%d)\n", fromSymbol, toSymbol, maxDepth)
			return
		}

		if jsonMode {
			json.NewEncoder(os.Stdout).Encode(path)
		} else {
			fmt.Printf("Path from %s to %s (length: %d):\n\n", fromSymbol, toSymbol, path.Length)
			for i, node := range path.Path {
				fmt.Printf("  %d. %s [%s] %s:%d\n", i+1, node.Name, node.Kind, node.Path, node.Line)
				if i < len(path.Edges) {
					fmt.Printf("     └─ %s ──>\n", path.Edges[i].Kind)
				}
			}
		}
		return
	}

	// From query: what does X call?
	if fromSymbol != "" {
		nodes := codeGraph.FindNodesByPattern(fromSymbol, nil)
		if len(nodes) == 0 {
			fmt.Fprintf(os.Stderr, "No nodes found matching '%s'\n", fromSymbol)
			os.Exit(1)
		}

		type edgeResult struct {
			From  *graph.Node   `json:"from"`
			Edges []*graph.Edge `json:"edges"`
			To    []*graph.Node `json:"targets"`
		}

		var results []edgeResult
		for _, node := range nodes {
			edges := codeGraph.GetOutgoingEdges(node.ID)
			var targets []*graph.Node
			for _, e := range edges {
				if target := codeGraph.GetNode(e.To); target != nil {
					targets = append(targets, target)
				}
			}
			if len(edges) > 0 {
				results = append(results, edgeResult{From: node, Edges: edges, To: targets})
			}
		}

		if jsonMode {
			json.NewEncoder(os.Stdout).Encode(results)
		} else {
			fmt.Printf("Outgoing edges from '%s':\n\n", fromSymbol)
			for _, r := range results {
				fmt.Printf("%s [%s] %s:%d\n", r.From.Name, r.From.Kind, r.From.Path, r.From.Line)
				for i, e := range r.Edges {
					target := r.To[i]
					if target != nil {
						fmt.Printf("  └─ %s ──> %s [%s] %s:%d\n", e.Kind, target.Name, target.Kind, target.Path, target.Line)
					}
				}
				fmt.Println()
			}
		}
		return
	}

	// To query: what calls X?
	if toSymbol != "" {
		nodes := codeGraph.FindNodesByPattern(toSymbol, nil)
		if len(nodes) == 0 {
			fmt.Fprintf(os.Stderr, "No nodes found matching '%s'\n", toSymbol)
			os.Exit(1)
		}

		type edgeResult struct {
			To      *graph.Node   `json:"to"`
			Edges   []*graph.Edge `json:"edges"`
			Callers []*graph.Node `json:"callers"`
		}

		var results []edgeResult
		for _, node := range nodes {
			edges := codeGraph.GetIncomingEdges(node.ID)
			var callers []*graph.Node
			for _, e := range edges {
				if caller := codeGraph.GetNode(e.From); caller != nil {
					callers = append(callers, caller)
				}
			}
			if len(edges) > 0 {
				results = append(results, edgeResult{To: node, Edges: edges, Callers: callers})
			}
		}

		if jsonMode {
			json.NewEncoder(os.Stdout).Encode(results)
		} else {
			fmt.Printf("Incoming edges to '%s':\n\n", toSymbol)
			for _, r := range results {
				fmt.Printf("%s [%s] %s:%d\n", r.To.Name, r.To.Kind, r.To.Path, r.To.Line)
				for i, e := range r.Edges {
					caller := r.Callers[i]
					if caller != nil {
						fmt.Printf("  <── %s ── %s [%s] %s:%d\n", e.Kind, caller.Name, caller.Kind, caller.Path, caller.Line)
					}
				}
				fmt.Println()
			}
		}
	}
}

// runExplainMode handles the --explain command for LLM-powered symbol explanation.
func runExplainMode(absRoot, symbol, modelOverride string, noCache, jsonMode bool) {
	if symbol == "" {
		fmt.Fprintln(os.Stderr, "Error: --symbol is required with --explain")
		fmt.Fprintln(os.Stderr, "Usage: codemap --explain --symbol <name> [path]")
		os.Exit(1)
	}

	// Load graph to find symbol
	graphPath := filepath.Join(absRoot, ".codemap", "graph.gob")
	if !graph.Exists(graphPath) {
		fmt.Fprintln(os.Stderr, "No index found. Run 'codemap --index' first.")
		os.Exit(1)
	}

	codeGraph, err := graph.LoadBinary(graphPath)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error loading index: %v\n", err)
		os.Exit(1)
	}

	// Find matching nodes
	nodes := codeGraph.FindNodesByPattern(symbol, nil)
	if len(nodes) == 0 {
		fmt.Fprintf(os.Stderr, "No symbols found matching '%s'\n", symbol)
		os.Exit(1)
	}

	// Use first match
	node := nodes[0]
	if len(nodes) > 1 && !jsonMode {
		fmt.Fprintf(os.Stderr, "Found %d matches for '%s', using: %s (%s:%d)\n",
			len(nodes), symbol, node.Name, node.Path, node.Line)
	}

	// Load configuration
	cfg, err := config.Load()
	if err != nil {
		// Use defaults if no config
		cfg = config.DefaultConfig()
	}

	// Override model if specified
	if modelOverride != "" {
		cfg.LLM.Model = modelOverride
	}

	// Create LLM client
	client, err := analyze.NewClient(cfg)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error creating LLM client: %v\n", err)
		fmt.Fprintln(os.Stderr, "Check your configuration (~/.config/codemap/config.yaml)")
		os.Exit(1)
	}

	// Check connectivity
	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	if err := client.Ping(ctx); err != nil {
		fmt.Fprintf(os.Stderr, "Error connecting to LLM provider (%s): %v\n", client.Name(), err)
		os.Exit(1)
	}

	// Read source code
	source, err := analyze.ReadSymbolSource(absRoot, node)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error reading source: %v\n", err)
		os.Exit(1)
	}

	// Initialize cache
	cacheOpts := cache.Options{
		Dir:     filepath.Join(absRoot, ".codemap", "cache"),
		Enabled: !noCache && cfg.Cache.Enabled,
	}
	responseCache, _ := cache.New(cacheOpts)

	// Check cache
	operation := "explain"
	if responseCache != nil && responseCache.Enabled() {
		if entry, ok := responseCache.GetByContentHash(source.ContentHash, operation, cfg.LLM.Model); ok {
			// Cache hit
			if jsonMode {
				json.NewEncoder(os.Stdout).Encode(map[string]interface{}{
					"symbol":      node.Name,
					"path":        node.Path,
					"line":        node.Line,
					"cached":      true,
					"model":       entry.Model,
					"explanation": entry.Response,
				})
			} else {
				fmt.Printf("## %s\n\n", node.Name)
				fmt.Printf("*%s:%d* (cached)\n\n", node.Path, node.Line)
				fmt.Println(entry.Response)
			}
			return
		}
	}

	// Generate prompt
	messages := analyze.ExplainSymbolPrompt(source)

	// Make request
	if !jsonMode {
		fmt.Fprintf(os.Stderr, "Explaining %s using %s...\n", node.Name, cfg.LLM.Model)
	}

	reqCtx, reqCancel := context.WithTimeout(context.Background(), time.Duration(cfg.LLM.Timeout)*time.Second)
	defer reqCancel()

	resp, err := client.Complete(reqCtx, &analyze.CompletionRequest{
		Messages:    messages,
		Temperature: cfg.LLM.Temperature,
		MaxTokens:   cfg.LLM.MaxTokens,
	})
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error from LLM: %v\n", err)
		os.Exit(1)
	}

	// Cache response
	if responseCache != nil && responseCache.Enabled() {
		responseCache.SetResponse(source.ContentHash, operation, resp.Model, resp.Content, &cache.TokenUsage{
			PromptTokens:     resp.Usage.PromptTokens,
			CompletionTokens: resp.Usage.CompletionTokens,
			TotalTokens:      resp.Usage.TotalTokens,
		})
	}

	// Output result
	if jsonMode {
		json.NewEncoder(os.Stdout).Encode(map[string]interface{}{
			"symbol":      node.Name,
			"path":        node.Path,
			"line":        node.Line,
			"kind":        node.Kind.String(),
			"cached":      false,
			"model":       resp.Model,
			"explanation": resp.Content,
			"usage": map[string]int{
				"prompt_tokens":     resp.Usage.PromptTokens,
				"completion_tokens": resp.Usage.CompletionTokens,
				"total_tokens":      resp.Usage.TotalTokens,
			},
			"duration_ms": resp.Duration.Milliseconds(),
		})
	} else {
		fmt.Printf("\n## %s\n\n", node.Name)
		fmt.Printf("*%s:%d* | %s | %d tokens | %v\n\n", node.Path, node.Line, resp.Model,
			resp.Usage.TotalTokens, resp.Duration.Round(time.Millisecond))
		fmt.Println(resp.Content)
	}
}

// runSummarizeMode handles the --summarize command for LLM-powered module summarization.
func runSummarizeMode(targetPath, modelOverride string, noCache, jsonMode bool) {
	// Resolve target path to absolute
	absTarget, err := filepath.Abs(targetPath)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error resolving path: %v\n", err)
		os.Exit(1)
	}

	// Ensure path exists
	info, err := os.Stat(absTarget)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error: path not found: %s\n", targetPath)
		os.Exit(1)
	}

	// Find project root (directory containing .codemap/)
	projectRoot := findProjectRoot(absTarget)
	if projectRoot == "" {
		// Fall back to current working directory
		projectRoot, _ = os.Getwd()
	}

	// Calculate relative path from project root to target
	relPath, err := filepath.Rel(projectRoot, absTarget)
	if err != nil || (len(relPath) > 1 && relPath[:2] == "..") {
		// Target is outside project root - use target's parent as project root
		if info.IsDir() {
			projectRoot = absTarget
			relPath = "."
		} else {
			projectRoot = filepath.Dir(absTarget)
			relPath = filepath.Base(absTarget)
		}
	}

	// For display purposes, use directory/file name if relPath is "."
	displayPath := relPath
	if displayPath == "." {
		displayPath = filepath.Base(absTarget)
	}

	// Load configuration
	cfg, err := config.Load()
	if err != nil {
		cfg = config.DefaultConfig()
	}

	if modelOverride != "" {
		cfg.LLM.Model = modelOverride
	}

	// Create LLM client
	client, err := analyze.NewClient(cfg)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error creating LLM client: %v\n", err)
		fmt.Fprintln(os.Stderr, "Check your configuration (~/.config/codemap/config.yaml)")
		os.Exit(1)
	}

	// Check connectivity
	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	if err := client.Ping(ctx); err != nil {
		fmt.Fprintf(os.Stderr, "Error connecting to LLM provider (%s): %v\n", client.Name(), err)
		os.Exit(1)
	}

	// Read source files
	var sources []*analyze.SymbolSource

	if info.IsDir() {
		sources, err = analyze.ReadModuleSource(projectRoot, relPath)
		if err != nil {
			fmt.Fprintf(os.Stderr, "Error reading module: %v\n", err)
			os.Exit(1)
		}
	} else {
		// Single file
		source, err := analyze.ReadSymbolSource(projectRoot, &graph.Node{
			Kind: graph.KindFile,
			Name: filepath.Base(targetPath),
			Path: relPath,
		})
		if err != nil {
			fmt.Fprintf(os.Stderr, "Error reading file: %v\n", err)
			os.Exit(1)
		}
		sources = []*analyze.SymbolSource{source}
	}

	if len(sources) == 0 {
		fmt.Fprintln(os.Stderr, "No source files found in the specified path")
		os.Exit(1)
	}

	// Compute combined content hash for caching
	var combinedHash string
	{
		var hashes []string
		for _, s := range sources {
			hashes = append(hashes, s.ContentHash)
		}
		combinedHash = analyze.ContentHash(fmt.Sprintf("%v", hashes))
	}

	// Initialize cache
	cacheOpts := cache.Options{
		Dir:     filepath.Join(projectRoot, ".codemap", "cache"),
		Enabled: !noCache && cfg.Cache.Enabled,
	}
	responseCache, _ := cache.New(cacheOpts)

	// Check cache
	operation := "summarize"
	if responseCache != nil && responseCache.Enabled() {
		if entry, ok := responseCache.GetByContentHash(combinedHash, operation, cfg.LLM.Model); ok {
			// Cache hit
			if jsonMode {
				json.NewEncoder(os.Stdout).Encode(map[string]interface{}{
					"path":    displayPath,
					"files":   len(sources),
					"cached":  true,
					"model":   entry.Model,
					"summary": entry.Response,
				})
			} else {
				fmt.Printf("## %s\n\n", displayPath)
				fmt.Printf("*%d files* (cached)\n\n", len(sources))
				fmt.Println(entry.Response)
			}
			return
		}
	}

	// Generate prompt
	messages := analyze.SummarizeModulePrompt(displayPath, sources)

	// Make request
	if !jsonMode {
		fmt.Fprintf(os.Stderr, "Summarizing %s (%d files) using %s...\n", displayPath, len(sources), cfg.LLM.Model)
	}

	reqCtx, reqCancel := context.WithTimeout(context.Background(), time.Duration(cfg.LLM.Timeout)*time.Second)
	defer reqCancel()

	resp, err := client.Complete(reqCtx, &analyze.CompletionRequest{
		Messages:    messages,
		Temperature: cfg.LLM.Temperature,
		MaxTokens:   cfg.LLM.MaxTokens,
	})
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error from LLM: %v\n", err)
		os.Exit(1)
	}

	// Cache response
	if responseCache != nil && responseCache.Enabled() {
		responseCache.SetResponse(combinedHash, operation, resp.Model, resp.Content, &cache.TokenUsage{
			PromptTokens:     resp.Usage.PromptTokens,
			CompletionTokens: resp.Usage.CompletionTokens,
			TotalTokens:      resp.Usage.TotalTokens,
		})
	}

	// Output result
	if jsonMode {
		// Collect file names
		var fileNames []string
		for _, s := range sources {
			fileNames = append(fileNames, s.Node.Name)
		}

		json.NewEncoder(os.Stdout).Encode(map[string]interface{}{
			"path":    displayPath,
			"files":   fileNames,
			"cached":  false,
			"model":   resp.Model,
			"summary": resp.Content,
			"usage": map[string]int{
				"prompt_tokens":     resp.Usage.PromptTokens,
				"completion_tokens": resp.Usage.CompletionTokens,
				"total_tokens":      resp.Usage.TotalTokens,
			},
			"duration_ms": resp.Duration.Milliseconds(),
		})
	} else {
		fmt.Printf("\n## %s\n\n", displayPath)
		fmt.Printf("*%d files* | %s | %d tokens | %v\n\n", len(sources), resp.Model,
			resp.Usage.TotalTokens, resp.Duration.Round(time.Millisecond))
		fmt.Println(resp.Content)
	}
}

// runEmbedMode handles the --embed command for generating embeddings.
func runEmbedMode(absRoot, modelOverride string, forceRebuild, jsonMode bool) {
	// Load graph
	graphPath := graph.GraphPath(absRoot)
	if !graph.Exists(graphPath) {
		fmt.Fprintln(os.Stderr, "No index found. Run 'codemap --index' first.")
		os.Exit(1)
	}

	codeGraph, err := graph.LoadBinary(graphPath)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error loading index: %v\n", err)
		os.Exit(1)
	}

	// Load or create vector index
	vectorPath := graph.VectorIndexPath(absRoot)
	var vectorIndex *graph.InMemoryVectorIndex

	if !forceRebuild && graph.VectorIndexExists(absRoot) {
		vectorIndex, err = graph.LoadVectorIndex(vectorPath)
		if err != nil {
			if !jsonMode {
				fmt.Fprintf(os.Stderr, "Warning: could not load existing vectors, rebuilding: %v\n", err)
			}
			vectorIndex = graph.NewVectorIndex(0)
		} else if !jsonMode {
			fmt.Fprintf(os.Stderr, "Loaded existing vectors: %d\n", vectorIndex.Count())
		}
	} else {
		vectorIndex = graph.NewVectorIndex(0)
	}

	// Load configuration
	cfg, err := config.Load()
	if err != nil {
		cfg = config.DefaultConfig()
	}

	if modelOverride != "" {
		cfg.LLM.EmbeddingModel = modelOverride
	}

	// Create LLM client
	client, err := analyze.NewClient(cfg)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error creating LLM client: %v\n", err)
		fmt.Fprintln(os.Stderr, "Check your configuration (~/.config/codemap/config.yaml)")
		os.Exit(1)
	}

	// Ping to verify connectivity
	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	if err := client.Ping(ctx); err != nil {
		fmt.Fprintf(os.Stderr, "Error connecting to LLM provider (%s): %v\n", client.Name(), err)
		os.Exit(1)
	}

	// Configure embedding
	embedConfig := analyze.DefaultEmbeddingConfig()
	embedConfig.SkipExisting = !forceRebuild
	embedConfig.ProgressFn = func(completed, total int) {
		if !jsonMode {
			fmt.Fprintf(os.Stderr, "\rEmbedding: %d/%d", completed, total)
		}
	}

	// Run embedding
	if !jsonMode {
		fmt.Fprintf(os.Stderr, "Generating embeddings using %s...\n", cfg.LLM.EmbeddingModel)
	}

	embedCtx, embedCancel := context.WithTimeout(context.Background(), 30*time.Minute)
	defer embedCancel()

	stats, err := analyze.EmbedGraph(embedCtx, client, codeGraph, vectorIndex, embedConfig)
	if err != nil {
		fmt.Fprintf(os.Stderr, "\nError during embedding: %v\n", err)
	}

	// Save vectors
	if err := vectorIndex.Save(vectorPath); err != nil {
		fmt.Fprintf(os.Stderr, "Error saving vectors: %v\n", err)
		os.Exit(1)
	}

	if !jsonMode {
		fmt.Fprintf(os.Stderr, "\n")
	}

	if jsonMode {
		json.NewEncoder(os.Stdout).Encode(map[string]interface{}{
			"status":    "completed",
			"path":      vectorPath,
			"total":     stats.Total,
			"embedded":  stats.Embedded,
			"skipped":   stats.Skipped,
			"failed":    stats.Failed,
			"dimension": vectorIndex.Dimension(),
			"vectors":   vectorIndex.Count(),
			"duration":  stats.Duration.String(),
			"tokens_in": stats.TokensIn,
		})
	} else {
		fmt.Printf("\n✓ Embeddings generated in %v\n", stats.Duration.Round(time.Millisecond))
		fmt.Printf("  Path: %s\n", vectorPath)
		fmt.Printf("  Vectors: %d (dimension: %d)\n", vectorIndex.Count(), vectorIndex.Dimension())
		fmt.Printf("  Embedded: %d, Skipped: %d, Failed: %d\n", stats.Embedded, stats.Skipped, stats.Failed)
		if stats.TokensIn > 0 {
			fmt.Printf("  Tokens used: %d\n", stats.TokensIn)
		}
	}
}

// runSearchMode handles the --search command for semantic/hybrid search.
func runSearchMode(absRoot, query string, limit int, expandContext bool, modelOverride string, jsonMode bool) {
	if query == "" {
		fmt.Fprintln(os.Stderr, "Error: --q is required with --search")
		fmt.Fprintln(os.Stderr, "Usage: codemap --search --q \"your query\" [path]")
		os.Exit(1)
	}

	// Load graph
	graphPath := graph.GraphPath(absRoot)
	if !graph.Exists(graphPath) {
		fmt.Fprintln(os.Stderr, "No index found. Run 'codemap --index' first.")
		os.Exit(1)
	}

	codeGraph, err := graph.LoadBinary(graphPath)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error loading index: %v\n", err)
		os.Exit(1)
	}

	// Load vector index if available
	var vectorIndex *graph.InMemoryVectorIndex
	vectorPath := graph.VectorIndexPath(absRoot)
	if graph.VectorIndexExists(absRoot) {
		vectorIndex, err = graph.LoadVectorIndex(vectorPath)
		if err != nil && !jsonMode {
			fmt.Fprintf(os.Stderr, "Warning: could not load vectors, falling back to graph search: %v\n", err)
		}
	}

	// Load configuration
	cfg, err := config.Load()
	if err != nil {
		cfg = config.DefaultConfig()
	}

	if modelOverride != "" {
		cfg.LLM.EmbeddingModel = modelOverride
	}

	// Create LLM client (needed for query embedding)
	var client analyze.LLMClient
	if vectorIndex != nil && vectorIndex.Count() > 0 {
		client, err = analyze.NewClient(cfg)
		if err != nil && !jsonMode {
			fmt.Fprintf(os.Stderr, "Warning: LLM client unavailable, falling back to graph search: %v\n", err)
		}
	}

	// Create retriever
	retriever := analyze.NewRetriever(codeGraph, vectorIndex, client, absRoot)

	// Configure search
	searchConfig := analyze.DefaultSearchConfig()
	searchConfig.Limit = limit
	searchConfig.ExpandContext = expandContext

	// Determine search mode based on available resources
	if client == nil || vectorIndex == nil || vectorIndex.Count() == 0 {
		searchConfig.Mode = analyze.SearchModeGraph
		if !jsonMode {
			fmt.Fprintf(os.Stderr, "Using graph search (run 'codemap --embed' for semantic search)\n")
		}
	}

	// Perform search
	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	results, err := retriever.Search(ctx, query, searchConfig)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error during search: %v\n", err)
		os.Exit(1)
	}

	if len(results) == 0 {
		if jsonMode {
			json.NewEncoder(os.Stdout).Encode(map[string]interface{}{
				"query":   query,
				"results": []interface{}{},
				"count":   0,
			})
		} else {
			fmt.Printf("No results found for: %s\n", query)
		}
		return
	}

	// Output results
	if jsonMode {
		type jsonResult struct {
			Name        string   `json:"name"`
			Kind        string   `json:"kind"`
			Path        string   `json:"path"`
			Line        int      `json:"line"`
			Score       float64  `json:"score"`
			VectorScore float64  `json:"vector_score,omitempty"`
			GraphScore  float64  `json:"graph_score,omitempty"`
			MatchReason string   `json:"match_reason"`
			Snippet     string   `json:"snippet,omitempty"`
			Callers     []string `json:"callers,omitempty"`
			Callees     []string `json:"callees,omitempty"`
		}

		var jsonResults []jsonResult
		for _, r := range results {
			jr := jsonResult{
				Name:        r.Node.Name,
				Kind:        r.Node.Kind.String(),
				Path:        r.Node.Path,
				Line:        r.Node.Line,
				Score:       r.FinalScore,
				VectorScore: r.VectorScore,
				GraphScore:  r.GraphScore,
				MatchReason: r.MatchReason,
				Snippet:     r.Snippet,
			}
			for _, c := range r.Callers {
				jr.Callers = append(jr.Callers, c.Name)
			}
			for _, c := range r.Callees {
				jr.Callees = append(jr.Callees, c.Name)
			}
			jsonResults = append(jsonResults, jr)
		}

		json.NewEncoder(os.Stdout).Encode(map[string]interface{}{
			"query":   query,
			"results": jsonResults,
			"count":   len(jsonResults),
		})
	} else {
		fmt.Printf("Search: \"%s\"\n", query)
		fmt.Printf("Found %d results:\n\n", len(results))

		for i, r := range results {
			fmt.Printf("%d. %s [%s] %.3f\n", i+1, r.Node.Name, r.Node.Kind, r.FinalScore)
			fmt.Printf("   %s:%d\n", r.Node.Path, r.Node.Line)
			if r.MatchReason != "" {
				fmt.Printf("   Matched: %s\n", r.MatchReason)
			}
			if len(r.Callers) > 0 {
				fmt.Printf("   Callers: ")
				for j, c := range r.Callers {
					if j > 0 {
						fmt.Print(", ")
					}
					fmt.Print(c.Name)
				}
				fmt.Println()
			}
			if len(r.Callees) > 0 {
				fmt.Printf("   Callees: ")
				for j, c := range r.Callees {
					if j > 0 {
						fmt.Print(", ")
					}
					fmt.Print(c.Name)
				}
				fmt.Println()
			}
			if r.Snippet != "" {
				fmt.Printf("   ```\n%s\n   ```\n", r.Snippet)
			}
			fmt.Println()
		}
	}
}

// findProjectRoot searches upward from startPath for a directory containing .codemap/
// Returns the project root path or empty string if not found.
func findProjectRoot(startPath string) string {
	current := startPath

	// If startPath is a file, start from its directory
	if info, err := os.Stat(current); err == nil && !info.IsDir() {
		current = filepath.Dir(current)
	}

	for {
		codemapDir := filepath.Join(current, ".codemap")
		if info, err := os.Stat(codemapDir); err == nil && info.IsDir() {
			return current
		}

		parent := filepath.Dir(current)
		if parent == current {
			// Reached filesystem root
			return ""
		}
		current = parent
	}
}
</file>
<file path="Makefile">
.PHONY: all build build-mcp run deps grammars clean install install-mcp uninstall

all: build

build:
	go build -o codemap .

build-mcp:
	go build -o codemap-mcp ./mcp/

DIR ?= .
ABS_DIR := $(shell cd "$(DIR)" && pwd)
SKYLINE_FLAG := $(if $(SKYLINE),--skyline,)
ANIMATE_FLAG := $(if $(ANIMATE),--animate,)
DEPS_FLAG := $(if $(DEPS),--deps,)

run: build
	./codemap $(SKYLINE_FLAG) $(ANIMATE_FLAG) $(DEPS_FLAG) "$(ABS_DIR)"

# Build tree-sitter grammar libraries (one-time setup for deps mode)
grammars:
	cd scanner && ./build-grammars.sh

# Dependency graph mode - shows functions and imports per file
deps: build grammars
	./codemap --deps "$(ABS_DIR)"

clean:
	rm -f codemap codemap-mcp
	rm -rf scanner/.grammar-build
	rm -rf scanner/grammars

# Installation paths
PREFIX ?= $(HOME)/.local
BINDIR ?= $(PREFIX)/bin
GRAMMAR_DIR ?= $(PREFIX)/lib/codemap/grammars

install: build
	@echo "Installing codemap to $(BINDIR)..."
	install -d $(BINDIR)
	install -m 755 codemap $(BINDIR)/codemap
	@if [ -d scanner/grammars ] && [ "$$(ls -A scanner/grammars 2>/dev/null)" ]; then \
		echo "Installing grammars to $(GRAMMAR_DIR)..."; \
		install -d $(GRAMMAR_DIR); \
		cp -r scanner/grammars/* $(GRAMMAR_DIR)/; \
	fi
	@echo "Done! Run 'codemap --help' to get started."

install-mcp: build-mcp
	@echo "Installing codemap-mcp to $(BINDIR)..."
	install -d $(BINDIR)
	install -m 755 codemap-mcp $(BINDIR)/codemap-mcp
	@echo "Done!"

uninstall:
	@echo "Removing codemap from $(BINDIR)..."
	rm -f $(BINDIR)/codemap
	rm -f $(BINDIR)/codemap-mcp
	rm -rf $(PREFIX)/lib/codemap
	@echo "Done!"
</file>
<file path="README.md">
# Codebase Analysis Tool (codemap)

## Project Overview

The `codemap` project is a sophisticated code intelligence tool built in Go. It is designed to provide deep, structural, and semantic analysis of a codebase by combining high-performance static analysis with the reasoning capabilities of Large Language Models (LLMs).

**Purpose and Main Functionality:**
The core purpose of `codemap` is to transform raw source code into a queryable **Knowledge Graph**. This graph, augmented with vector embeddings, serves as the foundation for a Retrieval-Augmented Generation (RAG) pipeline, enabling AI agents and developers to ask complex questions about the codebase structure, dependencies, and functionality.

**Key Features and Capabilities:**

*   **Multi-Language Static Analysis:** Uses Tree-sitter to parse various languages and extract symbols, types, and dependencies.
*   **Knowledge Graph Persistence:** Builds and stores a persistent graph of the codebase (Nodes, Edges, Vectors) for fast, repeated querying.
*   **LLM-Powered Insights:** Provides natural language explanations for code symbols and summaries for modules using external LLMs (OpenAI, Gemini, Anthropic, Ollama).
*   **Semantic Search:** Supports hybrid search capabilities using vector embeddings and graph traversal.
*   **Code Flow Tracing:** Can trace the shortest path of function calls between two symbols in the graph.
*   **Dual Interface:** Operates as a Command Line Interface (CLI) tool for developers and as a **Model Context Protocol (MCP) Server** for AI agents.
*   **Rich Visualization:** Offers terminal visualizations, including file trees, dependency graphs, and a code "skyline" view.

**Likely Intended Use Cases:**

*   **AI Agent Tooling:** Serving as a core tool for LLM orchestrators to gain deep, structured context about a repository.
*   **Developer Onboarding:** Quickly generating summaries and explanations for unfamiliar code modules.
*   **Automated Documentation:** Generating up-to-date structural and dependency documentation.
*   **Code Review Context:** Providing impact analysis for code changes (`--diff` mode).

## Table of Contents

1.  [Project Overview](#project-overview)
2.  [Architecture](#architecture)
3.  [C4 Model Architecture](#c4-model-architecture)
4.  [Repository Structure](#repository-structure)
5.  [Dependencies and Integration](#dependencies-and-integration)
6.  [API Documentation](#api-documentation)
7.  [Development Notes](#development-notes)
8.  [Known Issues and Limitations](#known-issues-and-limitations)
9.  [Additional Documentation](#additional-documentation)

## Architecture

The `codemap` application follows a clear, layered architecture centered around the Retrieval-Augmented Generation (RAG) pattern.

**High-Level Architecture Overview**
The system is structured as a pipeline: Data Acquisition -> Knowledge Layer -> Intelligence Layer -> Presentation Layer. The core state is managed by the persistent Knowledge Graph.

**Technology Stack and Frameworks**

| Category | Technology | Purpose |
| :--- | :--- | :--- |
| **Core Language** | Go | Primary development language. |
| **Static Analysis** | Tree-sitter | High-performance, multi-language parsing and AST generation. |
| **Persistence** | `encoding/gob` | Efficient serialization of the Knowledge Graph to disk. |
| **CLI/TUI** | `charmbracelet/bubbletea` | Framework for building rich, interactive terminal user interfaces. |
| **AI Integration** | Model Context Protocol (MCP) | Standardized protocol for communication with LLM orchestrators. |

**Key Design Patterns**

*   **Factory Pattern:** Used in the `analyze` package to instantiate the correct LLM client (OpenAI, Gemini, etc.) based on configuration, abstracting provider-specific details.
*   **Strategy Pattern:** Different LLM providers are interchangeable strategies conforming to a common `LLMClient` interface.
*   **Repository Pattern:** The `graph` package acts as a repository, abstracting the persistence and querying logic for the code knowledge graph.

**Component Relationships**

The diagram below illustrates the primary data and control flow between the core internal packages.

```mermaid
graph TD
    subgraph 0. Orchestration
        A[main: CLI Entry Point & Router]
    end

    subgraph 1. Presentation Layer (Output)
        R[render: Visualization & Formatting]
    end

    subgraph 2. Intelligence Layer (LLM Orchestration)
        F[analyze: LLM Clients, RAG, Embeddings]
    end

    subgraph 3. Knowledge Layer (Data Core)
        E[graph: Knowledge Graph Store & Query]
        D[cache: Persistence & Caching]
    end

    subgraph 4. Data Acquisition Layer (Input)
        C[scanner: Tree-sitter Parsing & Git]
        B[config: Configuration]
    end

    A --> B
    A --> C
    A --> D
    A --> E
    A --> F
    A --> R

    C -- Reads Settings --> B
    E -- Consumes DTOs --> C
    E -- Persists Data --> D
    F -- Loads Settings --> B
    F -- Uses Cache --> D
    F -- Queries Context --> E
    R -- Consumes Data --> C
    R -- Consumes Data --> E
```

## C4 Model Architecture

### Context Diagram (Level 1)

<details>
<summary>C4 Context Diagram: System and External Relationships</summary>

```mermaid
%% C4 Context Diagram
C4Context
    title Context Diagram for Codebase Analysis Tool

    Person(developer, "Developer/User", "Interacts via CLI for analysis and visualization.")
    System(codemap, "Codebase Analysis Tool (codemap)", "Analyzes source code, builds a knowledge graph, and provides RAG-powered insights.")
    System(llm_orchestrator, "LLM Orchestrator/Agent", "Consumes the MCP Server API for code context and analysis tools.")
    System_Ext(external_llms, "External LLM APIs", "OpenAI, Anthropic, Gemini, etc. Used for generation and embedding.")
    System_Ext(source_code, "Source Code Repository", "The codebase being analyzed (Go, Python, etc.).")

    developer --> codemap "Uses CLI to run analysis modes"
    llm_orchestrator --> codemap "Calls MCP Server API (stdio/IPC)"
    codemap --> external_llms "Calls for LLM generation and embeddings (HTTP/S)"
    codemap --> source_code "Reads and parses files (File I/O)"
```
</details>

### Container Diagram (Level 2)

<details>
<summary>C4 Container Diagram: High-Level Technical Building Blocks</summary>

```mermaid
%% C4 Container Diagram
C4Container
    title Container Diagram for Codebase Analysis Tool

    System_Boundary(codemap_system, "Codebase Analysis Tool")
        Container(cli, "CLI Application (codemap)", "Go Executable", "Handles command-line arguments, orchestrates analysis, and provides TUI output.")
        Container(mcp_server, "MCP Server", "Go Executable (stdio)", "Exposes analysis tools via the Model Context Protocol (MCP) for AI agents.")
        Container(knowledge_graph, "Knowledge Graph Store", "File System (.codemap/graph.gob)", "Persistent storage for the code graph (Nodes, Edges, Vectors).")
        Container(analysis_cache, "Analysis Cache", "File System (.codemap/cache)", "Stores cached LLM responses and analysis results.")
    System_Boundary

    System_Ext(external_llms, "External LLM APIs", "OpenAI, Anthropic, Gemini, Ollama")
    System_Ext(source_code, "Source Code Repository", "Files on disk")

    cli --> knowledge_graph "Reads/Writes Graph Data (gob)"
    cli --> analysis_cache "Reads/Writes Cached Results (JSON/Bytes)"
    cli --> source_code "Scans and Parses Code"
    cli --> external_llms "Calls LLM APIs (via analyze package)"

    mcp_server --> knowledge_graph "Reads/Writes Graph Data (gob)"
    mcp_server --> analysis_cache "Reads/Writes Cached Results (JSON/Bytes)"
    mcp_server --> source_code "Scans and Parses Code"
    mcp_server --> external_llms "Calls LLM APIs (via analyze package)"

    knowledge_graph --> source_code "Built from parsed code"
```
</details>

## Repository Structure

The repository is organized by functional package, reflecting the layered architecture.

| Directory/File | Purpose |
| :--- | :--- |
| `/main.go` | Application entry point, command-line argument parsing, and mode routing. |
| `/scanner` | Code parsing, file system traversal, Git integration, and symbol extraction using Tree-sitter. |
| `/graph` | Knowledge Graph construction, persistence, querying, and vector indexing (the RAG backend). |
| `/analyze` | LLM orchestration, client management, embedding generation, and RAG context retrieval logic. |
| `/render` | Output formatting and visualization for the terminal (trees, graphs, skyline). |
| `/config` | Centralized management of application settings and LLM configuration. |
| `/cache` | Persistent key-value store for caching expensive analysis results (LLM responses, embeddings). |
| `/.codemap` | Default directory created by the tool to store persistent state (graph, vectors, cache). |

## Dependencies and Integration

### Internal and External Service Dependencies

The application relies on several external services, primarily for its AI-powered features.

| Service/Integration Point | Internal Package | Protocol/Client | Purpose |
| :--- | :--- | :--- | :--- |
| **OpenAI API** | `analyze/openai.go` | HTTP/Go SDK | General-purpose LLM for `explain`, `summarize`, and `search`. |
| **Google Gemini API** | `analyze/gemini.go` | HTTP/Go SDK | Alternative LLM provider integration. |
| **Anthropic API** | `analyze/anthropic.go` | HTTP/Go SDK | Alternative LLM provider integration. |
| **Ollama** | `analyze/ollama.go` | HTTP/Go SDK | Integration for local/self-hosted LLMs. |
| **Model Context Protocol (MCP)** | `analyze` | `modelcontextprotocol/go-sdk` | Standardizes the RAG pipeline and context formatting for LLM calls. |
| **Git** | `scanner/git.go` | OS Command Execution (Inferred) | Used to determine changed files (`--diff` mode) and repository root. |

### Integration Patterns

*   **LLM Client Abstraction:** The `analyze` package uses a common `Client` interface and a **Factory Pattern** to abstract communication with all external LLM providers, ensuring the core RAG logic remains provider-agnostic.
*   **Resilience:** The system implements a **caching layer** (`/cache`) to prevent redundant external API calls. For transient errors (e.g., network issues, rate limits), the `analyze` package implements **retry logic** with a maximum of **3 attempts** and an exponential backoff strategy.

## API Documentation

The project exposes its analysis capabilities via the **Model Context Protocol (MCP) Server**, designed for inter-process communication with LLM orchestrators. The API version is **2.3.0**.

All requests are made as `CALL` methods with JSON payloads.

| Tool Name | Description | Key Request Parameters |
| :--- | :--- | :--- |
| `get_structure` | Provides a hierarchical file tree view of the codebase, including file sizes, language, and token estimates. | `path` (string, required) |
| `get_dependencies` | Generates a dependency graph report showing external dependencies and internal import chains. | `path` (string, required), `detail` (int, optional), `mode` (string, optional) |
| `trace_path` | Finds the shortest path of function calls connecting a source symbol to a target symbol. Requires a pre-built knowledge graph index. | `path`, `from`, `to` (strings, required), `depth` (int, optional) |
| `explain_symbol` | Uses an LLM to generate a natural language explanation for a specific code symbol (function, type, method). | `path`, `symbol` (strings, required), `model`, `no_cache` (optional) |
| `summarize_module` | Uses an LLM to generate a summary of a module or directory based on its contents. | `path` (string, required), `model`, `no_cache` (optional) |
| `semantic_search` | Performs a hybrid semantic and graph-based search across the codebase. | `path` (string, required), `query` (string, required - inferred) |
| `get_callers` | Finds all functions that call a specific symbol. | `path`, `symbol` (strings, required) |
| `status` | Checks the server's operational status and version. | None |

**Authentication & Security:**
The MCP server does not implement user authentication. It is designed to run in a secure, sandboxed environment where access control is managed by the orchestrating agent. Authentication for external LLM services is handled internally via API keys loaded from environment variables or configuration files.

## Development Notes

### Project-Specific Conventions

*   **Dependency Passing:** The project favors explicit dependency passing (passing structs and interfaces as function arguments) over global state or heavy dependency injection frameworks, which is idiomatic Go.
*   **Data Persistence:** Internal data structures (like the Knowledge Graph) are serialized using `encoding/gob` for performance, while external data (like cache entries) often use JSON.
*   **Configuration:** Configuration is centralized in the `config` package and loaded once at startup, providing a consistent state for all components.

### Testing Requirements

*   **Unit Testing:** Critical components like `scanner` (parsing logic), `graph` (querying and indexing), and `analyze` (LLM client interfaces) should have robust unit tests, particularly for error handling and data transformation.
*   **Integration Testing:** The RAG pipeline (`scanner` -> `graph` -> `analyze` -> LLM) requires integration tests to ensure context retrieval is accurate and LLM clients communicate correctly with external APIs.

### Performance Considerations

*   **Static Analysis:** The use of Tree-sitter with C bindings via `purego` is a deliberate choice to maximize parsing speed, which is the primary bottleneck for large codebases.
*   **Caching:** The `cache` layer is essential for performance and cost management, preventing redundant computation and external API calls.
*   **Graph Serialization:** Using `encoding/gob` for the graph store ensures that the knowledge base can be loaded and saved quickly, minimizing startup time for query and analysis modes.

## Known Issues and Limitations

*   **Tree-sitter C Bindings Complexity:** The reliance on `github.com/ebitengine/purego` and manually managed C grammars introduces significant build complexity and platform-specific dependencies. This is the most fragile part of the build chain.
*   **Tight Coupling in RAG Pipeline:** The `analyze` package is tightly coupled to the internal data structures of the `graph` package. Any significant refactoring of the graph model will require corresponding updates to the RAG retrieval logic.
*   **LLM Client Maintenance:** The need to maintain separate client implementations for four different LLM providers (OpenAI, Anthropic, Gemini, Ollama) increases the surface area for external dependency management and API changes.
*   **TUI Dependency Overhead:** The `charmbracelet/bubbletea` dependency, while providing a great user experience, adds significant binary size for features that are not core to the analysis logic.

## Additional Documentation

*   [Development Plans]: Contains detailed architectural decisions, such as the Graph RAG implementation plan and specific LLM integration strategies.
*   [LLM Integration Guides]: Specific documentation detailing the configuration and usage of individual LLM providers (e.g., Gemini, Claude).
*   [Model Context Protocol (MCP) Specification]: External documentation detailing the full protocol specification for AI agent integration. (Note: This link is inferred as necessary for the MCP server).
*   [CONTRIBUTING.md]: (Inferred) Guidelines for contributing to the codebase.
</file>
<file path="TOOLS-DESCRIPTION.md">
A ferramenta do servidor MCP (`codemap-mcp`) expõe as funcionalidades de análise da base de código, do grafo de conhecimento e do LLM para agentes de codificação.

As ferramentas disponíveis são:

---

## 🛠️ Tools do Servidor MCP

As ferramentas são implementadas nos *handlers* do arquivo `mcp/main.go` e utilizam as capacidades dos pacotes `scanner`, `graph` e `analyze`.

### 1. `get_structure` (Visão Geral da Estrutura)

| Campo | Tipo | Descrição |
| :--- | :--- | :--- |
| **`path`** | `string` | Caminho para a raiz do projeto. |

* **O que faz:** Retorna uma **visualização em árvore** da estrutura do projeto. Inclui detecção de linguagem, tamanho dos arquivos e estimativa de *tokens* por arquivo.
* **Exemplo de Utilidade:** Um agente a utiliza como **primeiro passo** para ter uma visão de alto nível do projeto, identificar os diretórios principais e estimar o tamanho do contexto de cada arquivo (`[!]` para arquivos grandes > 8k tokens).

---

### 2. `get_dependencies` (Análise de Dependências)

| Campo | Tipo | Descrição |
| :--- | :--- | :--- |
| **`path`** | `string` | Caminho para a raiz do projeto. |
| **`detail`** | `int` | Nível de detalhe: `0` (nomes), `1` (assinaturas), `2` (completo). |
| **`mode`** | `string` | Modo de saída: `deps` (fluxo de dependência, padrão) ou `api` (superfície de API pública). |

* **O que faz:** Realiza uma **análise profunda de imports, funções e tipos** utilizando Tree-sitter. Retorna o fluxo de dependência interna e lista dependências externas por linguagem. O modo `api` fornece um resumo compacto de funções e tipos exportados.
* **Exemplo de Utilidade:**
    * Um agente quer saber a **estrutura pública** de um módulo específico (usando `mode="api"`) antes de chamá-lo.
    * Um agente está refatorando um arquivo e precisa ver quais *outros* arquivos ele importa e quais dependências externas ele introduz.

---

### 3. `get_symbol` (Busca por Símbolo Precisa)

| Campo | Tipo | Descrição |
| :--- | :--- | :--- |
| **`path`** | `string` | Caminho para a raiz do projeto. |
| **`name`** | `string` | Nome do símbolo para buscar (busca de substring). |
| **`kind`** | `string` | Filtro por tipo: `function`, `type` ou `all` (padrão). |
| **`file`** | `string` | Filtro por caminho de arquivo (opcional). |

* **O que faz:** Procura por **funções, métodos e tipos** por nome na base de código. Retorna uma lista de correspondências com a **localização exata** (`caminho:linha`) e a **assinatura** completa (se `detail >= 1` for usado internamente).
* **Exemplo de Utilidade:** Um agente recebe um erro sobre a função `processRequest` e precisa encontrar todas as definições dessa função rapidamente, sabendo a linha exata e a assinatura para contextualizar.

---

### 4. `semantic_search` (Busca Híbrida)

| Campo | Tipo | Descrição |
| :--- | :--- | :--- |
| **`path`** | `string` | Caminho para a raiz do projeto. |
| **`query`** | `string` | Consulta em linguagem natural (ex: "função que faz parse da config"). |
| **`limit`** | `int` | Número máximo de resultados (padrão: `10`). |
| **`expand`** | `bool` | Incluir *callers* e *callees* no resultado. |

* **O que faz:** Combina **busca vetorial** (semântica) e **busca estrutural** (por nome no grafo) usando *Reciprocal Rank Fusion*. Retorna os símbolos mais relevantes para uma consulta em linguagem natural com pontuações.
* **Exemplo de Utilidade:** Um agente recebe uma tarefa sobre "como é feito o gerenciamento de tokens" e usa esta ferramenta com `query="token management implementation"` para encontrar as funções e tipos relevantes, mesmo que o termo não esteja no nome do arquivo ou da função.

---

### 5. `explain_symbol` (Explicação com LLM)

| Campo | Tipo | Descrição |
| :--- | :--- | :--- |
| **`path`** | `string` | Caminho para a raiz do projeto. |
| **`symbol`** | `string` | Nome do símbolo para explicar. |
| **`model`** | `string` | Modelo LLM a ser usado (sobrescreve a config). |
| **`no_cache`** | `bool` | Ignora o cache para esta requisição. |

* **O que faz:** Encontra o código-fonte de um símbolo (função, tipo, método) e utiliza um LLM (configurado via `/analyze`) para gerar uma **explicação concisa e estruturada** (*Propósito, Lógica Chave, Parâmetros*). Utiliza cache para evitar chamadas duplicadas.
* **Exemplo de Utilidade:** Um agente precisa entender a lógica de `handleTracePath` antes de modificá-la. A ferramenta fornece a explicação de forma mais rápida e focada do que pedir ao LLM para adivinhar a partir do nome.

---

### 6. `summarize_module` (Sumário de Módulo com LLM)

| Campo | Tipo | Descrição |
| :--- | :--- | :--- |
| **`path`** | `string` | Caminho para a raiz do projeto. |
| **`module`** | `string` | Módulo/diretório para sumarizar (relativo ao projeto). |
| **`model`** | `string` | Modelo LLM a ser usado (sobrescreve a config). |
| **`no_cache`** | `bool` | Ignora o cache para esta requisição. |

* **O que faz:** Lê todos os arquivos-fonte de um diretório ou arquivo e envia para o LLM gerar um **sumário de alto nível** sobre o **propósito do módulo, componentes principais e dependências**.
* **Exemplo de Utilidade:** Um agente precisa de uma visão geral do pacote `/analyze` para identificar onde a lógica de LLM está centralizada antes de integrar uma nova funcionalidade de *streaming*.

---

### 7. `trace_path`, `get_callers`, `get_callees` (Consultas de Grafo)

| Ferramenta | Tipo de Consulta | Exemplo de Uso |
| :--- | :--- | :--- |
| **`trace_path`** | Busca o **caminho mais curto** de chamadas entre dois símbolos (`from` → `to`) no grafo de conhecimento. | Útil para determinar como a função `main` se conecta a uma função de persistência de dados. |
| **`get_callers`** | Encontra quais funções **chamam** um símbolo específico, mostrando a cadeia de chamadas (**dependência reversa**). | Útil para análise de impacto: "Quem me chama e será afetado se eu mudar?" |
| **`get_callees`** | Encontra quais funções são **chamadas** por um símbolo, mostrando a cadeia de dependência. | Útil para entender o fluxo de execução e as dependências diretas de uma função. |

**Exemplo de Utilidade Geral:** Depois de usar `semantic_search` para encontrar um símbolo relevante, o agente usa `get_callers` para identificar o ponto de entrada da funcionalidade, ou `trace_path` para mapear o fluxo completo de uma transação.
</file>


---

## Instructions

Please analyze the provided information and:

1. Understand the task requirements
2. Review the project structure
3. Consider the specified rules and constraints
4. Provide a detailed solution
