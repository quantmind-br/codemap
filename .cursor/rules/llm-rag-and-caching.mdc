---
description: Rules for the intelligence layer, focusing on LLM interaction, RAG, caching, and the specific Anthropic embedding fallback logic.
globs:
  - "analyze/**/*.go"
  - "graph/vectors.go"
  - "cache/**/*.go"
alwaysApply: false
---

# LLM Interaction, RAG, and Cost Control

The `@analyze` package is responsible for efficient and context-aware interaction with external LLMs, focusing on cost reduction and retrieval accuracy.

## 1. Caching and Token Management

All LLM API calls must prioritize cost reduction through caching and token limits.

- **Caching Rule:** Before executing any external LLM API request (e.g., in `analyze/client.go`), the `@cache` package must be checked for a pre-existing response. The cache key should be a hash of the prompt and model configuration.
- **Bypass:** The cache check is only bypassed if the `--no-cache` flag is explicitly set in the configuration.
- **Token Limits:** Use the utilities in `@analyze/tokens.go` to calculate prompt size and ensure the total context (prompt + retrieved RAG chunks) does not exceed the configured `MaxTokens` limit for the selected model.

## 2. Retrieval-Augmented Generation (RAG)

The RAG pipeline is managed by the `analyze.Retriever` and relies on the vector index.

- **Retrieval Flow:** The `analyze.Retriever.Search(query)` method must first query the `graph/vectors.go` component (the VectorStore) to find semantically relevant code snippets (`analyze.Source`).
- **Prompt Augmentation:** The retrieved `analyze.Source` chunks must be injected into the LLM prompt template (from `@analyze/prompts.go`) to provide context for the analysis (`--explain`, `--summarize`).

## 3. Embedding Client Fallback (Anthropic Exception)

Due to provider limitations, a specific fallback logic is required for embedding generation.

- **Rule:** If the primary LLM provider is configured as Anthropic, the embedding client must *not* use the Anthropic API. Instead, the `analyze/factory.go` must explicitly instantiate an embedding client using a different provider (e.g., Ollama or OpenAI) as defined in the configuration. This is a critical, known leaky abstraction.
- **Vector Store:** The `graph/vectors.go` component is responsible for storing the resulting `[]float32` embeddings, mapping them to the deterministic Node IDs from the `graph.CodeGraph`.