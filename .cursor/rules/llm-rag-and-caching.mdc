---
description: Rules for the intelligence layer, focusing on LLM interaction, RAG, caching, and the specific Anthropic embedding fallback logic.
globs:
  - "analyze/**/*.go"
  - "graph/vectors.go"
  - "cache/**/*.go"
alwaysApply: false
---

# LLM Interaction, RAG, and Cost Control

The `@analyze` package is responsible for efficient and context-aware interaction with external LLMs, focusing on cost reduction and retrieval accuracy.

## 1. Caching and Resilience

All LLM API calls must prioritize cost reduction and stability.

- **Caching Rule:** Before executing any external LLM API request (e.g., in `analyze/client.go`), the `@cache` package must be checked for a pre-existing response. The cache key should be a hash of the prompt and model configuration.
- **Bypass:** The cache check is only bypassed if the `--no-cache` flag is explicitly set in the configuration.
- **Retry Logic:** All external API clients must implement a retry mechanism for transient errors (e.g., network issues, rate limits) with a maximum of **3 attempts** and an exponential backoff strategy (1s, 2s, 4s delays).

## 2. Retrieval-Augmented Generation (RAG)

The RAG pipeline is managed by the `analyze.Retriever` and relies on the vector index.

- **Retrieval Flow:** The `analyze.Retriever.Search(query)` method must first query the `graph/vectors.go` component (the VectorStore) to find semantically relevant code snippets (`analyze.Source`).
- **Prompt Augmentation:** The retrieved `analyze.Source` chunks must be injected into the LLM prompt template (from `@analyze/prompts.go`) to provide context for the analysis (`--explain`, `--summarize`).
- **Protocol:** All LLM communication must utilize the `github.com/modelcontextprotocol/go-sdk` for structured requests and responses.

## 3. Embedding Client Fallback (Anthropic Exception)

Due to provider limitations, a specific fallback logic is required for embedding generation.

- **Critical Rule:** If the primary LLM provider is configured as Anthropic, the embedding client must *not* use the Anthropic API. Instead, the `analyze/factory.go` must explicitly instantiate an embedding client using a different provider (e.g., Ollama or OpenAI) as defined in the configuration. This is a critical, known leaky abstraction that must be maintained.